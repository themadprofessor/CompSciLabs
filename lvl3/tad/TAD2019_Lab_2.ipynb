{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1suGewMzIKsC"
   },
   "source": [
    "# Lab 2\n",
    "\n",
    "The aims of the lab are to:\n",
    "*   Introduce the spaCy python library for text processing \n",
    "*   Learn the details of how a dictionary is implemented\n",
    "*   Practice creating a sparse one-hot encoding \n",
    "*   Implement Jaccard similarity\n",
    "*   Learn to use SciKit-Learn to vectorize text with a bag-of-words representation\n",
    "*   Use Cosine similarity to find similar documents in a collection\n",
    "*   Perform KMeans clustering on Reddit posts\n",
    "*   Practice performing basic evaluation of clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "napJywoXLO7u"
   },
   "source": [
    "## Reddit Data Review\n",
    "\n",
    "**Thread fields**\n",
    "*   URL - reddit URL of the thread\n",
    "*   title - title of the thread, as written by the first poster\n",
    "*   is_self_post - True if the first post in the thread is a self-post (text addressed to the reddit community as opposed to an external link)\n",
    "*   subreddit - the subreddit of the thread\n",
    "*   posts - a list of all posts in the thread\n",
    "\n",
    "**Post fields**\n",
    "*   id - post ID, reddit ID of the current post\n",
    "*   body - the text of the post\n",
    "*   in_reply_to - parent ID, reddit ID of the parent post, or the post that the current post is in reply to\n",
    "*   post_depth - the number of replies the current post is from the initial post\n",
    "*   is_first_post - True if the current post is the initial post\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw-TMlAZL7x2"
   },
   "source": [
    "Download the Reddit dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OyYm3gDwJKQ-",
    "outputId": "6f5a7ecc-9932-4a94-cab8-294a402d7cfb"
   },
   "outputs": [],
   "source": [
    "# The local location to store the reddit dataset.\n",
    "local_file = \"coarse_discourse_dump_reddit.json\"\n",
    "\n",
    "#!gsutil cp gs://textasdata/coarse_discourse_dump_reddit.json $local_file\n",
    "  \n",
    "# The ! performs a shell command to download the reddit dataset using wget.\n",
    "#!wget -O  $local_file https://storage.googleapis.com/textasdata/coarse_discourse_dump_reddit.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPyo3mtPvqux"
   },
   "source": [
    "Load the JSON data into DataFrame with each post as a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-XGj_AFCMBz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110595\n"
     ]
    }
   ],
   "source": [
    "# The reddit thread structure is nested with posts in a new content.\n",
    "# This block reads the file as json and cates a new data frame.\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# A temporary variable to store the list of post content.\n",
    "posts_tmp = list()\n",
    "\n",
    "with open(local_file) as jsonfile:\n",
    "  for i, line in enumerate(jsonfile):\n",
    "    thread = json.loads(line)\n",
    "    for post in thread['posts']:\n",
    "      # Keep the thread title and subreddit with each post.\n",
    "      posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
    "                        post['id'], post.get('author', \"\"), post.get('body', \"\")))\n",
    "print(len(posts_tmp))\n",
    "\n",
    "# Create the posts data frame.  \n",
    "labels = ['subreddit', 'title', 'id', 'url', 'author', 'body']\n",
    "post_frame = pd.DataFrame(posts_tmp, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nczWjc6jLDP_"
   },
   "source": [
    "## Introduction to spaCy\n",
    "[spaCy](https://spacy.io/) is an open-source software library for  Natural Language Processing, written in Python and Cython. The library is published under the MIT license and currently offers statistical models for English, German, Spanish, Portuguese, French, Italian, Dutch as well as tokenization for various other languages. In contrast to NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage. SpaCy is widely used by many companies for text and NLP processing. \n",
    "\n",
    "We will also use it later in the course for more advanced NLP tasks. \n",
    "\n",
    "**Note:** SpaCy includes a variety of models. Below we are using the english web small.  See the full list of [models](https://spacy.io/usage/models).  In practice, better effectiveness can be obtained by using a larger model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "070sQ63uTZ7V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.2 (default, Jan 10 2019, 23:51:51) \n",
      "[GCC 8.2.1 20181127]\n",
      "spacy version 2.0.16 is\n",
      "OK\n",
      "[('tagger', <spacy.pipeline.Tagger object at 0x7f0127724da0>), ('parser', <spacy.pipeline.DependencyParser object at 0x7f00f381e728>)]\n",
      "['tagger', 'parser']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en\n",
    "\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "# Version checks\n",
    "import importlib\n",
    "def version_check(libname, min_version):\n",
    "    m = importlib.import_module(libname)\n",
    "    print (\"%s version %s is\" % (libname, m.__version__))\n",
    "    print (\"OK\" if m.__version__ >= min_version \n",
    "           else \"out-of-date. Please upgrade!\")\n",
    "    \n",
    "version_check(\"spacy\", \"2.0\")\n",
    "\n",
    "# Load the small english model. \n",
    "# Disable the advanced NLP features in the pipeline for efficiency.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "print(nlp.pipeline)\n",
    "print(nlp.pipe_names)\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "# Verify they are empty.\n",
    "print(nlp.pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJ06kvvlKBLG"
   },
   "source": [
    "### SpaCy Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hc2ByowtVtD5"
   },
   "source": [
    "Last week we used NLTK to tokenized and normalized text.  This week we moved to a more advanced library. \n",
    "\n",
    "Below is example code of processing one of the Reddit posts with spaCy.  In particular, the code below prints out a few of the properties of the [Token](https://spacy.io/api/token) class. This class exposes many useful properties of tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqUx829bVOT1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\t0\tI\tFalse\tFalse\tX\n",
      "love\t2\tlove\tFalse\tFalse\txxxx\n",
      "cheese\t7\tcheese\tFalse\tFalse\txxxx\n",
      "cake\t14\tcake\tFalse\tFalse\txxxx\n",
      "!\t18\t!\tTrue\tFalse\t!\n",
      "I\t20\tI\tFalse\tFalse\tX\n",
      "love\t22\tlove\tFalse\tFalse\txxxx\n",
      "both\t27\tboth\tFalse\tFalse\txxxx\n",
      "making\t32\tmake\tFalse\tFalse\txxxx\n",
      "and\t39\tand\tFalse\tFalse\txxx\n",
      "eating\t43\teat\tFalse\tFalse\txxxx\n",
      "it\t50\tit\tFalse\tFalse\txx\n",
      ",\t52\t,\tTrue\tFalse\t,\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(post_frame.loc[10]['body'])\n",
    "for token in doc[:13]:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfghX7NhMulS"
   },
   "source": [
    "Note that spaCy includes the raw token, it's position in the original string, the lemma (using its [lemmatizer](https://spacy.io/api/lemmatizer)), as well other properties of the token. \n",
    "\n",
    "- What do the 'shape' patterns capture?  How might this token attribute be useful? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qx-e-Nu8KZxO"
   },
   "source": [
    "### Optional warm-up: tokenization and normalization with spaCy\n",
    "Below are several optional warm-up tasks that review material from the previous lab, but implemented with spaCy.   These should be done pretty quickly (5-10 minutes). If they are taking longer, see the solutions and move on to next section of the lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGv9Pr8wx1dg"
   },
   "source": [
    "#### Optional task\n",
    "- Create a function ``spacy_tokenize`` function that uses spaCy to tokenize a string. The function should:\n",
    " - Accept a string as input\n",
    " - Output a list of spaCy token objects\n",
    " \n",
    " You may click SHOW CODE below to see the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bcoe0PGJ3gU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Sk2z8rRNSIof"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8ualh3vydj2"
   },
   "source": [
    "Below we  apply the ``spacy_tokenize`` function to the ``body`` field of the posts in the ``post_frame`` DataFrame. The results are flattened into a ``flat_tokens`` variable that contains a single list of all tokens from all posts concatenated together. \n",
    "\n",
    "Note: Applying spaCy's tokenizer to all the posts will take a couple minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M77rXz2dM-DY"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This tokenizes the body posts and creates vector of tokens for each post.\n",
    "# Note: This selections the body column from the posts only. \n",
    "all_posts_tokenized = post_frame.body.apply(spacy_tokenize)\n",
    "\n",
    "import itertools\n",
    "# A single variable with the (flattened) tokens from all posts.\n",
    "flat_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEDz3GIN0GEl"
   },
   "source": [
    "#### Optional task\n",
    "\n",
    "- Inspect some of the tokenization to verify that it worked correctly \n",
    "- Print out the 20 most frequent (raw) tokens in the collection from the ``flat_tokens`` variable.\n",
    " - Hint: Recall the ``Counter`` object from the last lab. \n",
    " - *Tip*: There are multiple ways to do this. Consider using a list comprehension to extract the  text from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoAihiIcJy1q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Fdf5j1kMadZN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 244379),\n",
       " (',', 175780),\n",
       " ('the', 162024),\n",
       " ('I', 137844),\n",
       " ('to', 122575),\n",
       " ('a', 110136),\n",
       " ('and', 97918),\n",
       " ('of', 72791),\n",
       " ('it', 70102),\n",
       " ('you', 68689),\n",
       " ('is', 60717),\n",
       " ('\\n\\n', 60224),\n",
       " ('that', 57067),\n",
       " ('in', 53429),\n",
       " ('for', 45987),\n",
       " (\"'s\", 39097),\n",
       " ('-', 38366),\n",
       " (\"n't\", 38080),\n",
       " ('*', 36234),\n",
       " (')', 36016)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "import collections\n",
    "raw_words = [t.text for t in flat_tokens]\n",
    "raw_count = collections.Counter(raw_words)\n",
    "raw_count.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0mp6eapy2LK"
   },
   "source": [
    "#### Optional task:\n",
    "Create a ``normalize`` function that normalizes raw text into a canonical form. The function should:\n",
    " - Take a list of spaCy token objects as input\n",
    " - Output a list of normalized strings\n",
    " - The normalization should only keep tokens consisting of alphanumeric characters. \n",
    " - Normalization should use Spacy's lemma property from the token. \n",
    " - The output should be lowerecased and trimmed of any extra whitespace.\n",
    " - One special case to handle is when Spacy's lemma is \"-PRON-\" , instead preserve the lowercased text token.\n",
    " \n",
    " \n",
    " You will need this code below. You may see show the answer below if you get stuck.  Click SHOW CODE to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WfRcbOJOy6j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "14m7RfqGzo_x"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def normalize(tokens):\n",
    "  normalized = list()\n",
    "  for token in tokens:\n",
    "    if (token.is_alpha):\n",
    "      lemma = token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_\n",
    "      normalized.append(lemma)\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWDK5Fwi6ByI"
   },
   "source": [
    "The code below runs the ``normalize`` function on the ``flat_tokens`` and stores it in ``normalized_tokens``. We will use these for our vocabulary and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_D_wpV5Q4l69"
   },
   "outputs": [],
   "source": [
    "normalized_tokens = normalize(flat_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qGXn8xkgTn6O"
   },
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JBfGbpYBO8N"
   },
   "source": [
    "We will now implement a one-hot encoding text representation using a dictionary.\n",
    "\n",
    "Below is a skeleton class that implements a dictionary.  Recall from Lecture 1 that a dictionary allows us to translate a series of tokens to integer values (and back).\n",
    "\n",
    "#### Your task\n",
    "- The ``SimpleDictionary`` skeleton below is incomplete, fill in the missing elements. Specifically:  \n",
    " - Complete the ``_init_`` constructor to initialize the member variables appropriately\n",
    " - Implement the ``tokens_to_ids`` function that maps strings to integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAVcr270_gz2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SimpleDictionary(object):\\n  \\n  # Special UNK token for unseen tokens\\n  UNK_TOKEN = \"<unk>\"\\n\\n  def __init__(self, tokens, size=None):\\n    \\n    # All unigrams with their counts\\n    self.unigram_counts = \\n    \\n    # The total size of the collection in tokens\\n    self.collection_size = \\n    \\n    # The number of unique unigrams\\n    self.num_unigrams = \\n    \\n    # Set of most frequent words (limited to top K by size if defined)\\n    # These are in descending order of collection frequency.\\n    # Remember to leave space for \"<unk>\" tokens.\\n    # Where should it go in the ordering? Why?\\n    self.vocab = \\n\\n    # Dictionary that assigns an id to each token, by frequency.\\n    self.id_to_token = \\n    \\n    # Dictionary that assigns a token to id \\n    self.token_to_id = \\n    \\n    self.size = len(self.id_to_token)\\n    if size is not None:\\n        assert(self.size <= size)\\n\\n    # For convenience keep a set of unique words.\\n    self.tokenset = set(iter(self.token_to_id.keys()))\\n\\n    # Store special IDs for convenience\\n    self.UNK_ID = self.token_to_id[self.UNK_TOKEN]\\n\\n  # Given a sequence of ids, return a sequence of corresponding tokens.\\n  def ids_to_tokens(self, ids):\\n    return [self.id_to_token[i] for i in ids]\\n  \\n  # Given an input sequence of tokens, return a sequence of token id.\\n  def tokens_to_ids(self, tokens):\\n    # YOUR CODE HERE'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"class SimpleDictionary(object):\n",
    "  \n",
    "  # Special UNK token for unseen tokens\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    \n",
    "    # All unigrams with their counts\n",
    "    self.unigram_counts = \n",
    "    \n",
    "    # The total size of the collection in tokens\n",
    "    self.collection_size = \n",
    "    \n",
    "    # The number of unique unigrams\n",
    "    self.num_unigrams = \n",
    "    \n",
    "    # Set of most frequent words (limited to top K by size if defined)\n",
    "    # These are in descending order of collection frequency.\n",
    "    # Remember to leave space for \"<unk>\" tokens.\n",
    "    # Where should it go in the ordering? Why?\n",
    "    self.vocab = \n",
    "\n",
    "    # Dictionary that assigns an id to each token, by frequency.\n",
    "    self.id_to_token = \n",
    "    \n",
    "    # Dictionary that assigns a token to id \n",
    "    self.token_to_id = \n",
    "    \n",
    "    self.size = len(self.id_to_token)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience keep a set of unique words.\n",
    "    self.tokenset = set(iter(self.token_to_id.keys()))\n",
    "\n",
    "    # Store special IDs for convenience\n",
    "    self.UNK_ID = self.token_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  # Given a sequence of ids, return a sequence of corresponding tokens.\n",
    "  def ids_to_tokens(self, ids):\n",
    "    return [self.id_to_token[i] for i in ids]\n",
    "  \n",
    "  # Given an input sequence of tokens, return a sequence of token id.\n",
    "  def tokens_to_ids(self, tokens):\n",
    "    # YOUR CODE HERE\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjAEasEBYbR0"
   },
   "source": [
    "If you get stuck here, the solution is provided below, click SHOW CODE to see it.  \n",
    "\n",
    "We need it to work for the later exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "q97tRTc0D1i1"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "class SimpleDictionary(object):\n",
    "  \n",
    "  # Special UNK token\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    # All unigrams with their counts\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    \n",
    "    # The total size of the collection in tokens\n",
    "    self.collection_size = len(tokens)\n",
    "    \n",
    "    # The number of unique unigrams\n",
    "    self.num_unigrams = len(self.unigram_counts.keys())\n",
    "    \n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "\n",
    "    # Set of most frequent words (limited to top K by size if defined)\n",
    "    # Remember to leave space for \"<unk>\" tokens.\n",
    "    self.vocab = ([self.UNK_TOKEN] + [t for t,c in top_counts])\n",
    "\n",
    "    # Dictionary that assigns an id to each token, by frequency.\n",
    "    self.id_to_token = dict(enumerate(self.vocab))\n",
    "    \n",
    "    # Dictionary that assign a token to id \n",
    "    self.token_to_id = {v:k for k,v in iter(self.id_to_token.items())}\n",
    "    \n",
    "    self.size = len(self.id_to_token)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience keep a set of unique words.\n",
    "    self.tokenset = set(iter(self.token_to_id.keys()))\n",
    "\n",
    "    # Store special IDs for convenience\n",
    "    self.UNK_ID = self.token_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  # Given a sequence of ids, return a sequence of corresponding tokens.\n",
    "  def ids_to_tokens(self, ids):\n",
    "    return [self.id_to_token[i] for i in ids]\n",
    "  \n",
    "  # Given an input sequence of tokens, return a sequence of token IDs.\n",
    "  def tokens_to_ids(self, tokens):\n",
    "    return [self.token_to_id.get(t, self.UNK_ID) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVeVMgT3YthP"
   },
   "source": [
    "Run the dictionary on the ``normalized_tokens`` that contains all of the tokens in the collection.  In Sci-kit Learn this is called \"fitting\", creating a vocabulary from a fixed collection of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8BX_LjcgQwvk"
   },
   "outputs": [],
   "source": [
    "dictionary = SimpleDictionary(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1f3rt_fQ2Bw"
   },
   "source": [
    "#### Optional task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_B4Wa76J6P"
   },
   "source": [
    "- Use the ``dictionary`` to print out properties of the text collection. \n",
    "\n",
    " - Print out the total number of tokens (N)\n",
    " - Print out the size of the vocabulary  (V)\n",
    " - Print out the top 20 most frequent unigrams with three values: token, collection frequency, percentage of collection tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YMljTTdwtMI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "CUxo74iE6txn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection size: 4448955\n",
      "Vocabulary size: 72344\n",
      "the\t175938\t3.9545915838663235\n",
      "be\t167894\t3.77378507986707\n",
      "a\t149415\t3.3584291142526728\n",
      "i\t146584\t3.2947961937129056\n",
      "to\t124878\t2.8069063409272514\n",
      "and\t102057\t2.2939544230049527\n",
      "it\t83701\t1.8813631515715488\n",
      "you\t78023\t1.7537376754766008\n",
      "of\t73378\t1.6493311350643016\n",
      "that\t66063\t1.484910501454836\n",
      "in\t56212\t1.2634877179022939\n",
      "have\t56019\t1.2591496205288657\n",
      "for\t48242\t1.0843445258493287\n",
      "do\t48228\t1.0840298452108417\n",
      "on\t34400\t0.7732152831395238\n",
      "but\t34350\t0.7720914237163559\n",
      "with\t33157\t0.7452761378795695\n",
      "this\t32615\t0.7330935017324293\n",
      "can\t31966\t0.7185058064197098\n",
      "my\t29458\t0.6621330177536073\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "print(\"Collection size: \" + \"{0}\".format(dictionary.collection_size))\n",
    "print(\"Vocabulary size: \" + \"{0}\".format(dictionary.size))\n",
    "\n",
    "for (word, count) in dictionary.unigram_counts.most_common(20):\n",
    "  print(\"{0}\\t{1}\\t{2}\".format(word, count, 100 * count / dictionary.collection_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WF0X7aqS8Yz1"
   },
   "source": [
    "The most frequent word, *the*, accounts for approximately 4% of all tokens.  The top 10 most frequent words account for over 25% of all word occurrences.  Recall [Zipf's law](https://simple.wikipedia.org/wiki/Zipf%27s_law) from lecture 2 and the power law distribution of text data. A few number of terms account for a large fraction of occurrences, but many words occur rarely.  You might consider, how many words occur just once?  These are all taking up space in the vocabulary.  As the collection size increases, we may prune the size of the dictionary.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElIuBpMaSJ5X"
   },
   "source": [
    "### From tokens to IDs and back again\n",
    "Below are some examples of using the dictionary to map tokens to IDs in our vocabulary (and vice versa). Consider trying some of your own words to experiment with what happens here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_t2NjsIOpSR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29]\n",
      "[0]\n",
      "['be']\n",
      "['gazetter']\n"
     ]
    }
   ],
   "source": [
    "# Pick a word from the dictionary\n",
    "print(dictionary.tokens_to_ids([\"like\"]))\n",
    "\n",
    "# What's the value of a made up word? \n",
    "print(dictionary.tokens_to_ids([\"likemymadeupword\"]))\n",
    "\n",
    "# For fun, let's print out a couple random words from the vocab.\n",
    "# Feel free to explore the vocabulary.\n",
    "import random as rand\n",
    "print(dictionary.ids_to_tokens([2]))\n",
    "print(dictionary.ids_to_tokens([rand.randint(0, dictionary.size-1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "713ltAX6OTSz"
   },
   "source": [
    "- What is the value of the second word? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZk2lnk2Ojfv"
   },
   "source": [
    "SpaCy also provides access to a [vocabulary](https://spacy.io/api/vocab) object. It contains the normalized types, called Lexemes.  \n",
    " - What does spaCy use for its dictionary implementation? \n",
    "  - *Hint*: The values are from it's [StringStore](https://spacy.io/api/stringstore) object. \n",
    "  - What is the rationale for this implementation? What does this mean for token IDs for different spaCy models? \n",
    "  - Look at the token class. What field has the vocab integer identifier? \n",
    "  \n",
    "  You can access spaCy's vocab for the language you are using from the ``nlp.vocab`` variable.  The code below prints out the top n words of its vocab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5GCJNzXxKGM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convincing\n",
      "故明\n",
      "palm\n",
      "Bamboo\n",
      "Hundred\n",
      "nonprofit\n",
      "upholstery\n",
      "Beltway\n",
      "steakhouse\n",
      "maureen\n",
      "tentative\n",
      "Jiayangduoji\n",
      "encoded\n",
      "Run\n",
      "532,000\n",
      "futures\n",
      "bascially\n",
      "Surrounding\n",
      "midcapitalization\n",
      "flow\n",
      "Meetings\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for w in nlp.vocab:\n",
    "  if (n > 20): \n",
    "    break\n",
    "  print(w.text)\n",
    "  n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "md2M0HVgSe_s"
   },
   "source": [
    "### Creating a one-hot encoding representation\n",
    "\n",
    "#### Your task\n",
    "- Create a function: ``one_hot_encoding`` that uses the ``SimpleDictionary`` to take a string and return a vector of integers:\n",
    " - Takes a string as input and applies tokenization and normalization using the provided ``tokenize_normalize`` function.\n",
    " - Output a sparse one-hot encoding of the text (sorted in ascending order) \n",
    " - Test the code by running it on the post in row index 10 in the post_frame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZCnLAjhGJRo"
   },
   "outputs": [],
   "source": [
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLXL62OnRVeo"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(s):\n",
    "    return set(dictionary.tokens_to_ids(tokenize_normalize(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qjeSYIPW5km"
   },
   "source": [
    "The original input sequence for the post has 87 tokens. The one-hot encoding has 62 values (including all of tokens 1-7).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XI2IHWEUVqtM"
   },
   "source": [
    "Run the same on the string below.  (and then try favorite sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCTbHe0gU5n1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2, 4, 6, 34, 40, 57, 117, 124, 779, 813}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cofveve = \"What is covfefe and why am I seeing all over social media?\"\n",
    "one_hot_encoding(cofveve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MB614SMsVdD3"
   },
   "source": [
    "- What does the first 0 indicate?\n",
    "- What token does it represent? \n",
    "\n",
    "Our model only knows about tokens it has previously seen (and maybe only a subset of tokens if the size of the dictionary is truncated to remove rare words). The rest are assigned to the UNK token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98bwvM1HUQSD"
   },
   "source": [
    "## Jaccard similarity betwen pieces of text\n",
    "\n",
    "#### Your task \n",
    "\n",
    "- Create a function ``jaccard_similarity`` that takes two documents represented as sparse one-hot encodings and computes the jaccard similarity. \n",
    "- *Hint*: You might want to look at the operations on the built-in set datastructure \n",
    "(https://docs.python.org/3/tutorial/datastructures.html#sets)\n",
    "- *Debugging Tip*: Consider printing the different elements of Jaccard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWM7O6y1RwVK"
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(doc1, doc2):\n",
    "    return (len(doc1 & doc2)) / (len(doc1 | doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APOVSEndauri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 1292, 754, 2837, 117}\n",
      "{1, 491, 1644, 754, 117, 2837}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = one_hot_encoding(\"the cat jumped over the fox\")\n",
    "doc2 = one_hot_encoding(\"the brown fox jumped over the dog\")\n",
    "\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "jaccard_similarity(doc1, doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQT8PaaIdqMj"
   },
   "source": [
    "The jaccard similarity of the sequences should be approximately 0.5714.  You might also recall other set-based similarity measures we discussed. Implementing them with a one-hot encoding should be familiar now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKnOHrlEfOjj"
   },
   "source": [
    "### Section summary\n",
    "In the previous section we: \n",
    " - Created a dictionary object and used it to represent text. \n",
    " - Created a one-hot encoding of text documents\n",
    " - Implemented the Jaccard similarity function \n",
    "\n",
    "\n",
    "We could also have extended our functions to create bag-of-words representations.  In the next section we'll explore how to do this with one of the most widely used machine learning libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XWjf3yuTO6J"
   },
   "source": [
    "## Vector representations with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25XYnTCi2CWz"
   },
   "source": [
    "Scikit-learn is a widely machine learning library that includes tools for performing operations on data: similarity computation, clustering, classification, and many others. We'll use Scikit-learn to create vector representations of text data.\n",
    "\n",
    "We first extract out a few fields from the DataFrame and put it into a friendlier format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q997lE7FTiyo"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# Parallel arrays of the post keys and values.\n",
    "post_vals = list()\n",
    "post_keys = list()\n",
    "\n",
    "# Limit the size of the data loaded\n",
    "# Recall that there is approximately 110k posts in the dataset.\n",
    "posts_to_load = 10000\n",
    "\n",
    "for post in islice(post_frame.itertuples(index=True, name='Pandas'), posts_to_load):\n",
    "    post_keys.append(getattr(post, 'id'))\n",
    "    post_vals.append(getattr(post,'body'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjc1wcH8f-Xo"
   },
   "source": [
    "#### Your task\n",
    "Create a document-term matrix with term frequency (counts) from the collection using the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). This will have the following steps:\n",
    " - Import the ``CountVectorizer`` and create an instance; assign it to a variable, ``tf_vectorizer``\n",
    " - All Scikit-Learn vectorizers accept a tokenizer as an optional parameter (it has a built-in tokenizer).  Pass in the ``tokenize_normalize`` function to ``CountVectorize (tokenizer=...)`` we defined above that performs both operations in a single step with spaCy. \n",
    " - Call ``fit`` on the ``post_vals`` variable to learn a vocabulary\n",
    " - ``Transform`` the ``post_vals`` into a document-term matrix, assign it to a variable, tf_document_term_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJRhtkPGJBWP"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(tokenizer=tokenize_normalize)\n",
    "tf_vectorizer.fit(post_vals)\n",
    "tf_document_term_matrix = tf_vectorizer.transform(post_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppFI17VRiA56"
   },
   "source": [
    "What did this process do?\n",
    " - The ``fit()`` function tokenized the text collection and built a vocabulary/dictionary \n",
    " - The ``transform()`` function created a document-term matrix with a bag-of-words representation (raw TF) word counts as the weighting.\n",
    " \n",
    " These steps are sometimes combined together with the single step called ``fit_transform``.  The constructor of ``CountVectorizer`` accepts parameters on how to control the dictionary created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlZXjw0Fiif8"
   },
   "source": [
    "Let's now apply the vectorizer on new unseen text.  We do this by calling ``transform()`` on the string data (technically an array of strings, each entry a document). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p0tn37ZW1Ji"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t2\n",
      "  (0, 1165)\t1\n",
      "  (0, 1598)\t2\n",
      "  (0, 4687)\t1\n",
      "  (0, 4858)\t2\n",
      "  (0, 5355)\t2\n",
      "  (0, 6287)\t1\n",
      "  (0, 8494)\t1\n",
      "  (0, 8672)\t2\n",
      "  (0, 8687)\t1\n",
      "  (0, 8847)\t1\n",
      "  (0, 9219)\t1\n",
      "  (0, 10568)\t1\n",
      "  (0, 12950)\t1\n",
      "  (0, 13241)\t2\n",
      "  (0, 14141)\t1\n",
      "[array(['a', 'be', 'brand', 'fear', 'flagship', 'get', 'i', 'my', 'new',\n",
      "       'next', 'now', 'over', 'recently', 'the', 'town', 'war'],\n",
      "      dtype='<U152')]\n"
     ]
    }
   ],
   "source": [
    "mystring = 'The next town over recently got a brand new flagship Lidl – now my town is getting a brand new flagship Aldi. I fear war.'\n",
    "response = tf_vectorizer.transform([mystring])\n",
    "print (response)\n",
    "print (tf_vectorizer.inverse_transform(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0Q0vgPe3dqD"
   },
   "source": [
    "What is the output here? \n",
    " - (0, 2) 2 has three parts --> (row, column) count; this shows a simple document-term matrix with count values.\n",
    " \n",
    " \n",
    " What happened? \n",
    " - ``Transform`` applies the vectorizor, just like ``tokens_to_ids`` in our dictionary implementation.  \n",
    " - The result is a document-term matrix for the data passed to it. \n",
    " \n",
    " The ``inverse_transform`` is just like ``ids_to_tokens`` applied to every non-UNK value (which are not invertable). \n",
    " \n",
    " \n",
    " **Question:** We did not call ``fit``.  Why? What would this have done?\n",
    "\n",
    "In the inverse output,  see that some of the words are not present (e.g. lidl, aldi, etc...). These are just ignored; calling ``transform()`` does not update our vocabulary and there is no UNK representation in this vocabulary.  Scikit-Learn ignores all UNK tokens that haven't been seen when using CountVectorizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXmMp1CTWaWi"
   },
   "source": [
    "#### Your task \n",
    "- Create a TF-IDF vectorizer and apply it to the ``post_vals`` similar to what was done for CountVectorizer. \n",
    " - Use the [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    " - Set sublinear_tf = true to use the log scaling (as opposed to raw TF counts)\n",
    " - Create n-grams up to length 2\n",
    " - Limit the number of features (the size of the vocabulary) to 50000\n",
    " - Assign it to a ``ngram_vectorizer`` variable\n",
    " \n",
    " \n",
    "Read the documentation of the vectorizer for details on the parameters as needed.  Warning: if this takes too long, or crashes then your configuration is wrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgbExDPpJTjS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(0, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function tokenize_normalize at 0x7f00b32f9ae8>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ngram_vectorizer = TfidfVectorizer(sublinear_tf=True, tokenizer=tokenize_normalize, ngram_range=(0,2), max_features=50000)\n",
    "ngram_vectorizer.fit(post_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdGeP28VXdA3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 46655)\t0.1529200505454279\n",
      "  (0, 45176)\t0.21900116111582324\n",
      "  (0, 45174)\t0.26513495403373744\n",
      "  (0, 42302)\t0.1514435625636529\n",
      "  (0, 41407)\t0.045425923871779426\n",
      "  (0, 33698)\t0.21043380970617825\n",
      "  (0, 33681)\t0.1510874409082641\n",
      "  (0, 27804)\t0.11383289200470212\n",
      "  (0, 23089)\t0.22468290543476732\n",
      "  (0, 23060)\t0.10079337738312472\n",
      "  (0, 22491)\t0.12951330275210632\n",
      "  (0, 22426)\t0.19329392423313688\n",
      "  (0, 22126)\t0.2013520512394951\n",
      "  (0, 21781)\t0.06962748951097884\n",
      "  (0, 16447)\t0.23200794786233606\n",
      "  (0, 16325)\t0.04550970901909656\n",
      "  (0, 13682)\t0.2049998156315833\n",
      "  (0, 13681)\t0.12639447503160012\n",
      "  (0, 12174)\t0.18014428319259548\n",
      "  (0, 6375)\t0.3804212278568931\n",
      "  (0, 6373)\t0.3180425346969255\n",
      "  (0, 4968)\t0.16481152250476264\n",
      "  (0, 4617)\t0.04360237765441838\n",
      "  (0, 183)\t0.39282360279061307\n",
      "  (0, 36)\t0.07736838173801036\n",
      "  (0, 0)\t0.10742221143788376\n",
      "[array(['war', 'town be', 'town', 'the next', 'the', 'recently get',\n",
      "       'recently', 'over', 'now my', 'now', 'next', 'new', 'my town',\n",
      "       'my', 'i fear', 'i', 'get a', 'get', 'fear', 'brand new', 'brand',\n",
      "       'be get', 'be', 'a brand', 'a', ''], dtype='<U29')]\n"
     ]
    }
   ],
   "source": [
    "str = 'The next town over recently got a brand new flagship Lidl – now my town is getting a brand new flagship Aldi. I fear war.'\n",
    "ngram_matrix = ngram_vectorizer.transform([str])\n",
    "print (ngram_matrix)\n",
    "print (ngram_vectorizer.inverse_transform(ngram_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5QjuYZn4pyx"
   },
   "source": [
    "We're just scratching the surface of what's possible.  There are other types of representations as well.  For example, there is a [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) and others.\n",
    "- What are the pros and cons of using HasingVectorizer vs TFIDFVectorizer?\n",
    "- Question: What does the ``fit()`` function do on the HashingVectorizer?  Why is this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEplJqQ4Zl4l"
   },
   "source": [
    "## Cosine similarity\n",
    "We will now use sklearn's cosine similarity implementation to find similar posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWR6NZZIJKe4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "# A function that given an input query item returns the top-k most similar items \n",
    "# by their cosine similarity.\n",
    "def find_similar(query_vector, td_matrix, top_k = 5):\n",
    "    cosine_similarities = cosine_similarity(query_vector, td_matrix).flatten()\n",
    "    related_doc_indices = cosine_similarities.argsort()[::-1]\n",
    "    return [(index, cosine_similarities[index]) for index in related_doc_indices][0:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zc6zTizJLgs"
   },
   "source": [
    "#### Your task\n",
    "- Find the closes 10 posts to the string below. \n",
    "  - For each of the most similar posts, print out four values: the cosine similarity, index in the post data, its URL, and it's body content.  Hint: You might use the ``post_keys`` and ``post_values`` we are operating over. \n",
    "  - Repeat the exercise for both the CountVectorize as well as the TFIDFVectorizer with n-grams. \n",
    "   - Try a different string value and repeat . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwE6Gu6YJXm0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you\n",
      "----------\n",
      "Thank you!\n"
     ]
    }
   ],
   "source": [
    "# An input string.\n",
    "##str = 'The next town over recently got a brand new flagship Lidl – now my town is getting a brand new flagship Aldi. I fear war.'\n",
    "\n",
    "# Or take the content of a random post.\n",
    "import random as rand\n",
    "post_index = rand.randint(0, len(post_vals))\n",
    "string = post_vals[post_index]\n",
    "\n",
    "matrix = ngram_vectorizer.transform([string])\n",
    "similar = find_similar(matrix, ngram_vectorizer.transform(post_vals))\n",
    "print(string)\n",
    "print(\"-\" * 10)\n",
    "print(post_vals[similar[1][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W66BwN2Z7dPc"
   },
   "source": [
    "What do you see?  If you use a post, it should find itself and return a similarity score of 1.0.\n",
    "\n",
    "Try experimenting with different vectorizers and matrix representations (count, tfidf, ngrams). \n",
    "- How do the most similar posts change?\n",
    "- What do you think is most effective? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIx1ORo4Ziga"
   },
   "source": [
    "## KMeans clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUhTG7OQ8Wag"
   },
   "source": [
    "What's in the Reddit dataset? When we want to explore a dataset, one method is to apply clustering and then to inspect the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dj6xJlnz8BAN"
   },
   "source": [
    "\n",
    "From the SKlearn documentation:\n",
    "The [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.  K-means is sometimes referred to as Lloyd’s algorithm. \n",
    "\n",
    "Recall from lecture: In basic terms, the algorithm has three steps. \n",
    "1) The first step chooses the initial centroids, with the most basic method being to choose k samples from the dataset X. \n",
    "\n",
    "\n",
    "After initialization, K-means consists of looping between the two other steps. The first assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats until the centroids do not change significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoyaNBfPMFDa"
   },
   "source": [
    "#### Your task\n",
    " - Run K-Means on the document-term matrix from TFIDF vectorization. \n",
    "  - Start with ``k=8`` clusters\n",
    "  - Use KMeans with 'random' cluster initialization\n",
    "  - Add verbose=10 to show the clustering progress\n",
    "  - Just like fitting a vocabulary, clustering is produced by calling ``fit``, but on the document-term matrix (not raw data), to fit clusters to do matrix. \n",
    "  - Assign the result to a ``kmeans`` variable. \n",
    "  \n",
    " \n",
    " \n",
    " If it is slow, you might need to use MiniBatchKMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4bQ91RzMy34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 831853.000\n",
      "Iteration  1, inertia 601731.473\n",
      "Iteration  2, inertia 575068.552\n",
      "Iteration  3, inertia 560676.705\n",
      "Iteration  4, inertia 551549.977\n",
      "Iteration  5, inertia 542724.980\n",
      "Iteration  6, inertia 536735.640\n",
      "Iteration  7, inertia 530771.770\n",
      "Iteration  8, inertia 523503.240\n",
      "Iteration  9, inertia 517367.704\n",
      "Iteration 10, inertia 513723.236\n",
      "Iteration 11, inertia 510880.340\n",
      "Iteration 12, inertia 508810.143\n",
      "Iteration 13, inertia 507414.907\n",
      "Iteration 14, inertia 505985.749\n",
      "Iteration 15, inertia 504978.234\n",
      "Iteration 16, inertia 503898.227\n",
      "Iteration 17, inertia 502667.265\n",
      "Iteration 18, inertia 501784.532\n",
      "Iteration 19, inertia 501171.386\n",
      "Iteration 20, inertia 500437.983\n",
      "Iteration 21, inertia 499206.950\n",
      "Iteration 22, inertia 496985.285\n",
      "Iteration 23, inertia 492703.207\n",
      "Iteration 24, inertia 490638.710\n",
      "Iteration 25, inertia 489087.922\n",
      "Iteration 26, inertia 487916.624\n",
      "Iteration 27, inertia 487595.276\n",
      "Iteration 28, inertia 487101.877\n",
      "Iteration 29, inertia 486253.945\n",
      "Iteration 30, inertia 486082.881\n",
      "Iteration 31, inertia 486010.198\n",
      "Iteration 32, inertia 485944.088\n",
      "Iteration 33, inertia 485924.174\n",
      "Iteration 34, inertia 485909.323\n",
      "Iteration 35, inertia 485902.911\n",
      "Iteration 36, inertia 485895.286\n",
      "Iteration 37, inertia 485891.900\n",
      "Iteration 38, inertia 485888.885\n",
      "Iteration 39, inertia 485887.338\n",
      "Iteration 40, inertia 485886.332\n",
      "Iteration 41, inertia 485886.086\n",
      "Iteration 42, inertia 485885.749\n",
      "Iteration 43, inertia 485885.653\n",
      "Iteration 44, inertia 485885.537\n",
      "Iteration 45, inertia 485885.140\n",
      "Iteration 46, inertia 485882.614\n",
      "Iteration 47, inertia 485878.762\n",
      "Iteration 48, inertia 485874.353\n",
      "Iteration 49, inertia 485872.789\n",
      "Iteration 50, inertia 485872.012\n",
      "Iteration 51, inertia 485870.646\n",
      "Iteration 52, inertia 485870.246\n",
      "Iteration 53, inertia 485870.025\n",
      "Iteration 54, inertia 485869.773\n",
      "Iteration 55, inertia 485869.258\n",
      "Iteration 56, inertia 485869.145\n",
      "Iteration 57, inertia 485869.097\n",
      "Converged at iteration 57: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 847119.000\n",
      "Iteration  1, inertia 582004.225\n",
      "Iteration  2, inertia 558536.701\n",
      "Iteration  3, inertia 541660.693\n",
      "Iteration  4, inertia 533301.193\n",
      "Iteration  5, inertia 525274.800\n",
      "Iteration  6, inertia 519766.844\n",
      "Iteration  7, inertia 516835.547\n",
      "Iteration  8, inertia 514504.322\n",
      "Iteration  9, inertia 512726.734\n",
      "Iteration 10, inertia 511301.485\n",
      "Iteration 11, inertia 510362.818\n",
      "Iteration 12, inertia 509097.894\n",
      "Iteration 13, inertia 507826.888\n",
      "Iteration 14, inertia 506976.730\n",
      "Iteration 15, inertia 506081.945\n",
      "Iteration 16, inertia 505100.215\n",
      "Iteration 17, inertia 504084.330\n",
      "Iteration 18, inertia 503184.729\n",
      "Iteration 19, inertia 502595.595\n",
      "Iteration 20, inertia 501637.151\n",
      "Iteration 21, inertia 498475.558\n",
      "Iteration 22, inertia 496885.851\n",
      "Iteration 23, inertia 495437.357\n",
      "Iteration 24, inertia 494521.929\n",
      "Iteration 25, inertia 493332.170\n",
      "Iteration 26, inertia 492588.710\n",
      "Iteration 27, inertia 492272.581\n",
      "Iteration 28, inertia 492062.119\n",
      "Iteration 29, inertia 491992.255\n",
      "Iteration 30, inertia 491935.921\n",
      "Iteration 31, inertia 491889.118\n",
      "Iteration 32, inertia 491835.675\n",
      "Iteration 33, inertia 491776.889\n",
      "Iteration 34, inertia 491657.078\n",
      "Iteration 35, inertia 491511.247\n",
      "Iteration 36, inertia 491364.996\n",
      "Iteration 37, inertia 490734.684\n",
      "Iteration 38, inertia 490575.055\n",
      "Iteration 39, inertia 490487.189\n",
      "Iteration 40, inertia 490459.352\n",
      "Iteration 41, inertia 490409.125\n",
      "Iteration 42, inertia 490398.061\n",
      "Iteration 43, inertia 490388.461\n",
      "Iteration 44, inertia 490381.727\n",
      "Iteration 45, inertia 490379.208\n",
      "Iteration 46, inertia 490376.903\n",
      "Iteration 47, inertia 490371.756\n",
      "Iteration 48, inertia 490367.458\n",
      "Iteration 49, inertia 490365.641\n",
      "Iteration 50, inertia 490363.824\n",
      "Iteration 51, inertia 490360.982\n",
      "Iteration 52, inertia 490358.138\n",
      "Iteration 53, inertia 490355.689\n",
      "Iteration 54, inertia 490352.829\n",
      "Iteration 55, inertia 490352.225\n",
      "Iteration 56, inertia 490351.338\n",
      "Iteration 57, inertia 490347.963\n",
      "Iteration 58, inertia 490345.724\n",
      "Iteration 59, inertia 490343.054\n",
      "Iteration 60, inertia 490340.771\n",
      "Iteration 61, inertia 490340.665\n",
      "Iteration 62, inertia 490340.633\n",
      "Iteration 63, inertia 490340.203\n",
      "Iteration 64, inertia 490340.159\n",
      "Iteration 65, inertia 490339.777\n",
      "Iteration 66, inertia 490339.427\n",
      "Iteration 67, inertia 490339.365\n",
      "Iteration 68, inertia 490339.339\n",
      "Converged at iteration 68: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 813769.000\n",
      "Iteration  1, inertia 600451.090\n",
      "Iteration  2, inertia 574477.979\n",
      "Iteration  3, inertia 558816.631\n",
      "Iteration  4, inertia 548714.732\n",
      "Iteration  5, inertia 539057.912\n",
      "Iteration  6, inertia 532537.632\n",
      "Iteration  7, inertia 524844.981\n",
      "Iteration  8, inertia 517261.588\n",
      "Iteration  9, inertia 511550.758\n",
      "Iteration 10, inertia 508776.895\n",
      "Iteration 11, inertia 506816.596\n",
      "Iteration 12, inertia 505507.696\n",
      "Iteration 13, inertia 504102.405\n",
      "Iteration 14, inertia 503246.724\n",
      "Iteration 15, inertia 502661.767\n",
      "Iteration 16, inertia 501920.823\n",
      "Iteration 17, inertia 501495.707\n",
      "Iteration 18, inertia 501342.183\n",
      "Iteration 19, inertia 501187.351\n",
      "Iteration 20, inertia 500996.717\n",
      "Iteration 21, inertia 500818.236\n",
      "Iteration 22, inertia 500546.835\n",
      "Iteration 23, inertia 500383.455\n",
      "Iteration 24, inertia 500298.011\n",
      "Iteration 25, inertia 500225.057\n",
      "Iteration 26, inertia 500126.570\n",
      "Iteration 27, inertia 499975.609\n",
      "Iteration 28, inertia 499632.242\n",
      "Iteration 29, inertia 499244.227\n",
      "Iteration 30, inertia 498321.822\n",
      "Iteration 31, inertia 495235.492\n",
      "Iteration 32, inertia 491924.964\n",
      "Iteration 33, inertia 489562.741\n",
      "Iteration 34, inertia 487269.474\n",
      "Iteration 35, inertia 486105.960\n",
      "Iteration 36, inertia 484956.677\n",
      "Iteration 37, inertia 484443.001\n",
      "Iteration 38, inertia 484101.913\n",
      "Iteration 39, inertia 483990.138\n",
      "Iteration 40, inertia 483903.012\n",
      "Iteration 41, inertia 483838.255\n",
      "Iteration 42, inertia 483802.721\n",
      "Iteration 43, inertia 483779.849\n",
      "Iteration 44, inertia 483771.694\n",
      "Iteration 45, inertia 483768.422\n",
      "Iteration 46, inertia 483763.534\n",
      "Iteration 47, inertia 483754.377\n",
      "Iteration 48, inertia 483752.940\n",
      "Iteration 49, inertia 483750.385\n",
      "Iteration 50, inertia 483744.530\n",
      "Iteration 51, inertia 483742.857\n",
      "Iteration 52, inertia 483741.276\n",
      "Iteration 53, inertia 483737.630\n",
      "Iteration 54, inertia 483736.729\n",
      "Iteration 55, inertia 483735.226\n",
      "Iteration 56, inertia 483735.180\n",
      "Iteration 57, inertia 483735.162\n",
      "Iteration 58, inertia 483735.133\n",
      "Converged at iteration 58: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 873222.000\n",
      "Iteration  1, inertia 622981.407\n",
      "Iteration  2, inertia 581344.807\n",
      "Iteration  3, inertia 553426.472\n",
      "Iteration  4, inertia 538207.651\n",
      "Iteration  5, inertia 528929.034\n",
      "Iteration  6, inertia 519135.771\n",
      "Iteration  7, inertia 512637.898\n",
      "Iteration  8, inertia 508928.733\n",
      "Iteration  9, inertia 506574.687\n",
      "Iteration 10, inertia 504419.386\n",
      "Iteration 11, inertia 502380.629\n",
      "Iteration 12, inertia 500775.594\n",
      "Iteration 13, inertia 499901.582\n",
      "Iteration 14, inertia 498929.359\n",
      "Iteration 15, inertia 497829.135\n",
      "Iteration 16, inertia 494591.010\n",
      "Iteration 17, inertia 493123.801\n",
      "Iteration 18, inertia 491412.513\n",
      "Iteration 19, inertia 490075.419\n",
      "Iteration 20, inertia 488817.483\n",
      "Iteration 21, inertia 488332.670\n",
      "Iteration 22, inertia 488052.653\n",
      "Iteration 23, inertia 487790.306\n",
      "Iteration 24, inertia 487552.390\n",
      "Iteration 25, inertia 486793.406\n",
      "Iteration 26, inertia 486530.800\n",
      "Iteration 27, inertia 486316.960\n",
      "Iteration 28, inertia 486091.202\n",
      "Iteration 29, inertia 485934.894\n",
      "Iteration 30, inertia 485840.070\n",
      "Iteration 31, inertia 485754.571\n",
      "Iteration 32, inertia 485648.867\n",
      "Iteration 33, inertia 485548.611\n",
      "Iteration 34, inertia 485486.096\n",
      "Iteration 35, inertia 485446.986\n",
      "Iteration 36, inertia 485399.716\n",
      "Iteration 37, inertia 485355.669\n",
      "Iteration 38, inertia 485321.471\n",
      "Iteration 39, inertia 485293.674\n",
      "Iteration 40, inertia 485261.282\n",
      "Iteration 41, inertia 485235.749\n",
      "Iteration 42, inertia 485222.212\n",
      "Iteration 43, inertia 485213.501\n",
      "Iteration 44, inertia 485200.106\n",
      "Iteration 45, inertia 485193.332\n",
      "Iteration 46, inertia 485188.848\n",
      "Iteration 47, inertia 485176.891\n",
      "Iteration 48, inertia 485169.487\n",
      "Iteration 49, inertia 485162.357\n",
      "Iteration 50, inertia 485147.230\n",
      "Iteration 51, inertia 485137.578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, inertia 485121.870\n",
      "Iteration 53, inertia 485115.583\n",
      "Iteration 54, inertia 485115.266\n",
      "Iteration 55, inertia 485114.998\n",
      "Iteration 56, inertia 485114.177\n",
      "Iteration 57, inertia 485111.940\n",
      "Iteration 58, inertia 485110.945\n",
      "Iteration 59, inertia 485110.721\n",
      "Iteration 60, inertia 485110.329\n",
      "Iteration 61, inertia 485110.220\n",
      "Iteration 62, inertia 485110.133\n",
      "Converged at iteration 62: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 778459.000\n",
      "Iteration  1, inertia 565825.682\n",
      "Iteration  2, inertia 548230.848\n",
      "Iteration  3, inertia 540141.154\n",
      "Iteration  4, inertia 532849.064\n",
      "Iteration  5, inertia 525235.682\n",
      "Iteration  6, inertia 519676.813\n",
      "Iteration  7, inertia 515076.117\n",
      "Iteration  8, inertia 511816.753\n",
      "Iteration  9, inertia 509646.462\n",
      "Iteration 10, inertia 507962.746\n",
      "Iteration 11, inertia 506707.426\n",
      "Iteration 12, inertia 505504.203\n",
      "Iteration 13, inertia 504101.653\n",
      "Iteration 14, inertia 503050.978\n",
      "Iteration 15, inertia 502023.647\n",
      "Iteration 16, inertia 500973.096\n",
      "Iteration 17, inertia 500059.519\n",
      "Iteration 18, inertia 499491.277\n",
      "Iteration 19, inertia 498465.272\n",
      "Iteration 20, inertia 494658.547\n",
      "Iteration 21, inertia 492963.438\n",
      "Iteration 22, inertia 491393.557\n",
      "Iteration 23, inertia 490005.070\n",
      "Iteration 24, inertia 488965.701\n",
      "Iteration 25, inertia 488513.032\n",
      "Iteration 26, inertia 488271.981\n",
      "Iteration 27, inertia 488102.226\n",
      "Iteration 28, inertia 487950.642\n",
      "Iteration 29, inertia 487715.881\n",
      "Iteration 30, inertia 487525.151\n",
      "Iteration 31, inertia 486851.058\n",
      "Iteration 32, inertia 486634.895\n",
      "Iteration 33, inertia 486496.802\n",
      "Iteration 34, inertia 486389.329\n",
      "Iteration 35, inertia 486321.993\n",
      "Iteration 36, inertia 486256.692\n",
      "Iteration 37, inertia 486187.309\n",
      "Iteration 38, inertia 486126.462\n",
      "Iteration 39, inertia 486075.247\n",
      "Iteration 40, inertia 486020.671\n",
      "Iteration 41, inertia 485964.931\n",
      "Iteration 42, inertia 485901.091\n",
      "Iteration 43, inertia 485880.049\n",
      "Iteration 44, inertia 485863.948\n",
      "Iteration 45, inertia 485847.315\n",
      "Iteration 46, inertia 485837.206\n",
      "Iteration 47, inertia 485829.410\n",
      "Iteration 48, inertia 485822.615\n",
      "Iteration 49, inertia 485813.957\n",
      "Iteration 50, inertia 485801.108\n",
      "Iteration 51, inertia 485786.068\n",
      "Iteration 52, inertia 485768.575\n",
      "Iteration 53, inertia 485753.316\n",
      "Iteration 54, inertia 485741.068\n",
      "Iteration 55, inertia 485730.442\n",
      "Iteration 56, inertia 485719.289\n",
      "Iteration 57, inertia 485709.269\n",
      "Iteration 58, inertia 485689.405\n",
      "Iteration 59, inertia 485662.225\n",
      "Iteration 60, inertia 485609.826\n",
      "Iteration 61, inertia 485562.435\n",
      "Iteration 62, inertia 485528.340\n",
      "Iteration 63, inertia 485466.843\n",
      "Iteration 64, inertia 485448.935\n",
      "Iteration 65, inertia 485432.952\n",
      "Iteration 66, inertia 485421.012\n",
      "Iteration 67, inertia 485414.736\n",
      "Iteration 68, inertia 485406.894\n",
      "Iteration 69, inertia 485398.763\n",
      "Iteration 70, inertia 485387.350\n",
      "Iteration 71, inertia 485290.350\n",
      "Iteration 72, inertia 485244.491\n",
      "Iteration 73, inertia 485170.543\n",
      "Iteration 74, inertia 485045.232\n",
      "Iteration 75, inertia 484958.741\n",
      "Iteration 76, inertia 484901.854\n",
      "Iteration 77, inertia 484816.301\n",
      "Iteration 78, inertia 484717.820\n",
      "Iteration 79, inertia 484589.444\n",
      "Iteration 80, inertia 484471.451\n",
      "Iteration 81, inertia 484301.483\n",
      "Iteration 82, inertia 484162.299\n",
      "Iteration 83, inertia 484031.576\n",
      "Iteration 84, inertia 483963.238\n",
      "Iteration 85, inertia 483902.981\n",
      "Iteration 86, inertia 483852.376\n",
      "Iteration 87, inertia 483813.725\n",
      "Iteration 88, inertia 483781.651\n",
      "Iteration 89, inertia 483770.779\n",
      "Iteration 90, inertia 483762.681\n",
      "Iteration 91, inertia 483761.702\n",
      "Iteration 92, inertia 483761.162\n",
      "Iteration 93, inertia 483760.175\n",
      "Iteration 94, inertia 483759.795\n",
      "Iteration 95, inertia 483759.412\n",
      "Iteration 96, inertia 483759.343\n",
      "Iteration 97, inertia 483759.310\n",
      "Converged at iteration 97: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 826523.000\n",
      "Iteration  1, inertia 545940.943\n",
      "Iteration  2, inertia 523856.275\n",
      "Iteration  3, inertia 513580.244\n",
      "Iteration  4, inertia 509365.899\n",
      "Iteration  5, inertia 506843.632\n",
      "Iteration  6, inertia 505144.046\n",
      "Iteration  7, inertia 503927.460\n",
      "Iteration  8, inertia 502311.492\n",
      "Iteration  9, inertia 501613.328\n",
      "Iteration 10, inertia 500884.926\n",
      "Iteration 11, inertia 500247.240\n",
      "Iteration 12, inertia 499488.136\n",
      "Iteration 13, inertia 498443.407\n",
      "Iteration 14, inertia 495355.974\n",
      "Iteration 15, inertia 492111.988\n",
      "Iteration 16, inertia 490326.735\n",
      "Iteration 17, inertia 488534.639\n",
      "Iteration 18, inertia 487284.570\n",
      "Iteration 19, inertia 486777.428\n",
      "Iteration 20, inertia 486384.347\n",
      "Iteration 21, inertia 486089.960\n",
      "Iteration 22, inertia 485382.882\n",
      "Iteration 23, inertia 484980.332\n",
      "Iteration 24, inertia 484686.287\n",
      "Iteration 25, inertia 484465.905\n",
      "Iteration 26, inertia 484250.063\n",
      "Iteration 27, inertia 484073.242\n",
      "Iteration 28, inertia 483914.387\n",
      "Iteration 29, inertia 483868.500\n",
      "Iteration 30, inertia 483854.150\n",
      "Iteration 31, inertia 483845.820\n",
      "Iteration 32, inertia 483840.695\n",
      "Iteration 33, inertia 483829.260\n",
      "Iteration 34, inertia 483811.213\n",
      "Iteration 35, inertia 483792.148\n",
      "Iteration 36, inertia 483780.946\n",
      "Iteration 37, inertia 483771.190\n",
      "Iteration 38, inertia 483768.023\n",
      "Iteration 39, inertia 483764.139\n",
      "Iteration 40, inertia 483763.321\n",
      "Iteration 41, inertia 483762.294\n",
      "Iteration 42, inertia 483761.237\n",
      "Iteration 43, inertia 483761.119\n",
      "Converged at iteration 43: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 836483.000\n",
      "Iteration  1, inertia 606350.359\n",
      "Iteration  2, inertia 569667.283\n",
      "Iteration  3, inertia 553181.694\n",
      "Iteration  4, inertia 540695.626\n",
      "Iteration  5, inertia 532305.102\n",
      "Iteration  6, inertia 522709.158\n",
      "Iteration  7, inertia 514124.886\n",
      "Iteration  8, inertia 509266.813\n",
      "Iteration  9, inertia 506560.450\n",
      "Iteration 10, inertia 504519.180\n",
      "Iteration 11, inertia 502364.176\n",
      "Iteration 12, inertia 500141.615\n",
      "Iteration 13, inertia 498836.793\n",
      "Iteration 14, inertia 497917.333\n",
      "Iteration 15, inertia 496776.854\n",
      "Iteration 16, inertia 494274.952\n",
      "Iteration 17, inertia 491505.805\n",
      "Iteration 18, inertia 489420.460\n",
      "Iteration 19, inertia 488177.825\n",
      "Iteration 20, inertia 487277.471\n",
      "Iteration 21, inertia 486944.180\n",
      "Iteration 22, inertia 486750.369\n",
      "Iteration 23, inertia 486494.830\n",
      "Iteration 24, inertia 486348.023\n",
      "Iteration 25, inertia 485756.741\n",
      "Iteration 26, inertia 485644.247\n",
      "Iteration 27, inertia 485542.709\n",
      "Iteration 28, inertia 485484.716\n",
      "Iteration 29, inertia 485466.097\n",
      "Iteration 30, inertia 485440.147\n",
      "Iteration 31, inertia 485405.477\n",
      "Iteration 32, inertia 485355.687\n",
      "Iteration 33, inertia 485326.430\n",
      "Iteration 34, inertia 485300.691\n",
      "Iteration 35, inertia 485277.953\n",
      "Iteration 36, inertia 485259.718\n",
      "Iteration 37, inertia 485216.942\n",
      "Iteration 38, inertia 485199.199\n",
      "Iteration 39, inertia 485187.141\n",
      "Iteration 40, inertia 485177.098\n",
      "Iteration 41, inertia 485160.196\n",
      "Iteration 42, inertia 485147.653\n",
      "Iteration 43, inertia 485134.832\n",
      "Iteration 44, inertia 485123.582\n",
      "Iteration 45, inertia 485118.859\n",
      "Iteration 46, inertia 485117.508\n",
      "Iteration 47, inertia 485117.484\n",
      "Iteration 48, inertia 485117.467\n",
      "Converged at iteration 48: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 801549.000\n",
      "Iteration  1, inertia 530970.020\n",
      "Iteration  2, inertia 510628.710\n",
      "Iteration  3, inertia 506646.401\n",
      "Iteration  4, inertia 504696.167\n",
      "Iteration  5, inertia 503583.166\n",
      "Iteration  6, inertia 502500.161\n",
      "Iteration  7, inertia 501539.865\n",
      "Iteration  8, inertia 499806.882\n",
      "Iteration  9, inertia 497060.316\n",
      "Iteration 10, inertia 493903.691\n",
      "Iteration 11, inertia 490848.149\n",
      "Iteration 12, inertia 488929.794\n",
      "Iteration 13, inertia 487914.181\n",
      "Iteration 14, inertia 487714.425\n",
      "Iteration 15, inertia 487137.724\n",
      "Iteration 16, inertia 487034.795\n",
      "Iteration 17, inertia 486935.279\n",
      "Iteration 18, inertia 486874.810\n",
      "Iteration 19, inertia 486862.469\n",
      "Iteration 20, inertia 486860.597\n",
      "Iteration 21, inertia 486859.734\n",
      "Iteration 22, inertia 486859.053\n",
      "Iteration 23, inertia 486858.894\n",
      "Iteration 24, inertia 486858.839\n",
      "Iteration 25, inertia 486858.819\n",
      "Iteration 26, inertia 486858.810\n",
      "Converged at iteration 26: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 802811.000\n",
      "Iteration  1, inertia 555626.642\n",
      "Iteration  2, inertia 538969.663\n",
      "Iteration  3, inertia 530159.855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  4, inertia 521592.289\n",
      "Iteration  5, inertia 514923.090\n",
      "Iteration  6, inertia 510727.567\n",
      "Iteration  7, inertia 508216.084\n",
      "Iteration  8, inertia 506053.201\n",
      "Iteration  9, inertia 504089.313\n",
      "Iteration 10, inertia 502183.846\n",
      "Iteration 11, inertia 500409.994\n",
      "Iteration 12, inertia 499635.937\n",
      "Iteration 13, inertia 498930.218\n",
      "Iteration 14, inertia 498226.481\n",
      "Iteration 15, inertia 497160.984\n",
      "Iteration 16, inertia 493902.761\n",
      "Iteration 17, inertia 492372.966\n",
      "Iteration 18, inertia 490814.984\n",
      "Iteration 19, inertia 489673.175\n",
      "Iteration 20, inertia 488229.334\n",
      "Iteration 21, inertia 487613.796\n",
      "Iteration 22, inertia 487274.247\n",
      "Iteration 23, inertia 487015.645\n",
      "Iteration 24, inertia 486809.031\n",
      "Iteration 25, inertia 486651.478\n",
      "Iteration 26, inertia 485950.984\n",
      "Iteration 27, inertia 485808.690\n",
      "Iteration 28, inertia 485755.276\n",
      "Iteration 29, inertia 485703.549\n",
      "Iteration 30, inertia 485681.679\n",
      "Iteration 31, inertia 485657.670\n",
      "Iteration 32, inertia 485616.249\n",
      "Iteration 33, inertia 485570.428\n",
      "Iteration 34, inertia 485512.950\n",
      "Iteration 35, inertia 485489.554\n",
      "Iteration 36, inertia 485470.718\n",
      "Iteration 37, inertia 485451.122\n",
      "Iteration 38, inertia 485437.662\n",
      "Iteration 39, inertia 485418.992\n",
      "Iteration 40, inertia 485399.753\n",
      "Iteration 41, inertia 485379.243\n",
      "Iteration 42, inertia 485348.754\n",
      "Iteration 43, inertia 485314.982\n",
      "Iteration 44, inertia 485272.106\n",
      "Iteration 45, inertia 485243.501\n",
      "Iteration 46, inertia 485227.965\n",
      "Iteration 47, inertia 485208.920\n",
      "Iteration 48, inertia 485198.699\n",
      "Iteration 49, inertia 485193.200\n",
      "Iteration 50, inertia 485187.991\n",
      "Iteration 51, inertia 485183.802\n",
      "Iteration 52, inertia 485176.490\n",
      "Iteration 53, inertia 485171.747\n",
      "Iteration 54, inertia 485156.983\n",
      "Iteration 55, inertia 485131.329\n",
      "Iteration 56, inertia 485120.660\n",
      "Iteration 57, inertia 485115.841\n",
      "Iteration 58, inertia 485115.688\n",
      "Iteration 59, inertia 485115.670\n",
      "Converged at iteration 59: center shift 0.000000e+00 within tolerance 5.531234e-07\n",
      "Initialization complete\n",
      "Iteration  0, inertia 884825.000\n",
      "Iteration  1, inertia 619916.842\n",
      "Iteration  2, inertia 572289.752\n",
      "Iteration  3, inertia 557505.507\n",
      "Iteration  4, inertia 548983.266\n",
      "Iteration  5, inertia 541018.148\n",
      "Iteration  6, inertia 534886.641\n",
      "Iteration  7, inertia 529060.070\n",
      "Iteration  8, inertia 522871.841\n",
      "Iteration  9, inertia 517919.314\n",
      "Iteration 10, inertia 512550.703\n",
      "Iteration 11, inertia 509356.245\n",
      "Iteration 12, inertia 507349.701\n",
      "Iteration 13, inertia 505396.725\n",
      "Iteration 14, inertia 503247.220\n",
      "Iteration 15, inertia 501180.720\n",
      "Iteration 16, inertia 499702.107\n",
      "Iteration 17, inertia 498859.409\n",
      "Iteration 18, inertia 497629.012\n",
      "Iteration 19, inertia 494210.747\n",
      "Iteration 20, inertia 492447.364\n",
      "Iteration 21, inertia 490849.086\n",
      "Iteration 22, inertia 489394.601\n",
      "Iteration 23, inertia 488171.673\n",
      "Iteration 24, inertia 487902.608\n",
      "Iteration 25, inertia 487654.454\n",
      "Iteration 26, inertia 487379.366\n",
      "Iteration 27, inertia 487155.658\n",
      "Iteration 28, inertia 486462.775\n",
      "Iteration 29, inertia 486338.751\n",
      "Iteration 30, inertia 486228.794\n",
      "Iteration 31, inertia 486164.852\n",
      "Iteration 32, inertia 486102.885\n",
      "Iteration 33, inertia 486050.216\n",
      "Iteration 34, inertia 486009.572\n",
      "Iteration 35, inertia 485966.123\n",
      "Iteration 36, inertia 485928.615\n",
      "Iteration 37, inertia 485879.195\n",
      "Iteration 38, inertia 485866.585\n",
      "Iteration 39, inertia 485855.094\n",
      "Iteration 40, inertia 485847.211\n",
      "Iteration 41, inertia 485840.280\n",
      "Iteration 42, inertia 485834.796\n",
      "Iteration 43, inertia 485830.387\n",
      "Iteration 44, inertia 485824.667\n",
      "Iteration 45, inertia 485815.716\n",
      "Iteration 46, inertia 485806.003\n",
      "Iteration 47, inertia 485796.369\n",
      "Iteration 48, inertia 485783.395\n",
      "Iteration 49, inertia 485768.599\n",
      "Iteration 50, inertia 485754.931\n",
      "Iteration 51, inertia 485740.281\n",
      "Iteration 52, inertia 485730.975\n",
      "Iteration 53, inertia 485720.704\n",
      "Iteration 54, inertia 485710.370\n",
      "Iteration 55, inertia 485691.235\n",
      "Iteration 56, inertia 485663.619\n",
      "Iteration 57, inertia 485613.073\n",
      "Iteration 58, inertia 485563.846\n",
      "Iteration 59, inertia 485528.316\n",
      "Iteration 60, inertia 485466.892\n",
      "Iteration 61, inertia 485448.892\n",
      "Iteration 62, inertia 485432.111\n",
      "Iteration 63, inertia 485419.836\n",
      "Iteration 64, inertia 485412.741\n",
      "Iteration 65, inertia 485404.621\n",
      "Iteration 66, inertia 485397.601\n",
      "Iteration 67, inertia 485386.418\n",
      "Iteration 68, inertia 485286.395\n",
      "Iteration 69, inertia 485240.099\n",
      "Iteration 70, inertia 485170.376\n",
      "Iteration 71, inertia 485045.128\n",
      "Iteration 72, inertia 484958.782\n",
      "Iteration 73, inertia 484901.983\n",
      "Iteration 74, inertia 484816.620\n",
      "Iteration 75, inertia 484718.422\n",
      "Iteration 76, inertia 484589.531\n",
      "Iteration 77, inertia 484471.560\n",
      "Iteration 78, inertia 484301.483\n",
      "Iteration 79, inertia 484162.299\n",
      "Iteration 80, inertia 484031.576\n",
      "Iteration 81, inertia 483963.238\n",
      "Iteration 82, inertia 483902.981\n",
      "Iteration 83, inertia 483852.376\n",
      "Iteration 84, inertia 483813.725\n",
      "Iteration 85, inertia 483781.651\n",
      "Iteration 86, inertia 483770.779\n",
      "Iteration 87, inertia 483762.681\n",
      "Iteration 88, inertia 483761.702\n",
      "Iteration 89, inertia 483761.162\n",
      "Iteration 90, inertia 483760.175\n",
      "Iteration 91, inertia 483759.795\n",
      "Iteration 92, inertia 483759.412\n",
      "Iteration 93, inertia 483759.343\n",
      "Iteration 94, inertia 483759.310\n",
      "Converged at iteration 94: center shift 0.000000e+00 within tolerance 5.531234e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "    n_clusters=8, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=10)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 8\n",
    "kmeans = KMeans(n_clusters=num_cluster, init='random', verbose=10)\n",
    "kmeans.fit(tf_document_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obt13q2h85su"
   },
   "source": [
    "We should now have a clustering with K clusters. Let's examine the centroids. Recall that they are not documents; they represent a typical example (average) document in the cluster.\n",
    "\n",
    "We'll print out the top 10 terms from each of the centroids.\n",
    "\n",
    "NOTE: ``vectorizer`` below should be replaced with the name of your TFIDFVectorizor used to produce the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMZQiQtddm4s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " be\n",
      " i\n",
      " the\n",
      " a\n",
      " you\n",
      " to\n",
      " it\n",
      " and\n",
      " that\n",
      " of\n",
      "\n",
      "Cluster 1:\n",
      " i\n",
      " be\n",
      " a\n",
      " the\n",
      " to\n",
      " and\n",
      " it\n",
      " my\n",
      " that\n",
      " have\n",
      "\n",
      "Cluster 2:\n",
      " the\n",
      " be\n",
      " to\n",
      " a\n",
      " i\n",
      " and\n",
      " of\n",
      " it\n",
      " that\n",
      " in\n",
      "\n",
      "Cluster 3:\n",
      " the\n",
      " be\n",
      " and\n",
      " i\n",
      " to\n",
      " a\n",
      " of\n",
      " in\n",
      " have\n",
      " that\n",
      "\n",
      "Cluster 4:\n",
      " i\n",
      " be\n",
      " a\n",
      " the\n",
      " to\n",
      " and\n",
      " of\n",
      " have\n",
      " it\n",
      " that\n",
      "\n",
      "Cluster 5:\n",
      " the\n",
      " be\n",
      " a\n",
      " to\n",
      " and\n",
      " of\n",
      " you\n",
      " it\n",
      " i\n",
      " that\n",
      "\n",
      "Cluster 6:\n",
      " the\n",
      " be\n",
      " a\n",
      " to\n",
      " and\n",
      " of\n",
      " i\n",
      " in\n",
      " you\n",
      " it\n",
      "\n",
      "Cluster 7:\n",
      " i\n",
      " be\n",
      " a\n",
      " the\n",
      " to\n",
      " and\n",
      " it\n",
      " you\n",
      " of\n",
      " that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tf_vectorizer.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "  print(\"Cluster %d:\" % i)\n",
    "  for ind in order_centroids[i, :10]:\n",
    "    print(' %s' % terms[ind])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e8f0AE1Y9_-G"
   },
   "source": [
    "We can also look at the cluster assignments.  Each post is assigned to one cluster (partioning the document space). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTkj56Tzklk3"
   },
   "outputs": [],
   "source": [
    "# Group the posts by their cluster labels.\n",
    "clustering = collections.defaultdict(list)\n",
    "for idx, label in enumerate(kmeans.labels_):\n",
    "  clustering[label].append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcwzuz7vlvS5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster: 2  Num posts:  39\n",
      "0 https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/ 4/7/13  7/27/12  http://www.imdb.com/title/tt0073440/referenceIt was only a..\n",
      "65 https://www.reddit.com/r/2007scape/comments/1uz5j7/training_or_slayer/ Lawl I seriously hate this argument. Probably nobody will see this outside ..\n",
      "566 https://www.reddit.com/r/911truth/comments/3as2wh/are_there_people_who_still_believe_911_unfolded/ The real problem is not that there are people who believe everything the go..\n",
      "567 https://www.reddit.com/r/911truth/comments/3as2wh/are_there_people_who_still_believe_911_unfolded/ It's the farthest thing from \"self evident\", the officially sanctioned and ..\n",
      "643 https://www.reddit.com/r/AMA/comments/4ee3q7/im_chrisacrosstheworld_japans_first_salaried/ 1. What are your dreams and aspiration.I have so many dreams!! I made a buc..\n",
      "885 https://www.reddit.com/r/Advice/comments/4bfbxd/jealousy_issues/ My boyfriend and I work for the same company (yes I know this can cause tro..\n",
      "1014 https://www.reddit.com/r/AlanWatts/comments/4dtbqj/i_just_read_my_favorite_alan_watts_book_and_it/ I think it glorifies selfishness too much.All the \"going with the flow\" typ..\n",
      "1112 https://www.reddit.com/r/Amsterdam/comments/4d63kb/potential_scam_asking_for_a_scanned_passport/ Hello again guys,I'm moving to Amsterdam on the 6th of June and I'm current..\n",
      "1339 https://www.reddit.com/r/AndroidGaming/comments/221r3t/ask_redditwhat_makes_a_game_addicting/ easy to learn/hard to master, is great. i agree with you there. some sense ..\n",
      "1731 https://www.reddit.com/r/AskBattlestations/comments/1ywx3e/im_a_noob/ Alright. My time to shine. Mousepad depends on the mouse you have but I gen..\n",
      "1782 https://www.reddit.com/r/AskEngineers/comments/128tqk/hey_askengineers_can_you_give_me_some_hope_what/ As for myself? I work next door to [the biggest building in the world by vo..\n",
      "\n",
      "Cluster: 7  Num posts:  2559\n",
      "1 https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/ I've wanted to watch this for a long time. I was also turned off by the cou..\n",
      "3 https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/ Yeah, I've always heard that Altman was famous for his ensemble casts. But ..\n",
      "7 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ Thanks man! Yeah I'm trying to just keep the \"who cares have fun\" attitude ..\n",
      "18 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ I can't find them anywhere! I just want the most basic model they have such..\n",
      "20 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ Where do you live? Eagle Guns and Range in Concord, NC has one in stock. Wa..\n",
      "21 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ I saw several new ones yesterday, including a Range Officer at Piazza Jewel..\n",
      "29 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ Yea I really want something with a forged frame to build on. I'm pretty set..\n",
      "34 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ That's the springfield equivalent to the Remington R1.Unrelated: how did yo..\n",
      "35 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ Click on the \"formatting help\" at the bottom right of the comment box. It w..\n",
      "40 https://www.reddit.com/r/1911/comments/35igzp/help_with_a_possible_trade/ I've got nothing against Kimbers. As stated above they can be fine 1911s. I..\n",
      "42 https://www.reddit.com/r/1911/comments/35igzp/help_with_a_possible_trade/ You won't regret keeping it. As far as longevity goes, it is almost indefin..\n",
      "\n",
      "Cluster: 5  Num posts:  797\n",
      "2 https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/ You strike me as the type who would appreciate it. I would give it a go. Th..\n",
      "15 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ #####&#009;######&#009;####&#009; [**Digestive biscuit**](https://en.wikipe..\n",
      "30 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ then I believe the R1 model is what you are looking for.Fixed Sights, Full ..\n",
      "39 https://www.reddit.com/r/1911/comments/35igzp/help_with_a_possible_trade/ Thanks for the input! I sort of share the same sentiment as you. Just wante..\n",
      "76 https://www.reddit.com/r/2007scape/comments/25hor7/new_skill/ To be honest, there aren't really too many ideas for a skill that are so in..\n",
      "86 https://www.reddit.com/r/2007scape/comments/27aan0/recording_osrs_with_frapsalternative_need_help/ use XSplit or OBS.  Fraps requires the target game to use a hardware accele..\n",
      "90 https://www.reddit.com/r/2007scape/comments/27aan0/recording_osrs_with_frapsalternative_need_help/ Thanks everyone for the help :)I will try out the different softwares you a..\n",
      "101 https://www.reddit.com/r/2007scape/comments/2fu170/how_to_win_goblin_invasion/ There's no reward as yet, but a confetti shooting bazooka similar to the on..\n",
      "105 https://www.reddit.com/r/2007scape/comments/2hdp7c/since_divine_didnt_pass_theres_other_options/ i personally think the other shield should be the range equivalent of the a..\n",
      "166 https://www.reddit.com/r/2007scape/comments/3bxzpg/corp_ffa/ Depending on the size, range *can* be better than melee. In a very large gr..\n",
      "169 https://www.reddit.com/r/2007scape/comments/3cle3y/old_account/ Is it even possible to recover an account from 2005 that was never a member..\n",
      "\n",
      "Cluster: 4  Num posts:  113\n",
      "4 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ **January 16th 3 Sets:** Went out shopping with my grandma as I visited her..\n",
      "69 https://www.reddit.com/r/2007scape/comments/20pv9h/never_give_up/ This morning I got my first RS account back after I had changed (and forgot..\n",
      "81 https://www.reddit.com/r/2007scape/comments/27aan0/recording_osrs_with_frapsalternative_need_help/ So I have a YouTube channel with 40,000+ subscribers and at the moment are ..\n",
      "153 https://www.reddit.com/r/2007scape/comments/3422rx/how_should_i_best_optimize_my_barrows_runs/ Hey guys, I'm relatively new to OSRS members (I primarily played f2p rs2) a..\n",
      "157 https://www.reddit.com/r/2007scape/comments/3422rx/how_should_i_best_optimize_my_barrows_runs/ No problem man, thanks for the detailed reply! Yep, I use the minigame tele..\n",
      "270 https://www.reddit.com/r/24hoursupport/comments/11mxeb/problems_connecting_ps3_to_lcd_monitor_via_hmdi/ Apologies in advance for the long post, just trying to include as much rele..\n",
      "443 https://www.reddit.com/r/40something/comments/351n7m/hobbies/ When my ex and I split up I really got into playing Left 4 Dead (OK, I was ..\n",
      "447 https://www.reddit.com/r/40something/comments/351n7m/hobbies/ Good on you for finding a hobby. Find a couple. They will enrich your life...\n",
      "454 https://www.reddit.com/r/40something/comments/351n7m/hobbies/ LOL, I knew I shouldn't have written that! Nothing \"exciting\". But definite..\n",
      "499 https://www.reddit.com/r/4hourbodyslowcarb/comments/2ps9b0/shifting_cheat_day_to_accommodate_social_events/ Hi Slow Carbers! I'm a newbie to the slow carb diet, I just started last Su..\n",
      "556 https://www.reddit.com/r/7kglobal/comments/4765ki/i_got_evan_at_34_do_i_need_rudy/ Its all about balance. If you got Rudy as first 7k then all good build a Ru..\n",
      "\n",
      "Cluster: 0  Num posts:  5841\n",
      "5 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ grandmas are the best wingmen.\n",
      "6 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ dude, these sets are awesome. You're doing great. Sounds like you're a natu..\n",
      "9 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ More updates!\n",
      "13 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ /r/keto *****[^report ^a ^**problem**](http://reddit.com/r/LinkFixerBotSnr)..\n",
      "16 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ Looks delicious, thanks for sharing!\n",
      "17 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ Looks tasty!\n",
      "19 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ You want your 1911 to be in .38?\n",
      "22 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ lots of pawn shops around me, Augusta GA, have 1911 .45 ACP on the shelves.\n",
      "23 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ I want this one for myself but here you go. http://www.impactguns.com/produ..\n",
      "24 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ Oh hell no. Sorry, I linked to the wrong pistol. I want that but in .45 ACP..\n",
      "25 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ Hahaha. Ok. I was wondering. \n",
      "\n",
      "Cluster: 1  Num posts:  426\n",
      "8 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ Ok. Update! Sorry I haven't been doing this day to day like I should be. Le..\n",
      "10 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ I love cheese cake! I love both making and eating it, so I'm sad to see tha..\n",
      "11 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ >very specific items that aren't available everywhereI find this funny, bec..\n",
      "12 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ I know,  but I figured it would be easier to find equivilants to those item..\n",
      "14 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ Oh, we have gelatin, but I've never seen it in sheet form.  From what I've ..\n",
      "37 https://www.reddit.com/r/1911/comments/35igzp/help_with_a_possible_trade/ Hey there, I just discovered this sub so this is my first post. I recently ..\n",
      "67 https://www.reddit.com/r/2007scape/comments/1uz5j7/training_or_slayer/ Oh certainly. I wouldn't be an advocate of anything that makes the game bor..\n",
      "84 https://www.reddit.com/r/2007scape/comments/27aan0/recording_osrs_with_frapsalternative_need_help/ I haven't had any luck with Fraps, and I don't like using Camtasia. Fraps m..\n",
      "131 https://www.reddit.com/r/2007scape/comments/2x3amn/can_i_use_this_ahk_script/ Im gonna piggyride this thread a bit.after seeing pen+fan setup to auto alc..\n",
      "156 https://www.reddit.com/r/2007scape/comments/3422rx/how_should_i_best_optimize_my_barrows_runs/ I assume you use tele to shades of morton? if so then 5-6 chests is enough ..\n",
      "210 https://www.reddit.com/r/2007scape/comments/4024ia/rs_authenticator/ so i got a new phone since my old one died. Tried to log onto rs and it ask..\n",
      "\n",
      "Cluster: 6  Num posts:  218\n",
      "38 https://www.reddit.com/r/1911/comments/35igzp/help_with_a_possible_trade/ \"Going from a Colt to a Kimber is a bad move.\" Well... Kinda. I wouldn't ca..\n",
      "94 https://www.reddit.com/r/2007scape/comments/289m0u/i_played_2007scape_when_it_was_first_released/ There's been a ton of stuff, most notably has been the wilderness revamp wh..\n",
      "102 https://www.reddit.com/r/2007scape/comments/2hdp7c/since_divine_didnt_pass_theres_other_options/ Okay since then divine didn't and i a new item that can be possibly nice an..\n",
      "346 https://www.reddit.com/r/3DS/comments/2v2t04/help_with_3ds_sd_card_problems/ Hey everybody. So I recently ran out of room on my original 3DS SD card and..\n",
      "404 https://www.reddit.com/r/3d6/comments/41cgzk/dd5eneed_help_making_a_halfling_barbarian/ You're small-sized, so you won't be able to use heavy weapons the way most ..\n",
      "409 https://www.reddit.com/r/3d6/comments/41cgzk/dd5eneed_help_making_a_halfling_barbarian/ Halflings and Gnomes can't wield weapons with the 'heavy' prefix without di..\n",
      "422 https://www.reddit.com/r/40kLore/comments/3au23g/a_human_in_taus_clothing/ There are two sides to this coin.   In one of the last chancers books they ..\n",
      "560 https://www.reddit.com/r/90sPunk/comments/2q0xzl/i_think_i_known_the_band_but_cant_find_the/ LastFM confirms the existence of \"Buttplug\" by 30 foot fall, but I can't fi..\n",
      "562 https://www.reddit.com/r/911truth/comments/3as2wh/are_there_people_who_still_believe_911_unfolded/ There is still debate about 911. Some people hold believes that are not sup..\n",
      "577 https://www.reddit.com/r/ABraThatFits/comments/2qgrke/im_desperate_and_need_help_finding_comfortable/ There are many wireless options available in 42DD, but before you decide to..\n",
      "672 https://www.reddit.com/r/APStudents/comments/4fg75l/struggling_student_looking_for_apush_review_book/ I read through the five steps to a five APUSH book and watch most of John G..\n",
      "\n",
      "Cluster: 3  Num posts:  7\n",
      "2667 https://www.reddit.com/r/AskReddit/comments/1dc1zo/things_you_heard_while_at_the_hospital_you_wish/ As we all know the hospital is not a fun place, many times you overhear sto..\n",
      "7556 https://www.reddit.com/r/Assistance/comments/x245j/im_a_21_year_old_community_college_student_owe/ (Pre-read):  By way of proof of the following, I have a whole bucket full o..\n",
      "7692 https://www.reddit.com/r/Austria/comments/16372d/where_to_live_in_vienna_descriptions_of_the/ First of all, we have to distinguish between the Inner City, the Inner Dist..\n",
      "8233 https://www.reddit.com/r/BibleUnbiased/comments/4k32so/contraception_not_allowed/ Ok I'll give it a try. But I don't want to have to defend myself to anyone ..\n",
      "9087 https://www.reddit.com/r/CFB/comments/133duv/official_rcfb_poll_week_11_rankings/ http://i.imgur.com/cLCN9.png------The Top 25:Kansas State moves to #1.  The..\n",
      "9566 https://www.reddit.com/r/China/comments/1r5t9y/need_some_advice_grandfather_in_china_seems_very/ (Was just suggested to post here for input, as well.  Thanks in advance for..\n",
      "9975 https://www.reddit.com/r/Civcraft/comments/34w7sy/my_involvement_in_the_titan_war_full/ This is a barebones, off the top of my head, breakdown of my full involveme..\n"
     ]
    }
   ],
   "source": [
    "for cluster, indices in clustering.items():\n",
    "  print(\"\\nCluster:\", cluster, \" Num posts: \", len(indices))\n",
    "  cur_docs = 0\n",
    "  for index in indices:\n",
    "    if (cur_docs > 10):\n",
    "      break\n",
    "    post_contents = post_vals[index].replace('\\n', '')\n",
    "    print(index, post_keys[index], (post_contents[:75] + '..') if len(post_contents) > 75 else post_contents)\n",
    "    cur_docs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKLaVZHMAF52"
   },
   "source": [
    "\n",
    "\n",
    "*   Is the clustering useful to explore the data?\n",
    "*   Can you label the clusters?\n",
    "*   Are these 'good' clusters?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7K2nTPzlqjfs"
   },
   "source": [
    "#### Optional task : Creating a better clustering\n",
    "\n",
    "Create a better clustering than the one above.  \n",
    " - Use MiniBatchKMeans instead of kmeans (you might start with a batch size of 500 or so).\n",
    " - Try using kmeans++ instead of random\n",
    " - Vary the number of clusters (k)\n",
    " - Plot the sum of distances of samples to their closest cluster center (see the kmeans.inertia_ value) for different values of K. You may also try the silhouette score mentioned in lecture - sklearn.metrics.silhouette_score.  See also  https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    " - What is a 'good value'  of K for this data?\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "daaIS8XJPEHP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rW03I8Kn0KCq"
   },
   "source": [
    "## Summary \n",
    "\n",
    "In this lab we covered a lot of practical ground:\n",
    "- We introduced spaCy for text processing\n",
    "- We implemented a dictionary and created a one-hot encoding of text\n",
    "- Implemented the Jaccard similarity function\n",
    "- Used Sci-kit Learn to vectorize text using bag-of-words representation with TF and TF-IDF weights\n",
    "- Ran KMeans clustering on Reddit data and used it to explore data. \n",
    "\n",
    "\n",
    "Next time we'll perform more advanced language prediction tasks focusing on modeling sequences of text.\n",
    "\n",
    "Please take the [Lab 2 Moodle Feedback quiz](https://moodle.gla.ac.uk/mod/feedback/view.php?id=1110531). "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TAD2019 Lab 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
