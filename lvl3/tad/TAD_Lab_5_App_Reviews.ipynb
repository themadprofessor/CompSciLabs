{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cl3cfQa7qdSR"
   },
   "source": [
    "# Lab 5: Sentiment classification\n",
    "\n",
    "In this week's lab, we perform supervised text classifcation. You will see:\n",
    " * How to perform text classification using Sklearn, specifically sentiment classification\n",
    " * How to evaluate a classifier with key metrics including Precision, Recall, F1, and confusion matrix\n",
    " * The impact of imbalanced class distributions\n",
    " * How to use FeatureUnion and Pipelines to incorporate multiple features\n",
    " * How to incorporate different types of features (sparse and dense)\n",
    " * How to use GridSearchCV to tune parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9eLqycD_5dF"
   },
   "source": [
    "Our dataset this week comes from reviews for Android apps on the Play Store. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArN6SGUFqlWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 91.0M  100 91.0M    0     0  1048k      0  0:01:28  0:01:28 --:--:-- 1459k00:19 1458k\n"
     ]
    }
   ],
   "source": [
    "local_file = \"reviews_Apps_for_Android_5.json.gz\"\n",
    "#!curl -o  $local_file https://storage.googleapis.com/tad2018/reviews_Apps_for_Android_5.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zZjyjvM_AFh"
   },
   "source": [
    "We will limit the number of reviews to make it smaller so that the lab is faster to complete. You may remove the limit if you desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMfcZ4nFq-Zn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 752937 reviews in our dataset\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "review_list = list()\n",
    "\n",
    "# Construct a dataframe, by opening the JSON file line-by-line\n",
    "with gzip.open(local_file) as jsonfile:\n",
    "  for i, line in enumerate(jsonfile):\n",
    "    review = json.loads(line)\n",
    "    #print(review)\n",
    "    #if (i >= review_limit): break\n",
    "    # asin is the product number, overall is the number of stars awarded by the user for that product\n",
    "    review_list.append( (review['asin'], review['reviewerID'], review['reviewText'], review['summary'], review['overall']))\n",
    "                   \n",
    "print(\"We have %d reviews in our dataset\"  % len(review_list))\n",
    "\n",
    "collabels = ['productId', 'reviewerID', 'reviewText', 'summary', 'overall']\n",
    "reviews = pd.DataFrame(review_list, columns=collabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Uoo02mDV-a"
   },
   "source": [
    "Let's explore the data before we jump into classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YwRH2wXD5eP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A1N4O8VOJZTDVB</td>\n",
       "      <td>Loves the song, so he really couldn't wait to ...</td>\n",
       "      <td>Really cute</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A2HQWU6HUKIEC7</td>\n",
       "      <td>Oh, how my little grandson loves this app. He'...</td>\n",
       "      <td>2-year-old loves it</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A1SXASF6GYG96I</td>\n",
       "      <td>I found this at a perfect time since my daught...</td>\n",
       "      <td>Fun game</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A2B54P9ZDYH167</td>\n",
       "      <td>My 1 year old goes back to this game over and ...</td>\n",
       "      <td>We love our Monkeys!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>AFOFZDTX5UC6D</td>\n",
       "      <td>There are three different versions of the song...</td>\n",
       "      <td>This is my granddaughters favorite app on my K...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A331GYAT4ESYI3</td>\n",
       "      <td>THis is just so cute and a great app for littl...</td>\n",
       "      <td>so cute</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A2YEHF8T823TDC</td>\n",
       "      <td>I watch my great grandson 4 days a week and it...</td>\n",
       "      <td>Terrific!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A3699WHISXX94Z</td>\n",
       "      <td>This app is wild and crazy.  Little ones love ...</td>\n",
       "      <td>Five Little Monkeys</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A2BXV49EIES2TB</td>\n",
       "      <td>love love love this app. I was going through d...</td>\n",
       "      <td>love but to quite</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A37HM5TMCMHJES</td>\n",
       "      <td>Very cute, with alot of items to move about.  ...</td>\n",
       "      <td>Cute</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A1FYZPJLU78R2Y</td>\n",
       "      <td>WELL THE CHILDREN LOVE IT AFTER AWHILE YOU GET...</td>\n",
       "      <td>MONKEYS</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>AV58L39SOERMI</td>\n",
       "      <td>I got this app because my 2 year old daughter ...</td>\n",
       "      <td>Could be better....</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A31XG60B64OW74</td>\n",
       "      <td>My three year old Plays this game the most he ...</td>\n",
       "      <td>Five little monkeys</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A39JZ61LPIVY91</td>\n",
       "      <td>Good for little ones, especially if  know the ...</td>\n",
       "      <td>Good for small children</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>AIRKROQMCBVG4</td>\n",
       "      <td>My two year old grandson loves this game and p...</td>\n",
       "      <td>Fun, fun, fun !!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>AELVL0VXR3VFN</td>\n",
       "      <td>My granddaughter really loves this. She will l...</td>\n",
       "      <td>So cute</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B004A9SDD8</td>\n",
       "      <td>A55641MA1CS5F</td>\n",
       "      <td>love it, but wish it was fullscreen on honeyco...</td>\n",
       "      <td>love it</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B004AFQAUA</td>\n",
       "      <td>A39TLD5D8M76M4</td>\n",
       "      <td>does not let you sample only for a sec. if you...</td>\n",
       "      <td>dont play there</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B004AFQAUA</td>\n",
       "      <td>A2XJJKZSEYYW8T</td>\n",
       "      <td>this is a very good app and it you need this s...</td>\n",
       "      <td>app</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B004AFQAUA</td>\n",
       "      <td>AJ9JRKE8AADW0</td>\n",
       "      <td>It's easy to use and transfer songs. If I purc...</td>\n",
       "      <td>Worth the monthly fee</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     productId      reviewerID  \\\n",
       "0   B004A9SDD8  A1N4O8VOJZTDVB   \n",
       "1   B004A9SDD8  A2HQWU6HUKIEC7   \n",
       "2   B004A9SDD8  A1SXASF6GYG96I   \n",
       "3   B004A9SDD8  A2B54P9ZDYH167   \n",
       "4   B004A9SDD8   AFOFZDTX5UC6D   \n",
       "5   B004A9SDD8  A331GYAT4ESYI3   \n",
       "6   B004A9SDD8  A2YEHF8T823TDC   \n",
       "7   B004A9SDD8  A3699WHISXX94Z   \n",
       "8   B004A9SDD8  A2BXV49EIES2TB   \n",
       "9   B004A9SDD8  A37HM5TMCMHJES   \n",
       "10  B004A9SDD8  A1FYZPJLU78R2Y   \n",
       "11  B004A9SDD8   AV58L39SOERMI   \n",
       "12  B004A9SDD8  A31XG60B64OW74   \n",
       "13  B004A9SDD8  A39JZ61LPIVY91   \n",
       "14  B004A9SDD8   AIRKROQMCBVG4   \n",
       "15  B004A9SDD8   AELVL0VXR3VFN   \n",
       "16  B004A9SDD8   A55641MA1CS5F   \n",
       "17  B004AFQAUA  A39TLD5D8M76M4   \n",
       "18  B004AFQAUA  A2XJJKZSEYYW8T   \n",
       "19  B004AFQAUA   AJ9JRKE8AADW0   \n",
       "\n",
       "                                           reviewText  \\\n",
       "0   Loves the song, so he really couldn't wait to ...   \n",
       "1   Oh, how my little grandson loves this app. He'...   \n",
       "2   I found this at a perfect time since my daught...   \n",
       "3   My 1 year old goes back to this game over and ...   \n",
       "4   There are three different versions of the song...   \n",
       "5   THis is just so cute and a great app for littl...   \n",
       "6   I watch my great grandson 4 days a week and it...   \n",
       "7   This app is wild and crazy.  Little ones love ...   \n",
       "8   love love love this app. I was going through d...   \n",
       "9   Very cute, with alot of items to move about.  ...   \n",
       "10  WELL THE CHILDREN LOVE IT AFTER AWHILE YOU GET...   \n",
       "11  I got this app because my 2 year old daughter ...   \n",
       "12  My three year old Plays this game the most he ...   \n",
       "13  Good for little ones, especially if  know the ...   \n",
       "14  My two year old grandson loves this game and p...   \n",
       "15  My granddaughter really loves this. She will l...   \n",
       "16  love it, but wish it was fullscreen on honeyco...   \n",
       "17  does not let you sample only for a sec. if you...   \n",
       "18  this is a very good app and it you need this s...   \n",
       "19  It's easy to use and transfer songs. If I purc...   \n",
       "\n",
       "                                              summary  overall  \n",
       "0                                         Really cute      3.0  \n",
       "1                                 2-year-old loves it      5.0  \n",
       "2                                            Fun game      5.0  \n",
       "3                                We love our Monkeys!      5.0  \n",
       "4   This is my granddaughters favorite app on my K...      5.0  \n",
       "5                                             so cute      5.0  \n",
       "6                                           Terrific!      5.0  \n",
       "7                                 Five Little Monkeys      5.0  \n",
       "8                                   love but to quite      5.0  \n",
       "9                                                Cute      5.0  \n",
       "10                                            MONKEYS      4.0  \n",
       "11                                Could be better....      3.0  \n",
       "12                                Five little monkeys      5.0  \n",
       "13                            Good for small children      3.0  \n",
       "14                                   Fun, fun, fun !!      5.0  \n",
       "15                                            So cute      5.0  \n",
       "16                                            love it      4.0  \n",
       "17                                    dont play there      1.0  \n",
       "18                                                app      5.0  \n",
       "19                              Worth the monthly fee      4.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P97wGoh9DniP"
   },
   "source": [
    "Create a histogram of the scores ('overall') rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvY3IiENnZje"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f85a9e82908>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.hist('overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bvw6GK_G_unS"
   },
   "source": [
    "It seems that most reviews are positive. For this exercise, we'll be looking at the task of binary classification. We will separate the reviews into two classes for the purposes of binary sentiment classification -- Like vs Not like. \n",
    "\n",
    "Beyond binary classification, there are other ways to formulate this task. For example, we could also try to predict each of the rating classes. What type of classification would this be instead?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zn3vrvHxwxsM"
   },
   "source": [
    "### Create the (class) labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7Hzr4hrCNFl"
   },
   "source": [
    "#### Exercise\n",
    "*  Create a function, create_label that outputs a vector of class labels (Y vector). Reviews with a score strictly greater than *3* should be assigned a positive label (`1`), the label should be, `0` otherwise. \n",
    "*   Apply the function to the *overall* and create a new data column in the reviews dataframe, `reviews['Class']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7MxuQ8xDBzc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "8ortFsVuf_bP"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "##Solution\n",
    "\n",
    "# Alternate solution:\n",
    "#reviews['Class'] = 1 * (reviews['overall'] > 3)\n",
    "\n",
    "def create_label(x):\n",
    "    if x > 3:\n",
    "        return 1 # 'positive' \n",
    "    return 0 # 'negative'\n",
    "  \n",
    "reviews['Class'] = reviews.overall.apply(create_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRnEF82anmVC"
   },
   "source": [
    "#### Exercise  \n",
    "* Print the class prior probabilities, P(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-TTwLreAP9C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         1\n",
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "11        0\n",
      "12        1\n",
      "13        0\n",
      "14        1\n",
      "15        1\n",
      "16        1\n",
      "17        0\n",
      "18        1\n",
      "19        1\n",
      "20        1\n",
      "21        1\n",
      "22        1\n",
      "23        1\n",
      "24        0\n",
      "25        1\n",
      "26        0\n",
      "27        1\n",
      "28        1\n",
      "29        1\n",
      "         ..\n",
      "752907    1\n",
      "752908    1\n",
      "752909    1\n",
      "752910    1\n",
      "752911    1\n",
      "752912    1\n",
      "752913    1\n",
      "752914    1\n",
      "752915    1\n",
      "752916    1\n",
      "752917    1\n",
      "752918    1\n",
      "752919    1\n",
      "752920    1\n",
      "752921    1\n",
      "752922    1\n",
      "752923    1\n",
      "752924    1\n",
      "752925    1\n",
      "752926    1\n",
      "752927    1\n",
      "752928    1\n",
      "752929    1\n",
      "752930    1\n",
      "752931    1\n",
      "752932    1\n",
      "752933    0\n",
      "752934    1\n",
      "752935    1\n",
      "752936    1\n",
      "Name: Class, Length: 752937, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reviews['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbyB5ulCMw_6"
   },
   "source": [
    "You should see that most (~72% of the labels are positive overall in the dataset). This means we have an issue of class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fAwpW-5wtkM"
   },
   "source": [
    "### Train/Validation/Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAecZnucBHTl"
   },
   "source": [
    "Next, lets split the reviews dataframe in to train, validation, and test sets.  Recall that training data is used to train our model.  Validation data is used to develop our model (develop new features, etc...) and tune parameters.  The final result should be reported on the test data (that we haven't looked at throughout). \n",
    "\n",
    "1. Split your data (all instances) into *training* and *testing* (80/20 is a reasonable starting point)\n",
    "2. Split the **training data** into *training* and *validation* (again, 80/20 is a typical split).\n",
    "\n",
    "Note that as the size of the dataset increases, the ratio of train/validation/test may vary in other datasets.  For example, if you have millions (or billions) of instances the splits could be more like 99%/0.5%/0.5% for train/validation/test.  This is because you are still using a large number of instances to evaluate the model.\n",
    "\n",
    "We shuffle the data randomly to avoid potential ordering bias.  Note that because we are all using different random splits of the data every time we run the notebook the results will differ (very) slightly due to differences in the random splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjZlxdgxynj2"
   },
   "outputs": [],
   "source": [
    "# shuffle the data randomly to avoid possible bias.\n",
    "random_reviews = reviews.sample(frac=1)\n",
    "\n",
    "# You may change this, but it's set to not be \"too big\".\n",
    "review_limit = min(200000, len(random_reviews))\n",
    "random_reviews = random_reviews.iloc[:review_limit, :]\n",
    "\n",
    "# 1. Split the data 80/20 train/test\n",
    "train_split = int(len(random_reviews) * 0.8)\n",
    "tmp_train = random_reviews.iloc[:train_split,:]\n",
    "test_data = random_reviews.iloc[train_split:,:]\n",
    "\n",
    "# 2. Split the train data into a train/validation split that's 80% train, 20% developemnt \n",
    "validation_split = int(train_split * 0.8)\n",
    "train_data = tmp_train.iloc[:validation_split,:]\n",
    "validation_data = tmp_train.iloc[validation_split:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSpg8jjTBxR-"
   },
   "source": [
    "Lets see some statistics of our resulting datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iScW8CznIeSb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set contains 128000 reviews.\n",
      "Vadlidation set contains 32000 reviews.\n",
      "Test set contains 40000 reviews.\n",
      "Training set contains 72% positive reviews\n",
      "Validation set contains 72% positive reviews\n",
      "Test set contains 72% positive reviews\n"
     ]
    }
   ],
   "source": [
    "print('Training set contains {:d} reviews.'.format(len(train_data)))\n",
    "print('Vadlidation set contains {:d} reviews.'.format(len(validation_data)))\n",
    "print('Test set contains {:d} reviews.'.format(len(test_data)))\n",
    "\n",
    "number_positive_train = sum(train_data['Class'] == 1)\n",
    "number_positive_validation = sum(validation_data['Class'] == 1)\n",
    "number_positive_test = sum(test_data['Class'] == 1)\n",
    "\n",
    "print('Training set contains %0.0f%% positive reviews' % (100*number_positive_train/len(train_data)))\n",
    "print('Validation set contains %0.0f%% positive reviews' % (100*number_positive_validation/len(validation_data)))\n",
    "print('Test set contains %0.0f%% positive reviews' % (100*number_positive_test/len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZT-p3GBM-RY"
   },
   "source": [
    "We can see that both the train/dev/test data all have about 72% positive labels.  This is the same percentage as the overall collection. This is a good sanity check to make sure the data isn't biased.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1TDjC_d2Mx9"
   },
   "source": [
    "Now we have our instances with labels.  Now, we need to create a text representation.  We will start by processing the reviews with spaCy to tokenize and normalize the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYGX-Vb5dEh-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/stuart/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the medium english model. \n",
    "# We will use this model to get embedding features for tokens later.\n",
    "#!python -m spacy download en_core_web_md\n",
    "\n",
    "nlp = spacy.load('/usr/lib/python3.7/site-packages/en_core_web_md/en_core_web_md-2.0.0', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "\n",
    "# Download a stopword list\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7-maQNfu8vx"
   },
   "outputs": [],
   "source": [
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    normalized = token.text.lower().strip()\n",
    "    if ((token.is_alpha or token.is_digit)):\n",
    "      normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k631u7NTeiES"
   },
   "source": [
    "Test to make sure the normalization is working.  Note that for this simple example we aren't keeping punctuation.  It might be something that could be useful as an option to test later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Fjf0ddkcUnx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'app', 'is', 'fun', 'very', 'happy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_normalize(\"the app is fun. very happy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlYo0KmDcZ78"
   },
   "source": [
    "The result should be [the, app, is, fun, very, happy]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "haMIuPwxCPXg"
   },
   "source": [
    "Now we are ready to create the features. As this is for text classification, our features are terms. We'll start with a simple one-hot encoding.\n",
    "\n",
    "#### Exercise\n",
    "* Create a `CountVectorizer` initliazed with `tokenize_normalize` function for the tokenizer parameter. Also use `binary=True` to create a one-hot encoding.\n",
    "* Fit the vectorizer model on the `train_data`, specifically the `reviewText` \n",
    "* Transform each of the three datasets using the vectorizer: `train_data`, `validation_data`, and `test_data`. Assign the result to correspondingly named 'features' variables, `train_features`, `validation_features`, `test_features`\n",
    "\n",
    "**Hint**: Recall that `fit` builds a vocabulary and does pre-processing. `transform()`and then transforms the each review into a sparse vector.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FQBMhCbQdAO"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "one_hot_vectorizer = CountVectorizer(tokenizer=tokenize_normalize, binary=True)\n",
    "one_hot_vectorizer.fit(train_data[\"reviewText\"])\n",
    "train_features = one_hot_vectorizer.transform(train_data[\"reviewText\"])\n",
    "validation_features = one_hot_vectorizer.transform(validation_data[\"reviewText\"])\n",
    "test_features = one_hot_vectorizer.transform(test_data[\"reviewText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-p3_RG6tW3r"
   },
   "source": [
    "### Exercise\n",
    "* Extract the labels for each of the datasets and put them into variables (for convenience)\n",
    "* Recall the labels are in `['Class']` of the corresponding dataframe.\n",
    "* Assign the labels to the corresponding variable, for example `train` should be assigned to `train_labels`.  Follow the same convention for the other two sets of labels as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d97jhRE2tbET"
   },
   "outputs": [],
   "source": [
    "train_labels = train_data['Class']\n",
    "validation_labels = validation_data['Class']\n",
    "test_labels = test_data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWlsGjsdFhDq"
   },
   "source": [
    "Now lets train a classifier using the training data.\n",
    "\n",
    "We will train a [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) classifier. Recall from lecture that the first step for NB is to assume a distribution of the feature data. A \"bernoulli distribution\" is for binary values, whether a value is present or not.  We use the\n",
    "[BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) for features with a one-hot encoding.\n",
    "\n",
    "#### Exercise\n",
    "* Import and create a  `BernoulliNB` classifier for the data with default arguments. \n",
    "* `Fit` (train) the classifier model using the `train_features` and `train_labels`\n",
    "* Assign the output of training your classifier to a `nb_model` variable.\n",
    "\n",
    "**Question:** What would the appropriate Naive Bayes classifier be if we used token frequency counts instead of a one-hot encoding?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7pQh03Li4Pz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "JH2aequkgKoP"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#Solution\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "nb_model = classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDTLXaaULpKo"
   },
   "source": [
    "Can we see how well our classifier is doing? The `score()` function on classifiers calculates the classifier's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YF1P9SCUk76F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8254609375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.score(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4l3u6PC0lCzZ"
   },
   "source": [
    "On the training data we get a accuracy of about 83%. \n",
    "\n",
    "#### Exercise\n",
    "* Score the validation dataset using the trained classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvID85qoDjlc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8064375\n"
     ]
    }
   ],
   "source": [
    "validation_score = nb_model.score(validation_features, validation_labels)\n",
    "print(validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwjplrDjEhJJ"
   },
   "source": [
    "This gives us an accuracy of around 81.5%.  Pretty similar to the training data effectiveness, but slightly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMbZYmXeEDfE"
   },
   "source": [
    "Now we have our classifier learned, lets try it out on some example reviews. \n",
    "- The `predict()` function just returns the MAP estimate (highest probability class)\n",
    "- The `predict_proba()` returns the normalized probability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKu84e9lvip2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[0.3780106 0.6219894]]\n",
      "[1]\n",
      "[[9.24458822e-04 9.99075541e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(nb_model.predict(one_hot_vectorizer.transform([\"the app is awful. total spam.\"])))\n",
    "print(nb_model.predict_proba(one_hot_vectorizer.transform([\"the app is awful. total spam.\"])))\n",
    "\n",
    "print(nb_model.predict(one_hot_vectorizer.transform([\"the app is fun. very happy\"])))\n",
    "print(nb_model.predict_proba(one_hot_vectorizer.transform([\"the app is fun. very happy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BdZ4WLUETb8"
   },
   "source": [
    "We should expect the  first to be negative, the second positive. However, due to variation in the random subsets of data used (and the limited size, this doesn't always work).  This is why it's important to inspect the model outputs.   \n",
    "\n",
    "* Why might the model perform better on the positive case rather than the negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs3brY723-RL"
   },
   "source": [
    "Below is a function that prints out a evaluation summary with key metrics we used in class as well as a 'classification report'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5YlrCsgl6pE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def evaluation_summary(description, predictions, true_labels):\n",
    "  print(\"Evaluation for: \" + description)\n",
    "  precision = precision_score(predictions, true_labels)\n",
    "  recall = recall_score(predictions, true_labels)\n",
    "  accuracy = accuracy_score(predictions, true_labels)\n",
    "  f1 = fbeta_score(predictions, true_labels, 1) #1 means f_1 measure\n",
    "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
    "  print(classification_report(predictions, true_labels, digits=3))\n",
    "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions)) # Note the order here is true, predicted, odd.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfX8gUHjooIs"
   },
   "source": [
    "The sklearn.metrics package includes score functions, including the key [classification metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) discussed in lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioRqnrK_kSWK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: One-hot NB\n",
      "Classifier 'One-hot NB' has Acc=0.806 P=0.894 R=0.847 F1=0.870\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.578     0.676     0.623      7576\n",
      "           1      0.894     0.847     0.870     24424\n",
      "\n",
      "   micro avg      0.806     0.806     0.806     32000\n",
      "   macro avg      0.736     0.761     0.746     32000\n",
      "weighted avg      0.819     0.806     0.811     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 5119  3737]\n",
      " [ 2457 20687]]\n"
     ]
    }
   ],
   "source": [
    "validation_predicted_labels = nb_model.predict(validation_features)\n",
    "evaluation_summary(\"One-hot NB\",  validation_predicted_labels, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mX-LraAoWu9"
   },
   "source": [
    "See the documentation for [confusion_matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) for how to make it look 'pretty'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPFdsZMQwafc"
   },
   "source": [
    "It is important in practice to always have a *baseline* - you need to know that you are better than randomly guessing the class. As discussed in lecture, Sklearn provides instances of [DummyClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) for this purpose.  A dummy classifier has the same interface as all other Estimators (classifiers) in SKLearn.  We `fit` (train) it on the training data and `predict` on the test data.\n",
    "\n",
    "#### Exercise: \n",
    "* Create and train two DummyClassifier instances with `strategy=stratified` (which we sometimes will call 'Random') and `strategy=most_frequent` (MF) which assigns the most common (majority class) label.\n",
    "* Print an evaluation summary of each on the validation data.\n",
    "* Study the output. How do these baselines compare with the one-hot encoding classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39719WyV3XOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: Random\n",
      "Classifier 'Random' has Acc=0.601 P=0.725 R=0.724 F1=0.724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.277     0.278     0.278      8822\n",
      "           1      0.725     0.724     0.724     23178\n",
      "\n",
      "   micro avg      0.601     0.601     0.601     32000\n",
      "   macro avg      0.501     0.501     0.501     32000\n",
      "weighted avg      0.601     0.601     0.601     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 2454  6402]\n",
      " [ 6368 16776]]\n",
      "Evaluation for: Most Frequent\n",
      "Classifier 'Most Frequent' has Acc=0.723 P=1.000 R=0.723 F1=0.839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         0\n",
      "           1      1.000     0.723     0.839     32000\n",
      "\n",
      "   micro avg      0.723     0.723     0.723     32000\n",
      "   macro avg      0.500     0.362     0.420     32000\n",
      "weighted avg      1.000     0.723     0.839     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[    0  8856]\n",
      " [    0 23144]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "random = DummyClassifier(strategy=\"stratified\") \n",
    "mf = DummyClassifier(strategy=\"most_frequent\")\n",
    "random.fit(test_features, test_labels)\n",
    "mf.fit(test_features, test_labels)\n",
    "\n",
    "random_predict = random.predict(validation_features)\n",
    "mf_predict = mf.predict(validation_features)\n",
    "\n",
    "evaluation_summary(\"Random\", random_predict, validation_labels)\n",
    "evaluation_summary(\"Most Frequent\", mf_predict, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cpQ--Sqx3Uk"
   },
   "source": [
    "Ok, comparing the results with the dummy classifiers, our classifier's performance can be best described as \"maybe OK\".  The  class prior 'random' classifier gets an accuracy of about 60% and F1 of about 0.72.  The majority class has an F1 of 0.84 and an accuracy 0f 0.725.  This shouldn't be suprisingly; that's the percentage of the data that is positive. F1 is a useful summary measure combining both precision and recall, you will use this in the coursework as well.  Let's see if we can improve the effectiveness.\n",
    "\n",
    "####Exercise: \n",
    "* Now train and evaluate a `LogisticRegression` classifier. \n",
    "* Specify a `solver=saga`, a variation of stochastic gradient decent\n",
    "* Evaluate the effectiveness on the validation data. \n",
    "\n",
    "This could take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxDNuUZ7nG9v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: Logisitic\n",
      "Classifier 'Logisitic' has Acc=0.849 P=0.918 R=0.878 F1=0.898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.667     0.758     0.709      7793\n",
      "           1      0.918     0.878     0.898     24207\n",
      "\n",
      "   micro avg      0.849     0.849     0.849     32000\n",
      "   macro avg      0.793     0.818     0.804     32000\n",
      "weighted avg      0.857     0.849     0.852     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 5906  2950]\n",
      " [ 1887 21257]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(solver=\"saga\")\n",
    "logistic.fit(test_features, test_labels)\n",
    "predict = logistic.predict(validation_features)\n",
    "evaluation_summary(\"Logisitic\", predict, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8plyfRRxE2w"
   },
   "source": [
    "LR should provide a (small) improvement on F1 to approximately 0.9, which is better than a BernoulliNB for this data.\n",
    "\n",
    "Now, let's experiment with different features for LogisticRegression beyond a one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xk9k3H1tPwCG"
   },
   "source": [
    "## Adding Features\n",
    "\n",
    "Recall that we want to be able to incorporate features from external sources. \n",
    "We will use a simple lexicon dictionary of 'good' and 'bad' words as features. \n",
    "\n",
    "SKLearn has multiple ways of combining features.   We'll look at both of them in this lab. \n",
    "\n",
    "For part I, we will do this using SKLearn [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). The first (and easiest) ways is to use a [FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) as part of a pipeline.There's a nice post on how to be a [Kaggle Pro using FeatureUnion and Pipelines](https://www.kaggle.com/metadist/work-like-a-pro-with-pipelines-and-feature-unions). \n",
    "\n",
    "A pipeline is useful because it allows us to pass in a dataset and extract out different fields that we can process and vectorize separately. \n",
    "\n",
    "Recall that there is also a \"summary\" field in our data. Let's use it to crate a separate set of features. \n",
    "\n",
    "Unfortunately, Pandas does not play nice with SKLearn pipelines natively.  As result, to access the columns in the Panda data we need to use a transformer to select a column from the data.  You will use this as one of the steps in your pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnmvvFQgQ7do"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFzOBxMYQ9Bv"
   },
   "source": [
    "#### Exercise\n",
    "* Create a pipeline that creates a FeatureUnion between two features from the `reviewText` and `summary` fields.\n",
    "* Inside of the FeatureUnion, (for each field separately) use the ItemSelector to select the field, then create a vectorization step with the CountVectorizer that creates a one-hot encoding of each field. \n",
    "\n",
    "Unlike in the Kaggle example, do not include the classifier as part of your pipeline (yet).  \n",
    "\n",
    "* Apply fit / transform on the three pandas data frames to run your pipeline (train, validation, test).\n",
    "* Separately, train and evaluate a  logistic regression model on the resulting features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2892W2VLUuQB"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion \n",
    "pipeline = Pipeline([(\"union\", FeatureUnion([\n",
    "    (\"reviewText\", Pipeline([\n",
    "        (\"select\", ItemSelector(\"reviewText\")),\n",
    "        (\"vec\", one_hot_vectorizer)\n",
    "    ])),\n",
    "    (\"summary\", Pipeline([\n",
    "        (\"select\", ItemSelector(\"summary\")),\n",
    "        (\"vec\", one_hot_vectorizer)\n",
    "    ]))\n",
    "]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pSXnmNWC_q2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: Pipeline\n",
      "Classifier 'Pipeline' has Acc=0.858 P=0.926 R=0.883 F1=0.904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.678     0.779     0.725      7708\n",
      "           1      0.926     0.883     0.904     24292\n",
      "\n",
      "   micro avg      0.858     0.858     0.858     32000\n",
      "   macro avg      0.802     0.831     0.814     32000\n",
      "weighted avg      0.866     0.858     0.861     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 6002  2854]\n",
      " [ 1706 21438]]\n"
     ]
    }
   ],
   "source": [
    "union_train_features = pipeline.fit_transform(train_data)\n",
    "union_validation_features = pipeline.transform(validation_data)\n",
    "union_test_features = pipeline.transform(test_data)\n",
    "\n",
    "logistic_reg = LogisticRegression(solver=\"saga\")\n",
    "logistic_reg.fit(union_train_features, train_labels)\n",
    "predict = logistic_reg.predict(union_validation_features)\n",
    "evaluation_summary(\"Pipeline\", predict, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GT6SvAOCVb-p"
   },
   "source": [
    "This provides a significant boost in effectiveness up to around 0.925.  Having good features matters! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMm81LgzytIn"
   },
   "source": [
    "## Combining sparse and dense features ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJrH52Bks_eQ"
   },
   "source": [
    "We can build upon last week's lab, to provide more features for sentiment analysis - in particular, we can represent each review by the average of its constituent word vectors.  \n",
    "\n",
    "The medium and large spaCy models include word vector representations built into them.  We will use these instead of the glove embeddings we used in last lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Qf-ZQexMi2V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great True 5.4395933 False [-9.3846e-02  5.8296e-01 -1.9271e-02 -7.0072e-02  1.8095e-01  1.5343e-01\n",
      "  1.7444e-01 -1.8207e-01 -6.6300e-02  2.3681e+00 -1.2753e-01  1.7784e-02\n",
      "  1.0581e-01  1.9629e-01 -2.5103e-01 -2.7987e-01 -2.9529e-01  1.1575e+00\n",
      " -2.0997e-01  8.3031e-02 -2.6101e-02 -2.3911e-01  2.7443e-01 -2.2339e-01\n",
      " -4.9437e-02  1.9215e-01  1.2176e-01  2.2273e-01 -1.2051e-01  1.9972e-01\n",
      "  2.1834e-01  3.0302e-01 -1.7650e-02  6.6369e-02  1.5469e-01 -2.7746e-01\n",
      "  2.9550e-01 -3.5517e-01 -3.6803e-01 -2.1441e-01 -1.6825e-02  3.2859e-01\n",
      " -1.6417e-01 -4.3756e-02  3.2168e-01  4.7823e-01 -3.0072e-01  3.5865e-01\n",
      "  1.8450e-01 -1.1995e-01 -4.8905e-02  3.7055e-01  4.4224e-01  1.7276e-01\n",
      "  1.8705e-01  2.3734e-01  5.5195e-03  1.5334e-01 -8.0614e-02 -9.8517e-04\n",
      " -1.3972e-01 -5.1074e-01 -1.0340e-01  4.5437e-01  6.5120e-02 -1.9894e-01\n",
      "  2.0476e-01  2.5925e-01  1.5235e-01  6.9943e-02  2.5109e-01  1.1591e-01\n",
      " -1.1138e-01  1.0800e-01  2.0717e-01 -1.2912e-01 -9.8970e-02 -8.5548e-02\n",
      " -3.3701e-01  3.1039e-01  1.4305e-01  4.3963e-01  3.5747e-01  8.8332e-02\n",
      "  1.5892e-01 -2.0840e-01 -5.5627e-01  8.4637e-02  1.3979e-01 -1.2993e-01\n",
      " -1.3248e-01  8.8142e-03 -3.2534e-01  4.8462e-01  5.1233e-01 -6.3665e-02\n",
      "  1.1115e-01 -2.7077e-01  2.3871e-01  7.8161e-02 -2.5160e-01 -2.8224e-01\n",
      " -2.2184e-01 -1.1327e-01  9.8459e-02 -5.3567e-01  4.0205e-01  8.5075e-02\n",
      "  1.9930e-01 -4.1897e-01  1.5794e-01 -3.1213e-01 -2.8026e-02  8.3250e-02\n",
      " -5.3532e-02 -1.3679e-01 -1.4818e-01  6.8241e-02 -1.6335e-01 -9.9226e-02\n",
      "  2.6308e-01 -2.4857e-02  3.6254e-02  2.4600e-01 -2.2973e-02  6.1963e-02\n",
      " -5.5620e-01 -3.1834e-01  2.9555e-02 -1.7897e-02 -1.1667e-01 -5.7007e-01\n",
      " -6.4362e-02 -3.5952e-01 -2.2113e-01  2.7234e-01 -2.0025e-01 -2.1237e-01\n",
      "  1.1051e-01 -2.4965e-01 -2.1364e+00  6.1577e-01  7.5513e-01  9.1956e-02\n",
      " -8.5376e-02 -3.8080e-01  6.1236e-02 -1.5926e-01  2.4403e-01  2.3006e-01\n",
      " -4.0976e-01 -1.4060e-02  1.0240e-01 -3.6229e-01 -3.6398e-02 -2.0095e-01\n",
      "  3.3177e-01 -6.0438e-02 -2.5786e-03 -4.8177e-01  3.1641e-01  1.5644e-01\n",
      "  1.9951e-01 -6.3946e-02  4.6002e-01 -2.4506e-01  2.9228e-01 -9.1145e-02\n",
      "  2.4338e-01 -1.6651e-01 -2.0778e-01  1.8609e-01  3.3566e-01 -1.3386e-01\n",
      "  2.4688e-04  1.2751e-01 -2.0145e-01  2.2582e-01  1.2034e-02 -4.6975e-02\n",
      "  1.2110e-01 -2.8555e-01 -1.6521e-01  1.5822e-01 -1.4490e-01 -4.0716e-01\n",
      " -1.4427e-01 -3.1331e-01  8.2185e-02  6.5069e-02 -2.5508e-01 -3.3292e-02\n",
      "  9.2705e-02  6.9051e-02 -5.5293e-02  1.3867e-01  5.6291e-02 -8.5436e-02\n",
      "  4.7096e-01  2.0802e-01 -7.0866e-02 -1.8452e-01 -2.2423e-02  4.7695e-02\n",
      "  5.3932e-02  3.3512e-01 -1.5784e-01 -5.6534e-02 -1.5052e-01 -1.1890e-01\n",
      " -4.2663e-02 -1.3797e-01  2.0510e-02 -3.7339e-01 -1.2414e-01  1.2465e-01\n",
      "  6.3567e-01 -2.3536e-01 -8.6698e-02 -1.1009e-01  6.2452e-03 -6.4445e-02\n",
      "  1.3455e-01  6.3983e-01  5.2109e-01  1.2517e-01 -7.1185e-02  2.1012e-01\n",
      "  3.2931e-02  2.5686e-01  8.3267e-02  6.9838e-02 -4.4970e-01  1.2990e-01\n",
      "  1.0138e-01 -2.1039e-01 -3.3935e-01 -2.8580e-01 -3.2122e-01  6.9091e-02\n",
      "  9.8763e-02  1.0539e-01 -2.8357e-01  4.5972e-01  4.4680e-01  1.5884e-01\n",
      " -3.4796e-01 -3.6872e-01 -1.2029e-01 -7.3328e-02  1.8609e-01 -1.5804e-01\n",
      " -4.2720e-01 -8.0804e-02 -1.6371e-01  2.1788e-01  3.4897e-01 -6.0955e-02\n",
      " -2.8139e-01  6.9213e-02  3.2936e-01  3.3136e-02  1.3849e-01  3.2184e-01\n",
      " -5.5657e-02  2.2492e-02  2.7119e-01  3.9717e-01  1.9364e-01  4.4494e-01\n",
      " -1.7807e-01  6.9304e-02 -3.9351e-01 -5.6197e-01 -4.3662e-01 -5.2200e-02\n",
      "  6.5840e-02 -6.5327e-02  1.2017e-01  1.5355e-01 -4.5702e-02 -9.4543e-02\n",
      " -1.9552e-01  1.2832e-01 -1.3169e-01 -1.7249e-02  5.9724e-01 -8.0509e-02\n",
      "  1.3788e-01 -1.6002e-01 -5.8949e-01  8.5984e-02 -1.9932e-01 -1.5365e-02\n",
      "  1.8558e-01  9.3269e-03  4.0960e-01 -9.8804e-02  2.7538e-02  3.0042e-01]\n",
      "fun True 6.2949853 False [ 1.4147e-01  1.7264e-01 -3.5865e-01 -4.5490e-01  2.6629e-01  1.9800e-01\n",
      "  1.1048e-01  2.8269e-02  3.4253e-01  1.8359e+00 -5.6326e-01 -4.4621e-01\n",
      "  2.7219e-02  2.3749e-01  2.5605e-02 -4.0058e-01 -3.0532e-01  8.1734e-01\n",
      " -5.2661e-01  3.9568e-02  4.9383e-02 -8.7267e-02  2.5790e-01 -1.3237e-01\n",
      "  4.3894e-01  5.6373e-01  6.4652e-01 -4.6927e-02  1.9994e-01 -4.6308e-01\n",
      " -5.2285e-02 -3.9085e-01 -1.1236e-01  1.9775e-01  4.8632e-01 -2.7640e-01\n",
      "  2.6675e-01  3.2560e-01 -7.0285e-02  3.5359e-02 -6.7320e-03  3.2060e-01\n",
      " -1.0462e-01 -1.6121e-01  4.2248e-02  5.5931e-01 -2.2568e-01  3.8240e-02\n",
      "  5.2018e-01 -2.9203e-01 -1.1388e-01  3.2131e-01  5.7159e-01  1.4833e-02\n",
      " -1.7957e-01  3.4490e-01 -6.3636e-02 -1.2622e-01 -7.2219e-02  1.8993e-01\n",
      "  3.0634e-01  9.7579e-02 -2.0734e-01 -5.3802e-02  3.2095e-01 -3.0530e-01\n",
      " -2.8957e-01  3.7886e-01  9.1884e-02  7.9271e-02  1.8040e-01  1.5495e-01\n",
      " -5.2464e-02 -2.3231e-01  5.3210e-01  2.4408e-01 -2.6893e-02  3.5412e-01\n",
      " -2.2664e-01  7.4090e-01  3.1338e-01 -3.9181e-01 -2.3601e-01  3.1706e-01\n",
      " -2.6365e-01  1.5562e-01 -1.4691e-01  5.8394e-01  3.3724e-01  1.7659e-01\n",
      "  1.1025e-01  4.8322e-01 -1.2548e-01  1.8225e-01 -4.1037e-03  2.1132e-01\n",
      " -3.5440e-01 -9.2335e-02  1.4283e-01  2.4849e-01  1.4378e-01 -1.9534e-01\n",
      " -8.1928e-01 -1.9274e-01  2.2879e-01 -7.7323e-02  4.6459e-01  2.5525e-01\n",
      " -1.4862e-02 -3.8192e-02  2.9644e-01 -5.4688e-01 -5.1088e-02 -1.6700e-01\n",
      "  5.3598e-02 -5.8639e-01 -3.2375e-01 -1.8826e-01 -4.5202e-01 -1.5338e-02\n",
      "  1.9034e-01 -3.0542e-01  1.2313e-02  3.8852e-02  4.0715e-02  3.8178e-01\n",
      "  2.4584e-02 -2.4608e-01  1.1192e-01  3.4796e-03 -1.5171e-01 -2.3576e-01\n",
      " -4.6803e-01 -1.8865e-01 -1.5535e-02 -1.2382e-01 -5.7130e-02 -7.8921e-02\n",
      "  3.5235e-04  9.7942e-02 -3.0122e+00  3.7371e-01  4.5561e-01  2.0107e-02\n",
      "  4.5298e-02 -2.5724e-01  9.4185e-03 -2.3230e-01  1.1310e-01 -4.3477e-01\n",
      " -6.8896e-02  3.1937e-01 -1.1148e-01 -2.8969e-01 -4.0923e-02 -2.7736e-01\n",
      "  1.8533e-01 -1.5144e-01 -2.3900e-01 -6.3285e-01 -1.3048e-01  8.7927e-02\n",
      "  2.8302e-01 -1.5168e-01 -6.8515e-02 -1.6684e-01 -2.2133e-01 -1.6518e-01\n",
      "  4.6339e-01  2.0488e-01  3.4256e-01  3.0381e-01  3.1625e-01  2.1777e-01\n",
      " -1.7886e-01 -2.6362e-01 -3.3781e-01 -3.4365e-01  3.2295e-01  2.3219e-01\n",
      " -3.1758e-02 -2.4844e-01  5.8369e-02  2.9307e-01  2.1339e-02 -3.8613e-02\n",
      " -3.0478e-01  1.6481e-01  5.7077e-02  1.3912e-01 -1.4135e-01  1.7337e-01\n",
      "  1.3970e-01  4.5017e-03 -3.9141e-02  3.9683e-01 -8.6932e-02 -3.6612e-01\n",
      "  5.3232e-01  3.3235e-01 -4.2444e-02  4.5979e-03  5.6254e-01  1.3385e-01\n",
      " -2.0806e-02  1.8068e-01 -1.9375e-01  3.4898e-01  1.4984e-01 -2.2940e-01\n",
      " -6.3817e-01  2.4088e-01  1.8165e-01 -5.0338e-01 -2.1777e-01  5.8197e-01\n",
      "  5.3413e-01  5.6123e-02 -8.8850e-02  2.7202e-01  4.2716e-01  9.6386e-02\n",
      " -1.3225e-01 -1.0316e-01 -2.6889e-01 -1.4688e-01 -1.1915e-01  3.6497e-01\n",
      "  1.0800e-01 -2.2951e-01 -7.9811e-02  5.1883e-01 -1.7703e-01  1.0431e-01\n",
      "  2.7646e-01  3.2462e-01 -1.5500e-01  5.9818e-01 -2.5798e-01  5.1896e-01\n",
      "  9.3230e-02 -8.2696e-02 -3.7896e-01  4.5340e-01 -2.5272e-01  1.7635e-01\n",
      " -8.5509e-01 -1.8027e-01  7.8594e-02  3.8600e-01 -9.0344e-04 -8.8201e-02\n",
      " -4.1797e-01  3.5753e-02  6.3465e-02  2.6121e-01  3.9519e-01 -1.8404e-01\n",
      " -2.6034e-01  1.4317e-03  2.7473e-02  4.9070e-01 -4.6438e-01 -7.1891e-02\n",
      "  1.0879e-01  2.4578e-01 -2.6445e-01  3.0551e-01  6.5070e-01  6.0054e-02\n",
      " -1.3498e+00 -2.0365e-01 -6.8580e-01 -3.6015e-01 -6.5830e-01  4.4835e-02\n",
      "  2.3205e-02 -2.1322e-01 -1.1841e-01  2.8567e-01  2.9123e-01 -2.5859e-01\n",
      " -2.6295e-01  3.8299e-03  2.8629e-02 -5.9676e-02  4.9195e-01 -1.0685e-01\n",
      "  7.1947e-02  3.1096e-01 -3.2397e-01 -1.5628e-01 -2.0374e-01 -2.1067e-01\n",
      "  2.8628e-01  3.6254e-02  9.2874e-02 -1.4640e-01 -2.7180e-01  4.9267e-01]\n",
      ". True 4.9316354 False [ 0.012001   0.20751   -0.12578   -0.59325    0.12525    0.15975\n",
      "  0.13748   -0.33157   -0.13694    1.7893    -0.47094    0.70434\n",
      "  0.26673   -0.089961  -0.18168    0.067226   0.053347   1.5595\n",
      " -0.2541     0.038413  -0.01409    0.056774   0.023434   0.024042\n",
      "  0.31703    0.19025   -0.37505    0.035603   0.1181     0.012032\n",
      " -0.037566  -0.5046    -0.049261   0.092351   0.11031   -0.073062\n",
      "  0.33994    0.28239    0.13413    0.070128  -0.022099  -0.28103\n",
      "  0.49607   -0.48693   -0.090964  -0.1538    -0.38011   -0.014228\n",
      " -0.19392   -0.11068   -0.014088  -0.17906    0.24509   -0.16878\n",
      " -0.15351   -0.13808    0.02151    0.13699    0.0068061 -0.14915\n",
      " -0.38169    0.12727    0.44007    0.32678   -0.46117    0.068687\n",
      "  0.34747    0.18827   -0.31837    0.4447    -0.2095    -0.26987\n",
      "  0.48945    0.15388    0.05295   -0.049831   0.11207    0.14881\n",
      " -0.37003    0.30777   -0.33865    0.045149  -0.18987    0.26634\n",
      " -0.26401   -0.47556    0.68381   -0.30653    0.24606    0.31611\n",
      " -0.071098   0.030417   0.088119   0.045025   0.20125   -0.21618\n",
      " -0.36371   -0.25948   -0.42398   -0.14305   -0.10208    0.21498\n",
      " -0.21924   -0.17935    0.21546    0.13801    0.24504   -0.2559\n",
      "  0.054815   0.21307    0.2564    -0.25673    0.17961   -0.47638\n",
      " -0.25181   -0.0091498 -0.054362  -0.21007    0.12597   -0.40795\n",
      " -0.021164   0.20585    0.18925   -0.0051896 -0.51394    0.28862\n",
      " -0.077748  -0.27676    0.46567   -0.14225   -0.17879   -0.4357\n",
      " -0.32481    0.15034   -0.058367   0.49652    0.20472    0.019866\n",
      "  0.13326    0.12823   -1.0177     0.29007    0.28995    0.029994\n",
      " -0.10763    0.28665   -0.24387    0.22905   -0.26249   -0.069269\n",
      " -0.17889    0.21936    0.15146    0.04567   -0.050497   0.071482\n",
      " -0.1027    -0.080705   0.30296    0.031302   0.26613   -0.0060951\n",
      "  0.10313   -0.39987   -0.043945  -0.057625   0.08702   -0.098152\n",
      "  0.22835   -0.005211   0.038075   0.01591   -0.20622    0.021853\n",
      "  0.0040426 -0.043063  -0.002294  -0.26097   -0.25802   -0.28158\n",
      " -0.23118   -0.010404  -0.30102   -0.4042     0.014653  -0.10445\n",
      "  0.30377   -0.20957    0.3119     0.068272   0.1008     0.010423\n",
      "  0.54011    0.29865    0.12653    0.013761   0.21738   -0.39521\n",
      "  0.066633   0.50327    0.14913   -0.11554    0.010042   0.095698\n",
      "  0.16607   -0.18808    0.055019   0.026715  -0.3164    -0.046583\n",
      " -0.051591   0.023475  -0.11007    0.085642   0.28394    0.040497\n",
      "  0.071986   0.14157   -0.021199   0.44718    0.20088   -0.12964\n",
      " -0.067183   0.47614    0.13394   -0.17287   -0.37324   -0.17285\n",
      "  0.02683   -0.1316     0.09116   -0.46487    0.1274    -0.090159\n",
      " -0.10552    0.068006  -0.13381    0.17056    0.089509  -0.23133\n",
      " -0.27572    0.061534  -0.051646   0.28377    0.25286   -0.24139\n",
      " -0.19905    0.12049   -0.1011     0.27392    0.27843    0.26449\n",
      " -0.18292   -0.048961   0.19198    0.17192    0.33659   -0.20184\n",
      " -0.34305   -0.24553   -0.15399    0.3945     0.22839   -0.25753\n",
      " -0.25675   -0.37332   -0.23884   -0.048816   0.78323    0.18851\n",
      " -0.26477    0.096566   0.062658  -0.30668   -0.43334    0.10006\n",
      "  0.21136    0.039459  -0.11077    0.24421    0.60942   -0.46646\n",
      "  0.086385  -0.39702   -0.23363    0.021307  -0.10778   -0.2281\n",
      "  0.50803    0.11567    0.16165   -0.066737  -0.29556    0.022612\n",
      " -0.28135    0.0635     0.14019    0.13871   -0.36049   -0.035    ]\n",
      "afskfsd False 0.0 True [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_tokenize(\"Great fun. afskfsd\")\n",
    "for token in tokens:\n",
    "  print(token.text, token.has_vector, token.vector_norm, token.is_oov, token.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tR2oYLtPKBzn"
   },
   "source": [
    "The classifier below is a slightly modified version from Lab 4.  The key change is that it performs tokenization and uses the token vector built in to spaCy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiGs13mYttF7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AverageEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dimension = 300\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "      \n",
    "    def transform(self, X):  \n",
    "      # Skip OOV terms. \n",
    "      # Return 0 for all dimensions if no words are in the vocabulary.\n",
    "      dense_matrix =  np.array([ \n",
    "          np.mean([token.vector for token in self.tokenizer(doc) if not token.is_oov]\n",
    "                or [np.zeros(self.dimension)], axis=0)\n",
    "          for doc in X\n",
    "      ])\n",
    "      return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJ6_Kb41EA1R"
   },
   "outputs": [],
   "source": [
    "# Note: We don't call fit here because it doesn't do anything.\n",
    "embedding_vectorizer = AverageEmbeddingTransformer(spacy_tokenize)\n",
    "train_embedding_features = embedding_vectorizer.transform(train_data['reviewText'])\n",
    "validation_embedding_features = embedding_vectorizer.transform(validation_data['reviewText'])\n",
    "test_embedding_features = embedding_vectorizer.transform(test_data['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_jnrUdEbICki"
   },
   "source": [
    "#### Exercise\n",
    "* Train and evaluate a logistic regression model using the embedding features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGFCKVwTD7oS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: AvgEmbedding\n",
      "Classifier 'AvgEmbedding' has Acc=0.837 P=0.923 R=0.861 F1=0.891\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.611     0.752     0.674      7201\n",
      "           1      0.923     0.861     0.891     24799\n",
      "\n",
      "   micro avg      0.837     0.837     0.837     32000\n",
      "   macro avg      0.767     0.807     0.783     32000\n",
      "weighted avg      0.853     0.837     0.842     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 5415  3441]\n",
      " [ 1786 21358]]\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver=\"saga\")\n",
    "log_reg.fit(train_embedding_features, train_labels)\n",
    "predict = log_reg.predict(validation_embedding_features)\n",
    "evaluation_summary(\"AvgEmbedding\", predict, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWrdjOHsEv1Q"
   },
   "source": [
    "You should see that these are not as effective as words.  The score of embeddings on their own is probably around 0.89, but this is not bad considering we have just 300 features per post!  This is actually very compact and powerful. \n",
    "\n",
    "But, we can do better if we combine both types of representations. The features from embeddings are dense.  The word features are a sparse vector.  How do we combine them?  They need to be consistent.  We will convert the dense embeddings to sparse matrices.  *Question*: Why do we not convert the sparse word features to dense features instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7QwQApRrlqy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_embedding_features))\n",
    "# Below should be changed to the name of the variable that is the output of your pipeline. \n",
    "print(type(union_train_features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSW9wDA5PAg_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "train_sparse_embeddings = csr_matrix(train_embedding_features)\n",
    "validation_sparse_embeddings = csr_matrix(validation_embedding_features)\n",
    "type(validation_sparse_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1A8gq7waFPVX"
   },
   "source": [
    "We now have sparse features.  We can now combine them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1tB4BHZIjrw"
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "1. Combine the embedding feature matrix (e.g. train_embedding_features) with the one-hot encoding matrix (e.g. one_hot_train_features) for both the training and validation feature data.\n",
    "2. Train a and evaluate a logistic regression model on the combined features.\n",
    "\n",
    "**Hint: ** We need to \"stack\" the matrices to merge them.  The common functions are \"hstack\" (horizontal) stack adds columns and \"vstack\" (vertical stack) that adds rows.  Numpy has built-in support for dense arrays / matrices.  However, the one-hot encoding features are sparse features. We converted the dense embedding matrix to a sparse matrix.  We can now  use the  [sparse library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.hstack.html) in scipy to concatenate the matrices by 'horizontally stacking' them.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dCJYt2OEshf"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "stack_train = hstack([train_embedding_features, union_train_features])\n",
    "stack_validation = hstack([validation_embedding_features, union_validation_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZX5n4eY3GeJM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: Embedding\n",
      "Classifier 'Embedding' has Acc=0.862 P=0.927 R=0.887 F1=0.907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.690     0.784     0.734      7795\n",
      "           1      0.927     0.887     0.907     24205\n",
      "\n",
      "   micro avg      0.862     0.862     0.862     32000\n",
      "   macro avg      0.809     0.836     0.820     32000\n",
      "weighted avg      0.870     0.862     0.865     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 6114  2742]\n",
      " [ 1681 21463]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "log_regression = LogisticRegression(solver=\"saga\")\n",
    "log_regression.fit(stack_train, train_labels)\n",
    "predict = log_regression.predict(stack_validation)\n",
    "evaluation_summary(\"Embedding\", predict, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxBoIFFjG9tn"
   },
   "source": [
    "Putting them together should provide a little boost, possibly close to 0.930.\n",
    "\n",
    "Additional parameter tuning (or better features!) could improve this model further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Coo0HJvpTlIQ"
   },
   "source": [
    "## Tuning parameters: combining Pipelines + Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlBsfx1DVRsC"
   },
   "source": [
    "#### Exercise: Create a simple pipeline model with a classifier\n",
    "* Create a pipeline model for a VERY simple  model.\n",
    "* Select the `reviewText` column (only)\n",
    "* Create a one-hot encoding with CountVectorizer\n",
    "* Use a LogisticRegression model classifier with solver=saga\n",
    "* Train the model (training data) and evaluate the model (validation data).\n",
    "\n",
    "**Hint**: The pipeline should have three steps in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhPOkRmrHh3y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: Pipeline\n",
      "Classifier 'Pipeline' has Acc=0.858 P=0.927 R=0.882 F1=0.904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.675     0.781     0.724      7659\n",
      "           1      0.927     0.882     0.904     24341\n",
      "\n",
      "   micro avg      0.858     0.858     0.858     32000\n",
      "   macro avg      0.801     0.831     0.814     32000\n",
      "weighted avg      0.867     0.858     0.861     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 5979  2877]\n",
      " [ 1680 21464]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"select\", ItemSelector(\"reviewText\")),\n",
    "    (\"vec\", one_hot_vectorizer),\n",
    "    (\"class\", LogisticRegression(solver=\"saga\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_data, train_labels)\n",
    "predict = pipe.predict(validation_data)\n",
    "evaluation_summary(\"Pipeline\", predict, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yj2IZIJEWSdZ"
   },
   "source": [
    "#### Exercise\n",
    "* Use GridSearchCV to try one-hot vs bag-of-words (binary=True/False) one the pipeline.\n",
    "* Specify cv=2 to only perform two-fold cross validation (the default is 3, which is slower)\n",
    "* Specify the scoring= parameter.  Make it use f1_macro instead of the default (accuracy).\n",
    "\n",
    "**Hint:** You can see an example of this to tune parameters for text classification in the [20 newsgroups example](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py). \n",
    "\n",
    "This trains multiple models and selects the best set of model parameters. This can be expensive with large numbers of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AofiathpX_AO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: Grid one-hot\n",
      "Classifier 'Grid one-hot' has Acc=0.858 P=0.927 R=0.882 F1=0.904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.675     0.781     0.724      7663\n",
      "           1      0.927     0.882     0.904     24337\n",
      "\n",
      "   micro avg      0.858     0.858     0.858     32000\n",
      "   macro avg      0.801     0.831     0.814     32000\n",
      "weighted avg      0.867     0.858     0.861     32000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 5981  2875]\n",
      " [ 1682 21462]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "grid = GridSearchCV(pipe, {}, cv=2, scoring=make_scorer(f1_score))\n",
    "grid.fit(train_data, train_labels)\n",
    "predict = grid.predict(validation_data)\n",
    "evaluation_summary(\"Grid one-hot\", predict, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-M12UJSNJNrm"
   },
   "source": [
    "We should find that using counts doesn't help this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Z7YAjmFwu-_"
   },
   "source": [
    "## FInal evaluation on test data\n",
    "\n",
    "We developed our model and saw how well it performed on validation data so far.  The final step is to evaluate on held out test data that has not been used at all in our development process.  We should not develop our features / model based on the test results.  It's only used for reporting the final numbers.  This is like a submission to a leaderboard where you don't know the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "197cvpFEvrLf"
   },
   "source": [
    "#### Exercise: **Final evaluation**\n",
    "* The final step is to report classification numbers and confusion matrix on the held out test set \n",
    "* Evaluate the best of each of the different classifiers models (Dummy classifiers, Naive Bayes, One-Hot LR (multiple fields), and the  final combined model with embeddings) on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXZKLgoPGY9U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: MF\n",
      "Classifier 'MF' has Acc=0.723 P=1.000 R=0.723 F1=0.839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         0\n",
      "           1      1.000     0.723     0.839     40000\n",
      "\n",
      "   micro avg      0.723     0.723     0.723     40000\n",
      "   macro avg      0.500     0.361     0.420     40000\n",
      "weighted avg      1.000     0.723     0.839     40000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[    0 11081]\n",
      " [    0 28919]]\n",
      "Evaluation for: NB\n",
      "Classifier 'NB' has Acc=0.804 P=0.892 R=0.845 F1=0.868\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.574     0.671     0.619      9482\n",
      "           1      0.892     0.845     0.868     30518\n",
      "\n",
      "   micro avg      0.804     0.804     0.804     40000\n",
      "   macro avg      0.733     0.758     0.743     40000\n",
      "weighted avg      0.817     0.804     0.809     40000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 6361  4720]\n",
      " [ 3121 25798]]\n",
      "Evaluation for: One-Hot\n",
      "Classifier 'One-Hot' has Acc=0.861 P=0.928 R=0.885 F1=0.906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.685     0.784     0.731      9683\n",
      "           1      0.928     0.885     0.906     30317\n",
      "\n",
      "   micro avg      0.861     0.861     0.861     40000\n",
      "   macro avg      0.806     0.834     0.819     40000\n",
      "weighted avg      0.869     0.861     0.864     40000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 7592  3489]\n",
      " [ 2091 26828]]\n",
      "Evaluation for: Combined\n",
      "Classifier 'Combined' has Acc=0.861 P=0.928 R=0.885 F1=0.906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.685     0.784     0.731      9689\n",
      "           1      0.928     0.885     0.906     30311\n",
      "\n",
      "   micro avg      0.861     0.861     0.861     40000\n",
      "   macro avg      0.807     0.835     0.819     40000\n",
      "weighted avg      0.869     0.861     0.864     40000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 7596  3485]\n",
      " [ 2093 26826]]\n"
     ]
    }
   ],
   "source": [
    "evaluation_summary('MF', mf.predict(test_features), test_labels)\n",
    "evaluation_summary('NB', nb_model.predict(test_features), test_labels)\n",
    "evaluation_summary('One-Hot', pipe.predict(test_data), test_labels)\n",
    "evaluation_summary('Combined', grid.predict(test_data), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfukMo7312ed"
   },
   "source": [
    "# Feedback quiz\n",
    "\n",
    "Now that you have complete the lab, please take the time to complete the [lab feedback quiz](https://moodle.gla.ac.uk/mod/feedback/view.php?id=1120741) on Moodle. These quizzes are important, as they help us to calibrate the lab to class progress, and consider how to improve the class for next year.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEW0fx8JJesB"
   },
   "source": [
    "# Additional exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXRLnjwhxe7J"
   },
   "source": [
    "## Add external features\n",
    "\n",
    "### Sentiment lexicon\n",
    "You could add external features from sentiment lexicons and libraries. One widely used library for sentiment classification is VADER, which was developed by manually labeling data and manually assigning word scores for sentiment and polarity.\n",
    "\n",
    "VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text by C.J. Hutto and Eric Gilbert\n",
    "Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\n",
    "There is a python library [VADER sentiment library](https://github.com/cjhutto/vaderSentiment). Below is some sample code to see how it's used. \n",
    "* How well does it predict the labels on its own as a classifier?\n",
    "* You might apply it as a new feature and retrain LR classifier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrdMUQjHzAVE"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example code:\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyJfLuBEJqz5"
   },
   "source": [
    "### Paragraph vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOQKSBovJi7T"
   },
   "source": [
    "You could also try using paragraph vectors (doc2vec) instead of averaging word embeddings from the previous lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdnOyoS3yfNr"
   },
   "source": [
    "\n",
    "\n",
    "## Deeper evaluation ##\n",
    "Do you trust this classifier? Look at it's failures.\n",
    "\n",
    "When was a post predicted to be negative, when it was predicted strongly positive? You can use to obtain a `predict_proba()` which predicts the probability of the post in the class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vNEtO6r0DRr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TAD Lab 5 App Reviews.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
