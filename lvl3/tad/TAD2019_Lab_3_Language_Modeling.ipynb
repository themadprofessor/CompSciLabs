{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1suGewMzIKsC"
   },
   "source": [
    "# Lab 3: Language modeling\n",
    "\n",
    "In this week's lab, we'll explore techniques for language modeling. \n",
    "\n",
    "The aims of this lab are:\n",
    "*   Model the probability of generating language with unigrams and trigram LMs\n",
    "*   Evaluate the quality of language model using perplexity\n",
    "*   Understand and address issues of sparsity in language modeling\n",
    "*   Experiment with applications of language models in understanding text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "napJywoXLO7u"
   },
   "source": [
    "## Background: Load Reddit\n",
    "Just run the cells below, it should be familiar by now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OyYm3gDwJKQ-"
   },
   "outputs": [],
   "source": [
    "local_file = \"coarse_discourse_dump_reddit_unfiltered.json\"\n",
    "#!gsutil cp gs://textasdata/coarse_discourse_dump_reddit.json $local_file\n",
    "#!curl -o $local_file https://storage.googleapis.com/tad2018/coarse_discourse_dump_reddit.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mkk-DAI8O1l3"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ufunc size changed, may indicate binary incompatibility. Expected 216 from C header, got 192 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2d5a345b9e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     from pandas._libs import (hashtable as _hashtable,\n\u001b[0m\u001b[1;32m     27\u001b[0m                              \u001b[0mlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                              tslib as _tslib)\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/_libs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .tslibs import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime, Period)\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/_libs/tslibs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_convert_single\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m__init__.pxd\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216 from C header, got 192 from PyObject"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "posts = list()\n",
    "\n",
    "# If the dataset is too large, you can load a subset of the posts.\n",
    "post_limit = 100000000\n",
    "\n",
    "# Construct a dataframe, by opening the JSON file line-by-line\n",
    "with open(local_file) as jsonfile:\n",
    "  for i, line in enumerate(jsonfile):\n",
    "    thread = json.loads(line)\n",
    "    if (len(posts) > post_limit):\n",
    "      break\n",
    "      \n",
    "    for post in thread['posts']:\n",
    "      posts.append((thread['subreddit'], thread['title'], thread['url'],\n",
    "                        post['id'], post.get('author', \"\"), post.get('body', \"\")))\n",
    "print(len(posts))\n",
    "\n",
    "labels = ['subreddit', 'title', 'id', 'url', 'author', 'body']\n",
    "post_frame = pd.DataFrame(posts, columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gB1SEq_B1jYc"
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the small english model. \n",
    "# Disable the advanced NLP features in the pipeline for efficiency.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "\n",
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    if (not token.is_punct):\n",
    "#    if (token.is_alpha or token.is_digit):\n",
    "      normalized = token.text.lower().strip()\n",
    "      normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmy9VGvhPsad"
   },
   "source": [
    "It may take several minutes to run spaCy on all posts in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_XrKqR6N-8g"
   },
   "outputs": [],
   "source": [
    "# Use the tokenizer to extract all tokens from the body of the posts.\n",
    "# Flatten the tokens in the post into a single list of all the tokens.\n",
    "import itertools\n",
    "all_tokens = []\n",
    "all_posts_tokenized = post_frame.body.apply(tokenize_normalize)\n",
    "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
    "print(\"Num tokens: \", len(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78XpfWSHF9n6"
   },
   "source": [
    "## Unigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrbvxDImHYAd"
   },
   "source": [
    "We'll use the dictionary from Lab 2 as a building block to build a unigram language model.  We added special padding tokens (for start /end sentence) that will be used later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08xONN37bLd7"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class SimpleDictionary(object):\n",
    "\n",
    "  # Add special \"padding\" tokens as well as unk.\n",
    "  START_TOKEN = \"<p>\"\n",
    "  END_TOKEN = \"</p>\"\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.token_counts = collections.Counter(tokens)\n",
    "    self.N = sum(iter(self.token_counts.values()))\n",
    "\n",
    "    # Leave space for \"<p>\", \"</p>\", and \"<unk>\"\n",
    "    top_counts = self.token_counts.most_common(None if size is None else (size - 3))\n",
    "    vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in iter(self.id_to_word.items())}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience create the vocab set\n",
    "    self.vocab_set = set(iter(self.word_to_id.keys()))\n",
    "\n",
    "    # Store special IDs\n",
    "    self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "    self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "  \n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def sentence_to_ids(self, words):\n",
    "    return [self.START_ID] + self.words_to_ids(words) + [self.END_ID]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zGf2ADjbWNF"
   },
   "outputs": [],
   "source": [
    "dictionary = SimpleDictionary(all_tokens)\n",
    "print(\"Vocabulary size: %d unique words\" % dictionary.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jAqqE58rCTm"
   },
   "source": [
    "#### Your task\n",
    "\n",
    "- Create a class, ``UnigramLM``\n",
    "- Initializer should take a dictionary (``SimpleDictionary``)\n",
    "- It should pre-compute the unigram probabilities for all the words in the `vocab` and store them in a dictionary\n",
    "- Keep a copy of the `vocab_set` from the dictionary as a member variable for convenience.\n",
    "- Unknown (OOV) words should return a probability of 0.  Consider using the `defaultdict` collection with a 0.0 lambda.\n",
    "- **Recall from lecture **: What are the two key functions that a language model must satisfy? Define a function for each of these.  \n",
    "  \n",
    "Click SHOW CODE below for a skeleton (and required function names).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "UzTWStFirIgk"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class UnigramLM(object):\n",
    "  \n",
    "  def __init__(self, dictionary):\n",
    "    # Compute the probabilities and store them in a data structure.\n",
    "    self.vocab_set =\n",
    "    \n",
    "  # Compute the conditional probability of the next token for the unigram model. \n",
    "  def next_token_conditional_prob(self, previous_words, next_word):\n",
    "    \n",
    "  # Compute the probability of a sequence, P(w1, ..., wn)\n",
    "  # When Verbose is true, it prints the probability for each\n",
    "  # token in the sequence.\n",
    "  def sequence_probability(self, sequence, verbose=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XiiXFQ9ruejG"
   },
   "outputs": [],
   "source": [
    "unigram_lm = UnigramLM(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZ1mMO8jxD7W"
   },
   "source": [
    "Let's try computing the unigram probabilities of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoRWLmmCwa4o"
   },
   "outputs": [],
   "source": [
    "# Try computing the unigram probabilities for indivudal words.\n",
    "print(\"Pr(the):\", unigram_lm.next_token_conditional_prob(None, 'the'))\n",
    "print(\"Pr(glasgow):\", unigram_lm.next_token_conditional_prob(None,'glasgow'))\n",
    "print(\"Pr(defenestrate):\", unigram_lm.next_token_conditional_prob(None,'defenestrate'))\n",
    "\n",
    "# And for the same word, but with different contexts\n",
    "print(unigram_lm.next_token_conditional_prob([\"the\", \"end\", \"of\", \"the\", \"world\"], \"as\"))\n",
    "print(unigram_lm.next_token_conditional_prob([\"the\"], \"as\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PM6aSVrnzCOX"
   },
   "source": [
    "The probabilities should be about 3.5% for 'the' and 0 for defenstrate. \n",
    "\n",
    "Now, try computing the probability of a sequence of words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHIVlXHrxb9y"
   },
   "outputs": [],
   "source": [
    "probability = unigram_lm.sequence_probability([\"the\", \"end\", \"of\", \"the\", \"world\", \"as\", \"we\", \"know\", \"it\"], True)\n",
    "print('{:.40f}'.format(probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFnlo0cSzsLP"
   },
   "source": [
    "It should be about 1.8Ã—10-21. This is already a very small number and most of the values in these sequence are large by typical probability values. As a result, we usually do the probability computation in log space in practice to avoid problems of underflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5igWwh7t5uLx"
   },
   "source": [
    "### Application: Spelling correction\n",
    "\n",
    "We can use this simple language model to build a spelling corrector.\n",
    "\n",
    "Below is a spelling corrector based on code by Peter Norvig, a director of research at Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-PQBbJ_XEh4"
   },
   "source": [
    "#### Your task\n",
    "- Fill in the `P` function below to compute the unigram probability of a word.  Hint: use probability from the language model.\n",
    "- Modify `candidates` to generate the set of candidate words \n",
    " - The words can be up to two edits away. See the provided edit distance functions in the class. \n",
    " - Word candidates are `known` (in the vocab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjHnVDPsXA_m"
   },
   "outputs": [],
   "source": [
    "class SimpleSpellingCorrector(object): \n",
    "  \n",
    "  def __init__(self, lm):\n",
    "    self.lm = lm\n",
    "  \n",
    "  def P(self, word): \n",
    "    \"Unigram probability of `word`.\"\n",
    "    return # YOUR CODE HERE\n",
    "\n",
    "  def correction(self, word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(self.candidates(word), key=self.P)\n",
    "\n",
    "  def candidates(self, word): \n",
    "    \"Generate possible spelling corrections for word\"\n",
    "    return # YOUR CODE TO GENERATE CANDIDATES\n",
    "\n",
    "  def known(self, words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in self.lm.vocab_set)\n",
    "\n",
    "  def edits1(self, word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "  def edits2(self, word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vnLb1hI4cyYi"
   },
   "source": [
    "Now, let's test how the full spelling corrector works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRV-SUcO6DJH"
   },
   "outputs": [],
   "source": [
    "word = \"speling\"\n",
    "#word = TRY YOUR WORD\n",
    "\n",
    "corrector = SimpleSpellingCorrector(unigram_lm)\n",
    "candidates = corrector.candidates(word)\n",
    "print(\"Spelling candidates for:\", word)\n",
    "for candidate in candidates:\n",
    "  print(\"{} \\t{:.9f}\".format(candidate, corrector.P(candidate)))\n",
    "\n",
    "# The correct takes the word that has the highest probability (occurs most often).\n",
    "correction = corrector.correction(word)\n",
    "print(\"\\nSelected correction:\", correction, corrector.P(correction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rrpbrcXecGz"
   },
   "source": [
    "Wohooo! We have a very basic reddit spelling corrector. \n",
    "Try it yourself on other words. \n",
    "\n",
    "However, as we'll see it's not perfect.  Let's look at a sequence of words below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6fMOvSyCJL6"
   },
   "outputs": [],
   "source": [
    "# Simple function that takes a string as input\n",
    "# and runs the spelling corrector for each token. \n",
    "# Returns a list of spell-corrected tokens.\n",
    "def spell_correct(string, corrector):\n",
    "  corrections = list()\n",
    "  tokens = tokenize_normalize(string)\n",
    "  for t in tokens: \n",
    "    correction = corrector.correction(t)\n",
    "    corrections.append(correction)\n",
    "  return corrections \n",
    "\n",
    "string = \"it is amazon\"\n",
    "print(spell_correct(string, corrector))\n",
    "\n",
    "string = \"http www amazin\"\n",
    "print(spell_correct(string, corrector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKbC0-w0et6F"
   },
   "source": [
    "Not so 'amazing'. In both of these cases, we estimated the likelihood of spelling correcting each word on its own,  independently.  This fails in obvious cases where the correct word should be apparent given the sequence. \n",
    "\n",
    "We can do better by taking two factors into account:\n",
    "\n",
    "1.   Computing the probability of the whole sequence of words (in case there are multiple spelling mistakes) \n",
    "2.   Using word context (bi-grams and trigrams) to improve the word probablility estimate\n",
    "\n",
    "We'll look at how to compute both in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRslwUFqDn_t"
   },
   "source": [
    "# N-Gram Language Models\n",
    "\n",
    "The unigram model isn't a very good one - it doesn't model any previous context. On the other hand, we can't model _all_ of the preceding words, because that history will get prohibitively long and extremely sparse. As a compromise, we make a _Markov assumption_ and limit ourselves to a finite history of $n$ words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25Uw9ei-F7ZS"
   },
   "source": [
    "#### Thought exercise\n",
    "- For a corpus of 1 million tokens and a vocabulary size of 10,000:\n",
    " - What is the maximum number of bigrams that could theoretically exist? \n",
    " - How many possible trigrams?\n",
    "\n",
    "Think about this process as drawing a word, then another from the collection in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4kqEtYAeDsY"
   },
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "V = 10000\n",
    "\n",
    "# Number of possible bigrams\n",
    "bigrams = \n",
    "print(bigrams)\n",
    "\n",
    "# Number of possible trigrams\n",
    "trigrams = \n",
    "print(trigrams)\n",
    "\n",
    "# Technically, the practical limit is also limited by size of the collection, N.\n",
    "# Bigrams would be limited by N-1\n",
    "# Trigrams would be limited by N-2\n",
    "print(min(N-1, bigrams))\n",
    "print(min(N-2, trigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxibe5r3FVVg"
   },
   "source": [
    "#### Your task \n",
    "- Print number of possible bigrams and trigrams based on the size of the Reddit `dictionary`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsA6Dyi6eVIs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aSyqn4e8bmKS"
   },
   "source": [
    "- How much space would a table of all possible unigrams, bigrams and trigrams take? \n",
    "- For simplicity, let's assume 8 bytes per entry.  \n",
    "- How much space would we need to store all of them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEgIEGc-DsEa"
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "print (\"Vocab size: %d words\" % (dictionary.size))\n",
    "print (\"Unigrams need: %g KB\" % (8 * (dictionary.size ** 1) / (2**10)))\n",
    "print (\"Bigrams need:  %g GB\" % (8 * (dictionary.size ** 2) / (2**30)))\n",
    "print (\"Trigrams need:  %g TB\" % (8 * (dictionary.size ** 3) / (2**40)))\n",
    "print (\"Available:     %g GB\" % (psutil.virtual_memory().available / (2**30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOoet6sSEbKR"
   },
   "source": [
    "Look at the results! Given a typical machine, we can store all possible unigrams and some of the bigrams, but we'd never get to trigrams!\n",
    "\n",
    "Thankfully, we don't have to store every possible n-gram combination. Most words occur rarely. As a result, for bigrams and trigrams, the table will be very sparse. We will only store the entries that we observe; the rest we can taken to be zero or we can estimate their values using smoothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OpxWsOoFf1F"
   },
   "source": [
    "### Constructing our Model\n",
    "For now, we'll build a trigram model, which considers the two preceding words:\n",
    "\n",
    "$$ P(w_i\\ |\\ w_{i-1}, ..., w_0) \\approx P(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n",
    "\n",
    "We'll need to store a table of the word probabilities, indexed by triples $(w_i, w_{i-1}, w_{i-2})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gC5tanICEigU"
   },
   "source": [
    "\n",
    "We'll represent our model with a nested map `context => word => probability`, where word is $w_i$ and for our trigram model, the context is the two preceding words $(w_{i-1}, w_{i-2})$ .\n",
    "\n",
    "First, we'll go through the corpus and compute raw trigram counts $c(abc)$, which we'll then normalize into Maximum Likelihood Estimates of the probabilities:\n",
    "\n",
    "$$  P_{abc} = P(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{\\mathrm{c(abc)}}{\\sum_{c'}\\mathrm{c(abc')}} = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n",
    "\n",
    "*Notational note:* sometimes we write $(w_{i-1}, w_{i-2})$  and sometimes $(w_{i-2}, w_{i-1})$, because P(c|ab) is more readable and natural than P(c|ba).  It doesn't matter as long as the counts are consistent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSpzCN8c_OWH"
   },
   "source": [
    "#### Optional Task \n",
    "Note: This task may take awhile to complete.  It is **optional**, but goes into depth on how to implement an n-gram LM. Don't spend too long on it, be sure to leave enough time complete the rest of the lab. \n",
    "- Create a class, ``TrigramLM``\n",
    "- Initializer should take a sequence of tokens and construct the model with n-gram counts\n",
    "- The class should keep the ``vocab`` and its size, like ``SimpleDictionary``\n",
    "- For convenience, keep counts of both the trigram ``counts`` (numerator) and the ``context_totals`` (denominator) values. \n",
    "- The pobability of a sequence should be computed using log_2 space \n",
    "- **Tip:** You might use Python's defaultdict collection (with a lambda) to set default values, particularly for multi-level maps\n",
    "- **Tip: **Use tuples of tokens as keys in the dictionary, *not* lists of tokens\n",
    "\n",
    "Click SHOW CODE to see the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "0d71UAW8_Jzl"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class SimpleTrigramLM(object):\n",
    "   \"\"\"Simple Trigram LM\"\"\"\n",
    "\n",
    "  def __init__(self, tokens):\n",
    "      \"\"\"Build our trigram model.\n",
    "      Args:\n",
    "        tokens: (list or np.array) of training tokens\n",
    "      Returns:\n",
    "        None\n",
    "      \"\"\"    \n",
    "    # Compute the counts and store them in a data structure.\n",
    "    \n",
    "  # Compute the conditional probability of the next token,  P(wi | wi-1, wi-2)\n",
    "  def next_token_conditional_prob(self, previous_words, next_word):\n",
    "      \"\"\"Next token conditional probability.\n",
    "      Args:\n",
    "        word: (string) w in P(w | w_1 w_2 )\n",
    "        seq: (list of string) [w_1, w_2, w_3, ...]\n",
    "      Returns:\n",
    "        (float) P_k(w | w_1 w_2), according to the model\n",
    "      \"\"\"    \n",
    "    \n",
    "  # Compute the probability of a sequence, P(w1, ..., wn)\n",
    "  def sequence_probability(self, sequence):\n",
    "      \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "8JhKPaDJ_dQz"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class SimpleTrigramLM(object):\n",
    "    \"\"\"Simple Trigram LM\"\"\"\n",
    "    order_n = 3\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        \"\"\"Build our trigram model.\n",
    "        Args:\n",
    "          tokens: (list or np.array) of training tokens\n",
    "        Returns:\n",
    "          None\n",
    "        \"\"\"\n",
    "        print(\"Num tokens: \", len(tokens))\n",
    "        \n",
    "        # Raw trigram counts over the corpus.\n",
    "        # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n",
    "        # Be sure to use tuples (w_2,w_1) as keys, *not* lists [w_2,w_1]\n",
    "        self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "        # Map of (w_1, w_2) -> int\n",
    "        # Entries are c( w_2, w_1 ) = sum_w c(w_2, w_1, w)\n",
    "        self.context_totals = dict()\n",
    "\n",
    "        # Track unique words seen, for normalization\n",
    "        self.vocab = set()\n",
    "\n",
    "        # Iterate through the word stream once\n",
    "        # Compute trigram counts \n",
    "        # This is a sliding window over each word.\n",
    "        w_1, w_2 = None, None\n",
    "        for word in tokens:\n",
    "            self.vocab.add(word)\n",
    "            if w_1 is not None and w_2 is not None:\n",
    "                self.counts[(w_2,w_1)][word] += 1\n",
    "            # Update context\n",
    "            w_2 = w_1\n",
    "            w_1 = word\n",
    "\n",
    "        for context, ctr in iter(self.counts.items()):  \n",
    "            self.context_totals[context] = sum(iter(ctr.values())) \n",
    "\n",
    "        # Total vocabulary size, for normalization\n",
    "        self.V = len(self.vocab)\n",
    "\n",
    "    def next_token_conditional_prob(self, word, seq):\n",
    "        \"\"\"Next token conditional probability.\n",
    "        Args:\n",
    "          word: (string) w in P(w | w_1 w_2 )\n",
    "          seq: (list of string) [w_1, w_2, w_3, ...]\n",
    "        Returns:\n",
    "          (float) P_k(w | w_1 w_2), according to the model\n",
    "        \"\"\"\n",
    "        context = tuple(seq[-2:])  # (w_2, w_1)\n",
    "        cw = self.counts.get(context, {}).get(word, 0)  \n",
    "        numerator = cw \n",
    "        cc = self.context_totals.get(context, 0)  \n",
    "        denominator = cc  \n",
    "        return numerator / denominator  \n",
    "      \n",
    "    def sequence_probability(self, seq, verbose=False):\n",
    "      \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n",
    "      context_size = self.order_n - 1\n",
    "      score = 0.0\n",
    "      count = 0\n",
    "      # Start at third word, since we need a full context.\n",
    "      for i in range(context_size, len(seq)):\n",
    "        context = seq[i-context_size:i]\n",
    "        context_prob = self.next_token_conditional_prob(seq[i], context)\n",
    "        s = np.log2(context_prob)\n",
    "        # DEBUG.\n",
    "        if verbose:\n",
    "            print (\"log P(%s | %s) = %.03f\" % (seq[i], \" \".join(context), s))\n",
    "        score += s\n",
    "        count += 1\n",
    "       \n",
    "      return score, count      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkR9CNqyftiU"
   },
   "source": [
    "Build the trigram model on all tokens, it should take 15-30s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5AXZNazHexS"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "print (\"Building trigram LM...\"),\n",
    "lm = SimpleTrigramLM(all_tokens)\n",
    "print (\"done in %.02f s\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQdWiNkbfxsa"
   },
   "source": [
    "Let's inspect the trigram output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hh8gP95nf3SB"
   },
   "outputs": [],
   "source": [
    "print (\"Most frequent tri-grams:\")\n",
    "flat_counts = defaultdict(lambda: 0.0)\n",
    "# This code puts the contexts back together with their word counts\n",
    "for (context, counts) in lm.counts.items():\n",
    "  for (word, count) in counts.items():\n",
    "    flat_counts[context, word] = count\n",
    "\n",
    "sorted_trigrams = sorted(flat_counts.items(), key=lambda k_v: k_v[1], reverse=True)\n",
    "\n",
    "for (word, count) in sorted_trigrams[:50]:\n",
    "    print(\"\\\"%s\\\": %d\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i72I2CHUkgMV"
   },
   "source": [
    "The should be obvious, like \"a lot of\" -- sequence of frequent words.  However, given the large size of our corpus the counts are still quite small.\n",
    "\n",
    "Let's look at how much memory is used in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZg7-O9skY5N"
   },
   "outputs": [],
   "source": [
    "def print_stats(lm):\n",
    "    \"\"\"Output summary statistics about our language model.\"\"\"\n",
    "    print (\"=== N-gram Language Model stats ===\")\n",
    "    unique_ngrams = sum(len(c) for k,c in lm.counts.items())\n",
    "    print (\"%g unique 3-grams\" % (unique_ngrams))\n",
    "\n",
    "    optimal_memory_bytes = sum(\n",
    "            (4 * len(k) + 20 * len(v))\n",
    "             for k, v in lm.counts.items())\n",
    "    print (\"Optimal memory usage (counts only): %d MB\" %\n",
    "            (optimal_memory_bytes / (2**20)))\n",
    "\n",
    "print_stats(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5OXuFYTkuQa"
   },
   "source": [
    "Despite the theoretical size, the number of unique bigrams and trigrams seen in practice is very sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZbqWAJFJ_qP"
   },
   "source": [
    "## Generating made up Reddit posts\n",
    "\n",
    "- Language models are *generative*.  They model the probability of generating a sequence of words. The probabilities can be used in interesting ways, for example to score sequences or even to generate made up text sequences iteratively.  \n",
    "- Below is a function, `sample_next` that samples possible next words randomly proportional to their conditional probability as the next word in the sequence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-_tAW09B2KH"
   },
   "outputs": [],
   "source": [
    "def sample_next(lm, seq):\n",
    "    \"\"\"Sample a word from the conditional distribution.\"\"\"\n",
    "    # This looks through each possible next word in the vocab and computes its \n",
    "    # conditional probability with the current sequence.\n",
    "    probs = [lm.next_token_conditional_prob(token, seq) for token in lm.vocab]\n",
    "    \n",
    "    # Pick a word at random according to its conditional probability\n",
    "    return np.random.choice(list(lm.vocab), p=probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjObCY_6lay6"
   },
   "source": [
    "We can use a Language Model to generate new made up data from our model. We'll generate words sequentially, one token at a time by trying to predict the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSI2dfh3e3q3"
   },
   "outputs": [],
   "source": [
    "# Given it the start sequence to indicate the start of a post. \n",
    "   \n",
    "def hallucinate_text(start, max_length):\n",
    "  sequence = list()\n",
    "  sequence.extend(start)\n",
    "  for i in range(max_length - len(start)):\n",
    "      sequence.append(sample_next(lm, sequence))\n",
    "  #print (\" \".join(sequence))\n",
    "  #print (\"[{1:d} tokens; log P(seq): {0:.02f}]\".format(lm.sequence_probability(seq)))\n",
    "  return sequence\n",
    "    \n",
    "# Maximum length of sequence to generate. \n",
    "max_length = 20\n",
    "\n",
    "# Number of sequences to generate\n",
    "num_sequences = 5\n",
    "\n",
    "\n",
    "start = [\"i\", \"feel\"] # Needs to be an n-gram that occurs in our collection\n",
    "for _ in range(num_sequences):\n",
    "  sequence = hallucinate_text(start, max_length)\n",
    "  print (\" \".join(sequence))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4TLERMrKXj7"
   },
   "source": [
    "We now have a simple bot that can write reddit-like posts!  It sort-of-almost makes sense. Many naive spam bots work in a similar way by taking sample text and using it to generate made up auto-replies. \n",
    "\n",
    "Have fun with this example by modifying the starting sequence.  Note: currently the starting sequence must occur in our input! You can also change the length of posts or the number of posts.  \n",
    " - Consider: If you were building an auto-reply system, how would you alter this to take the similarity to an existing starting post? Recall the lab from last lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVHBk7zEo49K"
   },
   "source": [
    "Let's evaluate our models by seeing how well they can predict real text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buvdn5h1EGnw"
   },
   "source": [
    "## Language Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIuLPUSIEyJa"
   },
   "source": [
    "### Train and test sets\n",
    "\n",
    "We'll split the data into two parts: train and test. We train our model (compute counts) on the training sample of posts. This means that we fixed (or *fit* in sci-kit learn terminology) our vocabulary based on the words in *training* set.  We then evaluate on how well it can predict the new unseen test posts.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcb0HYwKFQ9x"
   },
   "outputs": [],
   "source": [
    "# We shuffle the posts randomly by calling Pandas sample function with a fraction of 1.\n",
    "shuffled_posts = post_frame.sample(frac=1)\n",
    "\n",
    "# Split the data into 80% train, 20% test posts.\n",
    "train_frac = 0.8\n",
    "split_idx = int(train_frac * len(shuffled_posts))\n",
    "train_posts = shuffled_posts[:split_idx]\n",
    "test_posts = shuffled_posts[split_idx:]\n",
    "\n",
    "print (\"Training set:\", len(train_posts))\n",
    "print (\"Test set: \", len(test_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iz3tO_QZJNJj"
   },
   "source": [
    "Apply the tokenizer and dictionary to ONLY the training posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HksEdFwzoKZJ"
   },
   "outputs": [],
   "source": [
    "train_post_tokens = train_posts.body.apply(tokenize_normalize)\n",
    "flat_train_tokens = list(itertools.chain.from_iterable(train_post_tokens))\n",
    "train_flat_dictionary = SimpleDictionary(flat_train_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOcaQbUdUgzn"
   },
   "source": [
    "We're about ready to evaluate. But we do some extra text processing steps first. \n",
    "\n",
    "We need to deal with two issues:\n",
    "\n",
    "\n",
    "1.   Add special delimiters to mark the start / end of posts. We want to model the probability of words starting or ending a post. We also want the probability of generating words in a post to be separate (independent) from each other. Note that the number of padding tokens (two at the start) depends on the order of the model (we are using a trigram).\n",
    "2.   Handle UNK (OOV) tokens in new data and replace the words with UNK tokens. \n",
    "\n",
    "The result is a new `post_to_tokens` function.  Although we don't do it here for readability, this function could also convert the token sequence into a sequence of integer word IDs using the dictionary (like we did for a one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPnqAGZ7H9i6"
   },
   "outputs": [],
   "source": [
    "# Replace unkown words with the special UNK word.\n",
    "def replace_unk(word, wordset=None):\n",
    "    if word in wordset: \n",
    "      return word \n",
    "    else: \n",
    "      return SimpleDictionary.UNK_TOKEN # unknown token\n",
    "    \n",
    "def posts_to_tokens(posts):\n",
    "    \"\"\"Returns an flattened list of the words in the posts, with padding for a trigram model.\"\"\"\n",
    "    # Pad each post with delimters.\n",
    "    # Why do we use two start delimiters?\n",
    "    padded_posts = ([SimpleDictionary.START_TOKEN, SimpleDictionary.START_TOKEN] + p + [SimpleDictionary.END_TOKEN] for p in posts)\n",
    "    \n",
    "    # This will replace anything not in vocab with <unk> \n",
    "    return np.array([replace_unk(w, wordset=train_flat_dictionary.vocab_set) \n",
    "                     for w in list(itertools.chain.from_iterable((padded_posts)))], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsyXVlBZqEyj"
   },
   "outputs": [],
   "source": [
    "flat_delimited_train_tokens = posts_to_tokens(train_post_tokens)\n",
    "train_dictionary = SimpleDictionary(flat_delimited_train_tokens)\n",
    "train_lm = SimpleTrigramLM(flat_delimited_train_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJIm9npFP8FN"
   },
   "source": [
    "## Perplexity\n",
    "\n",
    "We'll score our model using perplexity. As mentioned in lecture, we take $ 2^H $ (exponentiate it) to get the perplexity score, where $H$ is the cross-entropy. How do we get this? \n",
    "\n",
    "Running the `sequence_probability` function computes the log-likelihood of our data: \n",
    "\n",
    "$$ Log P(w_1, ... w_N) = \\sum_{i=1}^N \\log_2 \\hat{p}(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n",
    "\n",
    "Which is very close to the cross-entropy loss:\n",
    "$$ \\text{H}_{\\text{total}}(y, \\hat{y}) = -1 \\sum_{i=1}^N \\frac{1}{N} \\log_2 \\hat{p}(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n",
    "\n",
    "The cross-entropy is equal to $-1$ times the log-likelihood of the data under our model, averaged by the total number of instances (tokens).\n",
    "\n",
    "Let's run it.  We compute the probability of the whole collection as a sequence using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hozSKXzCNBwk"
   },
   "outputs": [],
   "source": [
    "log_p_data, num_real_tokens = train_lm.sequence_probability(flat_delimited_train_tokens)\n",
    "print (\"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlGfGhTyYGHP"
   },
   "source": [
    "#### Optional Task: \n",
    "- Compute the unigram perplexity of the training data (in log space)\n",
    "- Use the UnigramLM with the train_dictionary\n",
    "- Compare the unigram model perplexity vs the trigram model\n",
    "\n",
    "*Hint:* Compute the probabilities of each word separately since context isn't needed for the unigram model.  Or modify your UnigramLM model to compute sequence probabilities in log space, as we did for the trigram model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgHQAFL-lRlS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "M7IITLA7UxTL"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "log_p = 0 \n",
    "unigram_train_lm = UnigramLM(train_dictionary)\n",
    "# We do this here, we could also modify our LM to operate in log space\n",
    "# directly, which is what is commonly done.\n",
    "for w in flat_delimited_train_tokens:\n",
    "  prob = unigram_train_lm.next_token_conditional_prob(None, w)\n",
    "  if prob == 0: \n",
    "    print (w)\n",
    "  log_prob = np.log2(prob)\n",
    "  log_p += log_prob\n",
    "  \n",
    "print (\"Unigram train perplexity: %.02f\" % (2**(-1*log_p/train_dictionary.N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fUTuMCHzO3J3"
   },
   "source": [
    "You should find that the the unigram perplexity is slightly less than 1000. This means that on average at each word the model picks between on average 1000 words.  In contrast, you should find that the perplexity of the trigram model is around 10.  It reduces the uncertainty in picking the next word by a factor of 100! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fJZkA2y4MFy"
   },
   "source": [
    "But, this is just on the training data.  How well do the models do on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16IgxMuwtSnb"
   },
   "outputs": [],
   "source": [
    "test_post_tokens = test_posts.body.apply(tokenize_normalize)\n",
    "flat_delimited_test_tokens = posts_to_tokens(test_post_tokens)\n",
    "\n",
    "log_p_data, num_real_tokens = train_lm.sequence_probability(flat_delimited_test_tokens)\n",
    "print (\"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLF043MHQVrT"
   },
   "source": [
    "Whoops!! What's going on here? Our model gets an absurdly low perplexity on the training data, but a divide by zero error (it means a sequence has a zero probability of occurring) on the test data. Why?\n",
    "\n",
    "**Answer:** the n-gram model overfits without any smoothing; it thinks the generated sequences are completely likely and unseen sequences don't exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZTHRMsZQes_"
   },
   "source": [
    "## Smoothing and handling the unknown\n",
    "\n",
    "Our simple model doesn't have any real mechanism for handling unknown words - if we feed something unseen it will result in an error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeeAnLtGQtkE"
   },
   "source": [
    "Is assuming zero probabilities realistic? Let's look back at our unigram distribution:\n",
    "- What percentage of the words in the test tokens are UNK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hg_wy1eWQxyW"
   },
   "outputs": [],
   "source": [
    "print (\"%% <unk> in test set: %.02f%%\" % (np.sum(np.array(flat_delimited_test_tokens) == SimpleDictionary.UNK_TOKEN) * 100.0 / len(flat_delimited_test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3TsAE_iRZri"
   },
   "source": [
    "In this case, we find about 1-2% of all tokens are unseen (depending on tokenization). If we want to use our language model in the wild, we'll need to apply smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JsmaPLA69UKg"
   },
   "source": [
    "### Laplace Add-k Smoothing smoothing\n",
    "\n",
    "Remember that if a trigram hasn't been seen before, it will be assigned a zero probability. In this section we'll experiment with Laplace (Add-K) smoothing to fix that problem. Recall our unsmoothed maximum likelihood estimate of $ P(w_i\\ |\\ w_{i-1}, w_{i-2})$ where we use the raw distribution over words seen in a context in the training data:\n",
    "\n",
    "$$  \\hat{P}(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n",
    "\n",
    "Recall from lecture that Add-k smoothing is where we add $k > 0$ to each count $C_{abc}$, pretending we've seen every vocabulary word $k$ extra times in each context. So we have:\n",
    "\n",
    "$$ \\hat{P}_k(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc} + k}{\\sum_{c'} (C_{abc'} + k)} = \\frac{C_{abc} + k}{C_{ab} + k\\cdot|V|} $$\n",
    "\n",
    "where $|V|$ is the size of our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7UVblH05ZE7"
   },
   "source": [
    "#### Optional exercise\n",
    "NOTE: This exercise is OPTIONAL.  If you get stuck, run the working code to be able to try different values of K. \n",
    "\n",
    "- Copy your `SimpleTrigramLm` class and call it `AddKTrigramLM`\n",
    "- Add support for a `k` smoothing member variable with a default value of 0.0\n",
    "- Add a function to `set_k` so that it can be updated\n",
    "- Modify `next_token_conditional_prob` to smooth the probability values with Laplace smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "im2sKZVJ56sw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "TVKJh5sE9S-Y"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class AddKTrigramLM(object):\n",
    "    \"\"\"Trigram LM with add-k smoothing.\"\"\"\n",
    "    order_n = 3\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Do not modify.\"\"\"\n",
    "        state_vars = ['k', 'counts', 'context_totals', 'words', 'V']\n",
    "        return all([getattr(self, v) == getattr(other, v) for v in state_vars])\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        \"\"\"Build our trigram model.\n",
    "        Args:\n",
    "          tokens: (list or np.array) of training tokens\n",
    "        Returns:\n",
    "          None\n",
    "        \"\"\"\n",
    "        print(\"Num tokens: \", len(tokens))\n",
    "        self.k = 0.0\n",
    "        # Raw trigram counts over the corpus.\n",
    "        # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n",
    "        # Be sure to use tuples (w_2,w_1) as keys, *not* lists [w_2,w_1]\n",
    "        self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "        # Map of (w_1, w_2) -> int\n",
    "        # Entries are c( w_2, w_1 ) = sum_w c(w_2, w_1, w)\n",
    "        self.context_totals = dict()\n",
    "\n",
    "        # Track unique words seen, for normalization\n",
    "        # Use wordset.add(word) to add words\n",
    "        wordset = set()\n",
    "\n",
    "        # Iterate through the word stream once\n",
    "        # Compute trigram counts \n",
    "        # This is a sliding window over each word.\n",
    "        w_1, w_2 = None, None\n",
    "        for word in tokens:\n",
    "            wordset.add(word)\n",
    "            if w_1 is not None and w_2 is not None:\n",
    "                self.counts[(w_2,w_1)][word] += 1\n",
    "            # Update context\n",
    "            w_2 = w_1\n",
    "            w_1 = word\n",
    "\n",
    "        for context, ctr in iter(self.counts.items()):  \n",
    "            self.context_totals[context] = sum(iter(ctr.values())) \n",
    "\n",
    "        # Total vocabulary size, for normalization\n",
    "        self.vocab = wordset\n",
    "        self.V = len(self.vocab)\n",
    "\n",
    "    def set_k(self, k=0.0, **params):\n",
    "        self.k = k\n",
    "\n",
    "    def next_token_conditional_prob(self, word, seq):\n",
    "        \"\"\"Next token conditional probability.\n",
    "        Args:\n",
    "          word: (string) w in P(w | w_1 w_2 )\n",
    "          seq: (list of string) [w_1, w_2, w_3, ...]\n",
    "        Returns:\n",
    "          (float) P_k(w | w_1 w_2), according to the model\n",
    "        \"\"\"\n",
    "        context = tuple(seq[-2:])  # (w_2, w_1)\n",
    "        k = self.k\n",
    "      \n",
    "        cw = self.counts.get(context, {}).get(word, 0)  \n",
    "        numerator = cw + k  # add k smoothing\n",
    "        cc = self.context_totals.get(context, 0)  \n",
    "        denominator = cc + (self.V * k)  \n",
    "        return numerator / denominator  \n",
    "      \n",
    "    def sequence_probability(self, seq, verbose=False):\n",
    "      \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n",
    "      context_size = self.order_n - 1\n",
    "      score = 0.0\n",
    "      count = 0\n",
    "      # Start at third word, since we need a full context.\n",
    "      for i in range(context_size, len(seq)):\n",
    "        if (seq[i] == \"<p>\" or seq[i] == \"</p>\"):\n",
    "            continue  # Don't count special tokens in score.\n",
    "        context = seq[i-context_size:i]\n",
    "        context_prob = self.next_token_conditional_prob(seq[i], context)\n",
    "        s = np.log2(context_prob)\n",
    "        score += s\n",
    "        count += 1\n",
    "        # DEBUG.\n",
    "        if verbose:\n",
    "            print (\"log P(%s | %s) = %.03f\" % (seq[i], \" \".join(context), s))\n",
    "      return score, count      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pV2LF-OA6cc9"
   },
   "outputs": [],
   "source": [
    "smoothed_trigram_lm = AddKTrigramLM(flat_delimited_train_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5kElIJYSAD6"
   },
   "source": [
    "#### Exercise\n",
    "- Evaluate the perplexity of the model with different values of K. \n",
    "- What is the best value for k you can find? \n",
    "- What parameter should you use? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO30dnZmS4RF"
   },
   "source": [
    "Note: Each iteration may take time -- about 10-20 seconds in colab, so be careful how many parameters you test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21bXD4kwHsjU"
   },
   "outputs": [],
   "source": [
    "K = [0.000001, 0.0001, 1.0, 2.0]\n",
    "\n",
    "for k in K:\n",
    "  print(\"Trying k value: \", k)\n",
    "  smoothed_trigram_lm.set_k(k=k)\n",
    "  log_p_data, num_real_tokens = smoothed_trigram_lm.sequence_probability(flat_delimited_train_tokens)\n",
    "  print (\"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens)))\n",
    "\n",
    "\n",
    "  log_p_data, num_real_tokens = smoothed_trigram_lm.sequence_probability(flat_delimited_test_tokens)\n",
    "  print(\"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens)))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0R8WPaZ7ukB"
   },
   "source": [
    "What happens to the train/test perplexities as the value increases?  Why should we expect this?\n",
    "A curious thing happens with the test perplexity. It starts very high (remember it was infinite without smoothing), it then decreases indicating that the model fits the data better.  Finally, both train and test perplexity increase dramatically as we move away from good parameters.  This is typical of many ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtYJitVuJqUo"
   },
   "source": [
    "## Wrap up\n",
    "\n",
    "Let's go back to our original spelling corrector and see if we can do better with our trigram language model. Recall that the naive spelling corrector made silly mistakes:\n",
    "\n",
    "*   it is amazon -> it is amazon\n",
    "*   http www amazin -> http www amazing\n",
    "\n",
    "\n",
    "\n",
    "If we used our trigram model, could we do better? Let's see how our model would perform by scoring these sequences.\n",
    "**Note: **Change the k value to the best value you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWbSotunILxZ"
   },
   "outputs": [],
   "source": [
    "# [it is amazon]\n",
    "smoothed_trigram_lm.set_k(k=0.0001)\n",
    "print(smoothed_trigram_lm.sequence_probability([\"it\", \"is\", \"amazon\"], verbose=False)[0])\n",
    "\n",
    "# [it is amazing]\n",
    "print(smoothed_trigram_lm.sequence_probability([\"it\", \"is\", \"amazing\"], verbose=False)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZhHVhccLJ4e"
   },
   "source": [
    "The unsmoothed probability would be infinite; with our smoothed model the probabilities of the incorrect sequences are infinitessimal compared with the correct ones. Note that log probabilities are always negative, so the smaller magnitude is better. And remember the log scale: a difference of score of 10 units means one utterance is $2^{10} = 1K$ times more likely!\n",
    "\n",
    "In practice, spelling correctors are more complicated than this simple model. A real spelling corrector must also consider all possible sequences of words and find the most likely sequence! We'll see how this work when we discuss NLP in coming weeks. If you'd like to read more about an application in practice, feel free to read about how shopping service Etsy uses language models in [spelling correction in search](https://codeascraft.com/2017/05/01/modeling-spelling-correction-for-search-at-etsy/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93XMGHE-dCib"
   },
   "source": [
    "# End\n",
    "\n",
    "This is the end of Lab 3. Please leave us a little feedback about the lab [on the Moodle quiz](https://moodle.gla.ac.uk/mod/feedback/view.php?id=1114113).\n",
    "\n",
    "There is another application of language modelling in the Extra Material below if you would like more fun. Peter Norvig's book \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0ZpeD5vd_hc"
   },
   "source": [
    "## Extra material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw6eR5TidzIZ"
   },
   "source": [
    "### Application 2: Text Segmentation\n",
    "\n",
    "You can see we can apply this **really** simple model to another neat application[1]!\n",
    "\n",
    "In the two cells below, we test out our model by taking a piece of text with all the spaces removed, enumerating all possible ways of splitting it (efficiently using dynamic programming), scoring each, and then returning the highest scoring sequence.\n",
    "\n",
    "[1] Peter Norvig, a director of research at Google implemented this model here:  http://norvig.com/ngrams/ch14.pdf and extends it pretty far.  By the end, he has managed to decrypt WWI encryption using only this simple language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOma5RTxdytu"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SimpleSegmenter(object): \n",
    "  \n",
    "  def __init__(self, dict):\n",
    "    self.dict = dict\n",
    "  \n",
    "  def memo(self, fn):\n",
    "    cache = {}\n",
    "    def docache(arg):\n",
    "      if arg in cache:\n",
    "        return cache[arg]\n",
    "      val = fn(arg)\n",
    "      cache[arg] = val\n",
    "      return val\n",
    "    return docache\n",
    "\n",
    "  #@memo\n",
    "  def segment(self, text):\n",
    "    if not text: return []\n",
    "    candidates = ([first]+self.segment(rem) for first, rem in self.splits(text))\n",
    "    return max(candidates, key=lambda w: self.Pwords(w))\n",
    "\n",
    "  def splits(self, text, L=20):\n",
    "    return [(text[:i+1], text[i+1:])\n",
    "            for i in range(min(len(text), L))]\n",
    "\n",
    "  def Pw(self, w):\n",
    "      # We see here that we often perform operations in log space to avoid issues\n",
    "      # of floating point error with very small numbers.\n",
    "      if w in self.dict.vocab_set:\n",
    "          return math.log(self.dict.token_counts[w]) - math.log(self.dict.N)\n",
    "      else:\n",
    "          return math.log(self.dict.N) - 100*len(w)\n",
    "\n",
    "  def Pwords(self, words):\n",
    "    return sum(self.Pw(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zsav1I10B_H4"
   },
   "outputs": [],
   "source": [
    "segmenter = SimpleSegmenter(dictionary)\n",
    "print(segmenter.segment('hellotherehowareyou'))\n",
    "print(segmenter.segment('tryyourstringhere'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaUBSlT0assn"
   },
   "source": [
    "We could use this segmenter to optimally tokenize URLs, for example.  This segmenter just uses the simple unigram model, but it could also use more complex LMs.\n",
    "\n",
    "**Question**: What's the smoothing used in this probability?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TAD2019 Lab 3 - Language Modeling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
