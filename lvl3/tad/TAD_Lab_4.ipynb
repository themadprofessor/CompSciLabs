{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TAD Lab 4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "D7Ntn894nap0",
        "RB8CdPx9LiKy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cl3cfQa7qdSR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 4: Word Vector representations\n",
        "\n",
        "In this week's lab, we'll explore dense word vector representations.\n",
        "\n",
        "*   Use word vectors to explore word meaning (similar words, analogies, and what does not belong)\n",
        "*   Build a word2vec model of Reddit data\n",
        "*   Use dense vector word representations to model documents with doc2vec\n",
        "* Evaluate the quality of word embedding models\n"
      ]
    },
    {
      "metadata": {
        "id": "xXlWDEnqOtk4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Glove embeddings\n",
        "\n",
        "[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) developed at Stanford are another way of creating word embeddings.  These dense vectors are trained using sparse global word co-occurrence vectors. \n",
        "\n",
        "The website provides pre-trained word embeddings from a variety of sources. We'll be using the 6B collection, trained on Wikipedia and a large collection of news documents.  We'll be using a vector representation of size 200, but the pretrained embeddings are also available in other sizes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nqWvzaBEw3JW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "local_file = \"glove.6B.200d_gensim.txt.gz\"\n",
        "!gsutil cp  gs://textasdata/glove.6B.200d_gensim.txt.gz $local_file "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OutT1R5E306G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install 'gensim==3.2.0'\n",
        "!pip install --upgrade gensim\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7cBl41_kzPUP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note: Loading the glove vectors takes approximately 1 minute in Colab."
      ]
    },
    {
      "metadata": {
        "id": "40cAq2b5HPEb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d_gensim.txt.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clzxxvv9zZlk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Warmup: Read the Gensim documentation on [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) for working with pre-trained word embeedings.  If you want to go deeper on how Gensim works, you can read the [source code](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py).\n",
        "\n",
        "**Note:** The GLOVE embeddings are unigrams that have been normalized (lowercased). Word lookups on the vectors need to be the same or the lookups will fail. Not all words may be in the vector, these are OOV words that would be ignored."
      ]
    },
    {
      "metadata": {
        "id": "4B-TT8v_1geu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Exercise:\n",
        "* Use the gensim API to find the 10 `most_similar` words by cosine similarity\n",
        "* Try postive words \"university\" and \"glasgow\". \n",
        "* Beyond these words, experiment with other words as positive seeds."
      ]
    },
    {
      "metadata": {
        "id": "ruLdEQk9ou75",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmNFWB4JobDF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Try your own!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y9bgwU9y18iP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise: \n",
        "\n",
        "* Create a function `word_anology` that uses cosine to solve problem analogy A is to B =  BLANK  is to C?  (e.g. Paris is to France = ___ is to England)\n",
        "* It should take a parameter, `k`, the number of possible answers to return.\n",
        "\n",
        "**Hint**: Formulate this with addition and subtraction with the word vectors. "
      ]
    },
    {
      "metadata": {
        "id": "kCHGf1VxhArk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_analogy(word_a, word_b, word_c, k=5):\n",
        "    \"\"\"\n",
        "    Function that solves problem analogy word_a to word_b = word_c to ?\n",
        "    @param word_a, word_b, word_c: string\n",
        "    @param k: top k candidates to return\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RHb1a6czPUXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_analogy(\"london\", \"england\", \"scotland\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6DV2_u0kAxhM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_analogy(\"fish\", \"water\", \"soil\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nrjJev7gCSbn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Play with a few of your own word analogies. "
      ]
    },
    {
      "metadata": {
        "id": "VbcUvKSyDRiy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now play another game: \"Which of these things does not belong\". \n",
        "\n",
        "### Optional Exercise \n",
        "* Implement a function,  `doesnt_match `\n",
        "* Take a series of word as input, returns the word farthest from the mean (average) embedding by cosine similarity. \n",
        "\n",
        "**Hints**:\n",
        " - np.mean(vectors, axis=0) performs element-wise averages\n",
        " [20, 30]\n",
        " [10, 10]\n",
        " = [15, 20]\n",
        " - See  `cosine_similarities` built in to the  `glove_model ` (KeyedVector) object. \n",
        " \n",
        " You can click SHOW CODE to see the solution.\n"
      ]
    },
    {
      "metadata": {
        "id": "eAuWymiHD1pQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BYyyzG8V-DTz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "\n",
        "def doesnt_match(words):\n",
        "  filtered_words = [word for word in words if word in glove_model]\n",
        "  vectors = [glove_model.word_vec(word, use_norm=True) for word in filtered_words]\n",
        "  mean = np.mean(vectors, axis=0)\n",
        "  distances = glove_model.cosine_similarities(mean, vectors)\n",
        "  return sorted(zip(distances, filtered_words))[0][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xG5y1VifyHCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doesnt_match(\"apple pear orange car\".split())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-Lcy0j0ULCY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Try another example of not matching."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KKDk1Two6Od",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Try this out with another series of words and see if it works!"
      ]
    },
    {
      "metadata": {
        "id": "pbb1pQGSChZt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train a Reddit Word2Vec Model"
      ]
    },
    {
      "metadata": {
        "id": "3gkokb7r1CKE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the reddit data and tokenize it."
      ]
    },
    {
      "metadata": {
        "id": "ArN6SGUFqlWI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "local_file = \"coarse_discourse_dump_reddit.json\"\n",
        "\n",
        "!gsutil cp gs://textasdata/coarse_discourse_dump_reddit.json $local_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BYgoXyC-EVTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!python -m spacy download en\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the small english model. \n",
        "# Disable the advanced NLP features in the pipeline for efficiency.\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('parser')\n",
        "\n",
        "#@Tokenize\n",
        "def spacy_tokenize(string):\n",
        "  tokens = list()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "#@Normalize\n",
        "def normalize(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    if (token.is_alpha or token.is_digit):\n",
        "      normalized = token.text.lower().strip()\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "\n",
        "#@Tokenize and normalize\n",
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtHo_snoEuD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "posts = list()\n",
        "\n",
        "# If the dataset is too large, you can load a subset of the posts.\n",
        "post_limit = 100000000\n",
        "\n",
        "# Construct a dataframe, by opening the JSON file line-by-line\n",
        "with open(local_file) as jsonfile:\n",
        "  for i, line in enumerate(jsonfile):\n",
        "    thread = json.loads(line)\n",
        "    if (len(posts) > post_limit):\n",
        "      break\n",
        "      \n",
        "    for post in thread['posts']:\n",
        "      posts.append((thread['subreddit'], thread['title'], thread['url'],\n",
        "                        post['id'], post.get('author', \"\"), post.get('body', \"\")))\n",
        "print(len(posts))\n",
        "\n",
        "labels = ['subreddit', 'title', 'id', 'url', 'author', 'body']\n",
        "post_frame = pd.DataFrame(posts, columns=labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fMfcZ4nFq-Zn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use the tokenizer to extract all tokens from the body of the posts.\n",
        "# Flatten the tokens in the post into a single list of all the tokens.\n",
        "import itertools\n",
        "all_tokens = []\n",
        "all_posts_tokenized = post_frame.body.apply(tokenize_normalize)\n",
        "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
        "print(\"Num tokens: \", len(all_tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I1RPfRl26Idg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gensim word2vec model ###\n",
        "\n",
        "In this section, we'll train a word2vec model on the reddit data using [Gensim](https://radimrehurek.com/gensim/index.html). Gensim is a widely used 'topic modeling' library that is used for various word and document similarities in Python.\n",
        "\n",
        "You will need to refer to the documentation of Gensim's [Word2Vec Model](https://radimrehurek.com/gensim/models/word2vec.html).\n",
        "\n",
        "Some of the important parameters:\n",
        "*   `size`: Number of dimensions for word embedding model\n",
        "*   `window`: Number of context words to observe in each direction\n",
        "*   `min_count`: Minimum frequency for words included in model\n",
        "*   `sg` (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram\n",
        "*   `alpha`: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n",
        "*   `iterations`: Number of passes through dataset\n",
        "*   `batch_words`: Number of words to sample from data during each pass\n"
      ]
    },
    {
      "metadata": {
        "id": "3Bc0DndejtTy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note: Training the model should take less than 1-2 minutes in colab.\n",
        "\n",
        "**Exercise**: \n",
        "- Train a CBOW model on `all_posts_tokenized`\n",
        "- Add the correct parameters to gensim and train a model\n",
        "- Paramters: `window = 5, dimensions = 50, min_count=5, alpha=0.025`,  `size = 50`, `batch_words=10000` to have a batch size of 10k. \n",
        "\n",
        "We're training a CBOW model because on small collections, like the Reddit data we're using, CBOW is more effective than Skip-gram. Skip-gram models usually perform better on larger datasets. \n",
        "\n",
        "We also usually use more than 50 dimensions (typically 100-300 or more).\n"
      ]
    },
    {
      "metadata": {
        "id": "p3YEVvG5iLo1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "model = gensim.models.Word2Vec(all_posts_tokenized ...\n",
        "print (\"done in %.02f s\" % (time.time() - t0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ofy4xuhrLU3_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How well does the model work?  \n",
        "\n",
        "Let's compute the cosine similarity between several combinations of vectors to see if they make sense. \n",
        "Use the gensim [word2vec API](https://radimrehurek.com/gensim/models/word2vec.html).\n",
        "\n",
        "**Exercise:** Find the similarities between 'man' and 'woman'.\n",
        "Hint: Look at the `wv` member.\n"
      ]
    },
    {
      "metadata": {
        "id": "NgDGXAMrjchg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzuXJAEULQcG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, do the same for 'woman' and girl'.\n"
      ]
    },
    {
      "metadata": {
        "id": "gKJijkY5jobP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKCFYA8RinLC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Which one is more similar?  Is this what you expect?\n",
        "\n",
        "Let's dig deeper and look at months."
      ]
    },
    {
      "metadata": {
        "id": "_rXLjtYALYZl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pick a month of the year.  What are the most similar 11 words? 20 words?"
      ]
    },
    {
      "metadata": {
        "id": "UfbV_f1_jwEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zX89Vyeivpb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- What month is (usually) missing for some months? \n",
        "- Why do you think this could be given what we discussed in lecture?"
      ]
    },
    {
      "metadata": {
        "id": "Ec0qjPWWF0Uy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word vector document representations"
      ]
    },
    {
      "metadata": {
        "id": "YyRukZ5ChHUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've looked at how to represent words with dense vectors. Let's now apply this by using dense word vectors to represent documents. \n",
        "\n",
        "\n",
        "Recall from Lecture 1: We typically represent each document as a vector, with one dimension for each word in the dictionary (i.e. each document's vector is $|V|$). How can we expand this to deal with word embeddings? We could represent each term occurrence by the $|D|$ dimensions of its word embedding vector - i.e. where each word occurrence is represented with the vector for that word. \n",
        "\n",
        "\n",
        "However, this would lead to a very large document representation (a vector of $|D| * |V|$ in our case)! Instead, one approach is to combine the dense vector representations.\n",
        "\n",
        "\n",
        "In this part of the lab, we'll experiment with different ways of combining word vectors Each document will be represented by a single $|D|$ dimensional vector that is the combination of all of its word vectors.\n",
        "\n",
        "We could combine the vectors in different ways:\n",
        "*   Take the average of each dimension\n",
        "*   Take the min or max of each dimension\n",
        "\n",
        "### A note on SKLearn: BaseEstimator interface\n",
        "The root of the API is an `Estimator`, broadly any object that can learn from data. The primary `Estimator` objects implement classifiers, regressors, or clustering algorithms. However, they can also include a wide array of data manipulation, from dimensionality reduction to feature extraction from raw data. The `Estimator` essentially serves as an interface, and classes that implement `Estimator` functionality must have two methods—`fit` and `predict`. \n",
        "\n",
        "A `Transformer` is a special type of `Estimator` that creates a new dataset from an old one based on rules that it has learned from the fitting process. it follows  `fit` and `transform`.\n",
        "\n",
        "As you've seen, we'll use these extensively.  In many cases it's easier to create your own Estimator/Transformer for custom applications.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kpmZNj-7NzRf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class Estimator(BaseEstimator):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Accept input data, X, and optional target data, y. Returns self.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Accept input data, X and return a vector of predictions for each row.\n",
        "        \"\"\"\n",
        "        return yhat\n",
        "      \n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "class Transfomer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Learn how to transform data based on input data, X.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform X into a new dataset, Xprime and return it.\n",
        "        \"\"\"\n",
        "        return Xprime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ukKN1aqNN5-f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise:\n",
        "Create a SKlearn wrapper (Transformer) around Gensim's W2V features to create a document representation that averages word vectors to create a document vector.\n",
        "\n",
        "* Complete the implementation of Transform \n",
        "* Input is `X`: a vector of documents -> list of tokens\n",
        "* Ignore OOV words \n",
        "* Take the average (mean) of the embeddings for each word word in the document\n",
        "\n",
        "**Reminder** Transform takes a document-feature matrix as input.  Assume that X is a vector of documents each containing a vector of tokens (tokenization performed).  \n",
        "\n",
        "As in the `doesnt_match` above, use numpy for the vector arithmetic. \n",
        "\n",
        "**Hints** \n",
        "- If all words in the document are UNK, return a $|D|$ dimensional vector of 0s.\n",
        "- This is particularly elegant with nested list comprehensions (documents, tokens)\n",
        "\n",
        "If you are stuck, you can show code to see the solution below."
      ]
    },
    {
      "metadata": {
        "id": "4szpAvfnq-qj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class AverageEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
        "  \n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding = embedding_model\n",
        "        self.dimension = embedding_model.vector_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      # Nothing is required here. No collection properties are needed.\n",
        "      return self\n",
        "      \n",
        "    def transform(self, X):\n",
        "      # Input: X: an iterable of documents that have been tokenized.\n",
        "      # Example of input with two documents: \n",
        "      # [[\"the\", \"cat\", \"ran\"], [\"the\", \"cat\", \"jumped\"]]\n",
        "      # Output: a numpy array of vectors, one for each document.  Each vector\n",
        "      # is the mean of the vectors of each word in the document. \n",
        "      # Hint: It may require a nested loop / for comprehension.\n",
        "      # Be sure to skip OOV terms. Return 0 if no words are in the vocabulary.\n",
        "      return <YOUR CODE HERE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OhsuyEflOMZF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#Solution\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class AverageEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding = embedding_model\n",
        "        self.dimension = embedding_model.vector_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "      \n",
        "    def transform(self, X):  \n",
        "      # Skip OOV terms. Return 0 if no words are in the vocabulary.\n",
        "      #print (X)\n",
        "      return np.array([ \n",
        "          np.mean([self.embedding[token] for token in doc if token in self.embedding]\n",
        "                or [np.zeros(self.dimension)], axis=0)\n",
        "          for doc in X\n",
        "      ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SIxIQY7RdA7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use a vectorizer with the Glove vectors and Reddit w2v vectors.  This will \n",
        "# allow us to easily compare them.\n",
        "reddit_vectorizer = AverageEmbeddingVectorizer(model)\n",
        "glove_vectorizer = AverageEmbeddingVectorizer(glove_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OBPR1VWDnPCQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compare the reddit and glove vectorizers for the same vector of tokens. What's different (besides the fact that the numbers are different)?\n"
      ]
    },
    {
      "metadata": {
        "id": "AUrTFLXTimF0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = tokenize_normalize('watch the cat chase the dog')\n",
        "X = [doc]\n",
        "print(\"Glove:\")\n",
        "print(glove_vectorizer.transform(X))\n",
        "print(\"Reddit:\")\n",
        "print(reddit_vectorizer.transform(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Z73FjdslTSY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that the number of dimensions is different, as well as producing very different vector values."
      ]
    },
    {
      "metadata": {
        "id": "Ytu2OnTXXnIC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's process our collection with our embedding vectorizer."
      ]
    },
    {
      "metadata": {
        "id": "fZxy3jM4peZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove_post_vector_matrix = glove_vectorizer.transform(all_posts_tokenized)\n",
        "reddit_post_vector_matrix = reddit_vectorizer.transform(all_posts_tokenized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YXyPANmenzw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Word2Vec Questions:** \n",
        "*   Do you expect this would this work well for long documents?  Why or why not?\n",
        "*   What about stopwords or non-informative words? See the optional exercises for ideas on how to improve the vectorizer. \n",
        "*   Averaging word vectors has some disavantages, what happens to word order? \n",
        "\n",
        "Doc2Vec addresses some of these issues.\n"
      ]
    },
    {
      "metadata": {
        "id": "PiUzIS-Gc0lp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Paragraph embeddings\n",
        "There is an extension to word2vec to learn fixed-length representations of variable length documents, beyond taking a simple average.  Doc2Vec was shown to be effective as a text classification feature as well as paragraph similarity.\n",
        "\n",
        "See Gensim's description of [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html). It was developed at Google Research  by Quoc Le and Tomas Mikolov: “[Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053v2.pdf)\". "
      ]
    },
    {
      "metadata": {
        "id": "RaeAmviGcjQc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import collections\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "tagged_documents = [TaggedDocument(words=_d, tags=[i]) for i, _d in enumerate(all_posts_tokenized)]\n",
        "\n",
        "# In the paper they use 400 as the dimensions, may take many more epochs to converge.\n",
        "# The devil is in the details to make these work effectively.\n",
        "d2v_model = gensim.models.doc2vec.Doc2Vec(tagged_documents, vector_size=300, alpha=0.025, min_alpha=0.001, min_count=5, window=8, epochs=10)\n",
        "#vocab = collections.Counter(all_tokens)\n",
        "#d2v_model.build_vocab_from_freq(vocab)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BbYNnVxLoiFO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d2v_model.docvecs[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZh6r3q9jmsC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This should really be called 'transform'\n",
        "d2v_model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
        "#d2v_model.save(\"d2v.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rfMb-Fbyr9Nx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This represents the paragraph just like another word in the collection."
      ]
    },
    {
      "metadata": {
        "id": "94Yq3wffa09W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "We'll look at two ways to evaluate our models.\n",
        "\n",
        "1.   Finding similar documents\n",
        "2.   Word analogies\n",
        "\n",
        "These are both extrinsic evaluations.  Other extrinsic evaluations include using them as part of supervised learning, such as text classification."
      ]
    },
    {
      "metadata": {
        "id": "ZxxhnFVjtkVX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Find similar documents revisited ###\n",
        "\n",
        "One application of this is finding related reddit posts for a given document or string. For example, to create a 'Reddit widget' to embed in a webpage that shows related content from Reddit. \n",
        "\n",
        "We'll use our representation of documents to find similar posts and to query the documents.\n"
      ]
    },
    {
      "metadata": {
        "id": "WEQxi7MkOeB7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        " \n",
        "# A function that given an input query item returns the top-k most similar items \n",
        "# by their cosine similarity.\n",
        "def find_similar(query_vector, vd_matrix, top_k = 5):\n",
        "    cosine_similarities = cosine_similarity(query_vector, vd_matrix).flatten()\n",
        "    related_doc_indices = cosine_similarities.argsort()[::-1]\n",
        "    return [(index, cosine_similarities[index]) for index in related_doc_indices][0:top_k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swKnLZfCuIZp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Find the top 10 most similar posts based on cosine similarity in the word vector representation. Try this with both the glove and w2v embeddings."
      ]
    },
    {
      "metadata": {
        "id": "IDEqol2DV3T4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = tokenize_normalize('end of the world as we know it')\n",
        "\n",
        "# Glove vectorizer\n",
        "vectorizer = glove_vectorizer\n",
        "matrix = glove_post_vector_matrix\n",
        "\n",
        "# Reddit w2v\n",
        "#vectorizer = reddit_vectorizer\n",
        "#matrix = reddit_post_vector_matrix\n",
        "\n",
        "transformed = vectorizer.transform([doc])\n",
        "\n",
        "query_vector = transformed[0:1]\n",
        "print(\"\\nSimilar posts:\")\n",
        "for index, score in find_similar(query_vector, matrix, 10):\n",
        "  post_contents = post_frame.iloc[index]['body'].replace('\\n', '')\n",
        "  post_limited = (post_contents[:75] + '..') if len(post_contents) > 75 else post_contents\n",
        "  print(score, index, post_limited)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mYnEn9IrYF-D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random as rand\n",
        "post_index = 1000\n",
        "post_index = rand.randint(0, len(all_posts_tokenized))\n",
        "tokens = all_posts_tokenized[post_index]\n",
        "print(tokens)\n",
        "\n",
        "# Transform our string using the vocabulary\n",
        "transformed = vectorizer.transform([tokens])\n",
        "doc = transformed[0:1]\n",
        "\n",
        "print(\"\\nSimilar posts:\")\n",
        "for index, score in find_similar(doc, matrix, 10):\n",
        "  post_contents = post_frame.iloc[index]['body'].replace('\\n', '')\n",
        "  post_limited = (post_contents[:150] + '..') if len(post_contents) > 150 else post_contents\n",
        "  print(score, index, post_limited)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rsK3oNBVhZlc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below does the same step random post with doc2vec."
      ]
    },
    {
      "metadata": {
        "id": "bDYO4OzohUAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compare with doc2vec\n",
        "\n",
        "inferred_vector = d2v_model.infer_vector(tokens)\n",
        "print(tokens)\n",
        "similar = d2v_model.docvecs.most_similar([inferred_vector], topn=10)\n",
        "\n",
        "print(\"\\nSimilar posts:\")\n",
        "for (label, score) in similar:\n",
        "  post_contents = post_frame.iloc[label]['body'].replace('\\n', '')\n",
        "  post_limited = (post_contents[:150] + '..') if len(post_contents) > 150 else post_contents\n",
        "  print(score, index, post_limited)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ylQ_PRBdl21E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compare the similarity of Glove vs. Reddit vectorizers on this task.  Are the results different, which one do you think is more effective? \n",
        "\n",
        "In the current case, it looks like the doc2vec vectors are not (yet) optimal.  They may need additional training or different parameters to be very effective."
      ]
    },
    {
      "metadata": {
        "id": "D7Ntn894nap0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Extrinsic evaluation with word analogies ###\n",
        "We've seen how to combine word vectors and find the most similar words.  \n",
        "\n",
        "Recall: glove_model.wv.doesnt_match(\"apple pear orange car\".split())\n",
        "\n",
        "Revisit this problem and try implementing a solution yourself. After you're done, you could look at the source code in gensim to see how your approach compares.\n"
      ]
    },
    {
      "metadata": {
        "id": "9rsOT_cSe5PW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "!wget https://raw.githubusercontent.com/RaRe-Technologies/gensim/develop/gensim/test/test_data/questions-words.txt\n",
        "\n",
        "glove_model.wv.evaluate_word_analogies('questions-words.txt')\n",
        "model.wv.evaluate_word_analogies('questions-words.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oW3ciNtEbL-T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note: this may be slow for the 300D model."
      ]
    },
    {
      "metadata": {
        "id": "zRFU78rJfmt4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The reddit model accuracy."
      ]
    },
    {
      "metadata": {
        "id": "JcpFGG90b0LB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.wv.evaluate_word_analogies('questions-words.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbPnbSD4cSWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove_model.wv.evaluate_word_analogies('questions-words.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVWRA1Zlyyqp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have just scratched the surface of word embeddings. \n",
        "\n",
        "See this great post about the rise of [BERT, ELMo, and others](http://jalammar.github.io/illustrated-bert/).  These new embedding models that perform the task of language modeling are state-of-the-art and leading to a new revolution in NLP effectiveness. \n"
      ]
    },
    {
      "metadata": {
        "id": "_1BkhgGWfI-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapup\n",
        "\n",
        "Take the [Moodle quiz](https://moodle.gla.ac.uk/mod/feedback/view.php?id=1118007) for this lab and let us know what you think."
      ]
    },
    {
      "metadata": {
        "id": "hBs_5JNxhkDS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Optional) Extra exercises \n",
        "\n",
        "### Modify the document vector representation ###\n",
        "\n",
        "Experiment with other ways to combining the word vectors.  \n",
        "\n",
        "*   What about taking max?\n",
        "*   Add IDF weighting to create a weighted average\n",
        "*   Remove stopwords or other non-informative words\n",
        "\n",
        "### Modify embedding hyper-parameters ###\n",
        "Retrain the reddit model with different parameters. For example try varying some of the following:\n",
        "\n",
        "*   Dimension of embeddings\n",
        "*   Window size \n",
        "*   Skipgram vs CBOW\n",
        "*   Iterations or learning rates\n",
        "\n",
        "\n",
        "### Other embedding models\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RB8CdPx9LiKy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Playing with vizualizations ###"
      ]
    },
    {
      "metadata": {
        "id": "vYjfYgwqqoBv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use a WordCloud to vizualize the similar words. You could use the popular WordCloud python library. (And we'll use a nice font from Google Fonts.)"
      ]
    },
    {
      "metadata": {
        "id": "o6YsbXCZlGFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/tad2018/OpenSansCondensed-Bold.ttf\n",
        "!pip install wordcloud\n",
        "\n",
        "# Below is optional code to create a 'mask' to put the wordcloud into a cool shape.\n",
        "!wget https://storage.googleapis.com/tad2018/reddit-mask.png\n",
        "\n",
        "from PIL import Image\n",
        "from os import path\n",
        "import os \n",
        "\n",
        "reddit_mask = np.array(Image.open(path.join(os.getcwd(), \"reddit-mask.png\")))\n",
        "tad_mask = np.array(Image.open(path.join(os.getcwd(), \"tad-mask3.png\")))\n",
        "\n",
        "# !wget https://storage.googleapis.com/tad2018/tad-mask3.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cfjImZvqQFpb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "terms = model.wv.most_similar(positive=['lit', 'thanks'], topn=1000)\n",
        "print(terms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3grnP90trHDo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Convert the similarity score into an integer count, N.\n",
        "# Repeat the word in a space separated string N times.\n",
        "def terms_to_wordcounts(terms, multiplier=1000):\n",
        "    <Your Code here>\n",
        "\n",
        "term_counts = terms_to_wordcounts(terms)\n",
        "\n",
        "wc = WordCloud(font_path='/content/OpenSansCondensed-Bold.ttf',\n",
        "                      width=2048,\n",
        "                      height=2048,\n",
        "                      max_words=1000,\n",
        "                      mask=reddit_mask, # optional mask\n",
        "                      background_color=\"white\").generate(term_counts)\n",
        "\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.savefig(\"terms1\")\n",
        "plt.close()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-hOiFKaPopK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Solution \n",
        "\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def terms_to_wordcounts(terms, multiplier=1000):\n",
        "  counts = (i[0]+\" \" * int(multiplier*i[1]) for i in terms)\n",
        "  return  \" \".join(counts)\n",
        "\n",
        "\n",
        "term_counts = terms_to_wordcounts(terms)\n",
        "\n",
        "wc = WordCloud(font_path='/content/OpenSansCondensed-Bold.ttf',\n",
        "                      width=2048,\n",
        "                      height=2048,\n",
        "                      max_words=1000,\n",
        "                      mask=reddit_mask, # optional mask\n",
        "                      background_color=\"white\").generate(term_counts)\n",
        "\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.savefig(\"terms1\")\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01HzDyNSlb86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wc.to_file(\"terms.png\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('terms.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}